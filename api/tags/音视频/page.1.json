{"data":{"index":1,"total":1,"posts":[{"title":"H264码流结构理解整理","slug":"H264码流结构理解整理","date":"2025-09-15T13:31:00.000Z","updated":"2025-09-15T13:53:19.214Z","comments":true,"url":"2025/09/15/H264码流结构理解整理/","excerpt":"<p>本文将带你深入H.264文件的内部，从宏观到微观，逐一剖析其各个组成部分的作用、相互关系以及一些精妙的设计哲学。<br>在了解H264之前需要有以下的一些基础知识：</p>\n<h2 id=\"宏观结构：从文件到帧\"><a href=\"#宏观结构：从文件到帧\" class=\"headerlink\" title=\"宏观结构：从文件到帧\"></a>宏观结构：从文件到帧</h2><p>一个H.264原始码流（<code>.h264</code>或<code>.264</code>文件）并不是一个简单的“视频文件”，它不包含音频、字幕等元信息。它是一个<strong>纯粹的、编码后的视频数据比特流</strong>。这个流的结构可以看作一个分层模型，如下图所示，理解这个结构是理解H.264的关键：</p>\n<p><img src=\"https://www.hardening-consulting.com/images/h264_bitstream.png\" alt=\"\"></p>\n<h3 id=\"网络抽象层单元-NAL-Unit\"><a href=\"#网络抽象层单元-NAL-Unit\" class=\"headerlink\" title=\"网络抽象层单元 (NAL Unit)\"></a>网络抽象层单元 (NAL Unit)</h3><p>H.264设计的一个核心思想是<strong>网络友好性</strong>。为了实现这一目标，整个码流被分割成一个个独立的包，称为 <strong>NAL Unit（网络抽象层单元）</strong>。每个NAL Unit都是一个自包含的数据包，包含一个头部和负载数据。这种设计使得H.流非常适合在容易产生包丢失和延迟的网络（如RTP/UDP）中传输，因为一个NAL Unit的丢失通常不会导致整个视频无法解码。</p>\n<h3 id=\"关键概念：帧-Frame-与片-Slice\"><a href=\"#关键概念：帧-Frame-与片-Slice\" class=\"headerlink\" title=\"关键概念：帧 (Frame) 与片 (Slice)\"></a>关键概念：帧 (Frame) 与片 (Slice)</h3><p>在视频编码中，一<strong>帧（Frame）</strong> 通常对应一张静态图片。H.264对一帧图像进行编码后，其数据可能会被装进<strong>一个或多个NAL Unit</strong>中。</p>\n<p>为什么是一或多个？这是因为一帧数据可以被分割成多个<strong>片（Slice）</strong>。每个Slice都是一个独立的编码单元，包含了一帧图像中的一部分宏块（Macroblock）。将一帧分割成多个Slice主要有两个好处：</p>\n<ol>\n<li><strong>错误恢复</strong>：在网络传输中，如果一个Slice丢失了，解码器仍然可以利用错误隐藏技术来近似恢复图像，而不是丢失整帧。</li>\n<li><strong>并行处理</strong>：多个Slice可以并行编码或解码，提高效率。</li>\n</ol>\n<h2 id=\"微观结构：NAL-Unit的内部世界\"><a href=\"#微观结构：NAL-Unit的内部世界\" class=\"headerlink\" title=\"微观结构：NAL Unit的内部世界\"></a>微观结构：NAL Unit的内部世界</h2><p>现在，让我们打开一个NAL Unit，看看它里面到底有什么。</p>\n<h3 id=\"NAL-Unit-Header（头部）\"><a href=\"#NAL-Unit-Header（头部）\" class=\"headerlink\" title=\"NAL Unit Header（头部）\"></a>NAL Unit Header（头部）</h3><p>每个NAL Unit都以一个1字节（可扩展为2字节）的头部开始。这个头部虽然小，但信息量巨大：</p>\n<ul>\n<li><strong>禁止位（F）</strong>：通常为0，如果为1表示该单元出错。</li>\n<li><strong>重要性指示位（NRI）</strong>：表示这个NAL Unit的重要性。值越大，解码器越需要优先保护它（如SPS/PPS的NRI值最高）。</li>\n<li><strong>类型（Type）</strong>：这是最关键的部分！它定义了该单元负载数据的类型。主要分为两大类：<ul>\n<li><strong>VCL（视频编码层）单元</strong>：真正携带编码视频数据的单元（如Slice）。</li>\n<li><strong>Non-VCL（非视频编码层）单元</strong>：携带元数据和控制信息的单元，是解码的“说明书”。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"NAL-Unit-Payload（负载）\"><a href=\"#NAL-Unit-Payload（负载）\" class=\"headerlink\" title=\"NAL Unit Payload（负载）\"></a>NAL Unit Payload（负载）</h3><p>负载部分的数据内容完全由头部中的<strong>类型（Type）</strong> 决定。</p>\n<h4 id=\"关键的Non-VCL单元（元数据）\"><a href=\"#关键的Non-VCL单元（元数据）\" class=\"headerlink\" title=\"关键的Non-VCL单元（元数据）\"></a><strong>关键的Non-VCL单元（元数据）</strong></h4><p>这些单元不包含图像像素数据，但<strong>没有它们，VCL单元根本无法被解码</strong>。它们通常在视频流开始时发送一次，但如果解码器中途加入，也需要重新获取。</p>\n<p>  <strong>SPS（序列参数集 - Type 7）</strong></p>\n<pre><code>*   **作用**：包含了适用于**整个视频序列**的全局参数。它是解码器的“总纲”。\n*   **包含信息**：视频的档次、级别、分辨率（`pic_width_in_mbs_minus1`等）、帧率、色深、比特深度等。没有SPS，解码器连图像该解码成多大都不知道。\n</code></pre><p>   <strong>PPS（图像参数集 - Type 8）</strong></p>\n<pre><code>*   **作用**：包含了适用于**一幅或多幅图像**的解码参数。它更像是“章节细则”。\n*   **包含信息**：熵编码模式（CAVLC或CABAC）、量化参数等。PPS可以改变，从而在序列中实现不同的编码配置。\n</code></pre><p>   <strong>IDR（即时解码刷新 - 属于VCL，但特殊）</strong></p>\n<pre><code>*   **作用**：一个特殊的Slice（通常是I-Slice），它告诉解码器：“从这里开始，可以独立解码，不再需要参考之前的帧了。”\n*   **意义**：IDR帧是**随机访问和 seeking 的关键点**。当你拖动视频进度条时，播放器总是在寻找最近的IDR帧开始解码，因为它能清空之前的参考帧缓冲区，保证解码正确。\n</code></pre><h4 id=\"VCL单元（核心数据）\"><a href=\"#VCL单元（核心数据）\" class=\"headerlink\" title=\"VCL单元（核心数据）\"></a><strong>VCL单元（核心数据）</strong></h4><p>这些单元携带了实际的压缩视频数据，即Slice。</p>\n<p>  <strong>Slice Header（切片头）</strong></p>\n<pre><code>*   每个Slice都有自己的头，其中包含了当前Slice解码所需的**信息**：\n    *   引用哪个PPS（从而间接引用SPS）。\n    *   帧类型（I, P, B）。\n    *   量化参数。\n    *   根据帧类型，包含运动向量预测所需的信息。\n</code></pre><p>   <strong>Slice Data（切片数据）</strong></p>\n<pre><code>*   这是压缩数据的核心，由一系列**宏块（Macroblock）** 组成。\n*   **宏块**通常是16x16像素的编码单元，它包含了：\n    *   **预测信息**：对于I帧，是帧内预测模式；对于P/B帧，是运动向量（描述当前块是从参考帧的哪个位置移动过来的）。\n    *   **残差数据**：经过预测后，当前块与预测块之间的差值。这部分数据会经过**变换（DCT）、量化、熵编码（CAVLC/CABAC）**，从而获得极高的压缩率。\n</code></pre><h2 id=\"特殊设计点\"><a href=\"#特殊设计点\" class=\"headerlink\" title=\"特殊设计点\"></a>特殊设计点</h2><h3 id=\"3-1-参数集（SPS-PPS）机制\"><a href=\"#3-1-参数集（SPS-PPS）机制\" class=\"headerlink\" title=\"3.1 参数集（SPS/PPS）机制\"></a>3.1 参数集（SPS/PPS）机制</h3><p>这是H.264一个非常巧妙的设计。它将<strong>很少改变但至关重要的信息</strong>（SPS/PPS）与<strong>频繁变化的数据</strong>（Slice）分离开。</p>\n<ul>\n<li><strong>优点一：鲁棒性</strong>：即使丢失了一些Slice，只要SPS/PPS还在，解码器就能继续工作。</li>\n<li><strong>优点二：效率</strong>：无需在每一个Slice中都重复这些头部信息，大大节省了码流。</li>\n<li><strong>优点三：灵活性</strong>：一个码流中可以存在多个PPS，并在不同场景下切换使用。</li>\n</ul>\n<h3 id=\"3-2-I-P-B帧与GOP（图像组）\"><a href=\"#3-2-I-P-B帧与GOP（图像组）\" class=\"headerlink\" title=\"3.2 I, P, B帧与GOP（图像组）\"></a>3.2 I, P, B帧与GOP（图像组）</h3><ul>\n<li><strong>I帧（Intra）</strong>：自包含帧，仅使用本帧内的信息进行编码，不参考其他帧。它是压缩率最低但最关键的帧，是P帧和B帧的锚点。</li>\n<li><strong>P帧（Predicted）</strong>：参考前面的I帧或P帧进行运动补偿预测编码，压缩率高于I帧。</li>\n<li><strong>B帧（Bi-directional）</strong>：可以同时参考前面和后面的帧，获得最高的压缩率，但会带来编码延迟。</li>\n<li>一个<strong>GOP</strong>就是从上一个IDR帧到下一个IDR帧之前的所有帧序列。GOP长度越长，B/P帧越多，压缩率越高，但随机访问的间隔也越长。</li>\n</ul>\n<h3 id=\"3-3-熵编码：CAVLC-与-CABAC\"><a href=\"#3-3-熵编码：CAVLC-与-CABAC\" class=\"headerlink\" title=\"3.3 熵编码：CAVLC 与 CABAC\"></a>3.3 熵编码：CAVLC 与 CABAC</h3><p>这是压缩过程中的最后一步，将数据转换为二进制码流。</p>\n<ul>\n<li><strong>CAVLC（上下文自适应变长编码）</strong>：相对简单，压缩效率一般，用于Baseline等档次。</li>\n<li><strong>CABAC（上下文自适应二进制算术编码）</strong>：非常复杂，但压缩效率比CAVLC高出10%-20%，是Main和High档次效率高的主要原因之一。</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>H.264的结构是一个分层、模块化的杰作：</p>\n<ol>\n<li><strong>整体</strong>：码流由一个个<strong>NAL Unit</strong>组成，适合网络传输。</li>\n<li><strong>局部</strong>：NAL Unit分为<strong>VCL</strong>（携带Slice数据）和<strong>Non-VCL</strong>（携带SPS/PPS等元数据）。</li>\n<li><strong>核心</strong>：Slice数据由<strong>宏块</strong>组成，宏块包含了<strong>预测信息</strong>和<strong>残差数据</strong>，通过预测和变换编码实现压缩。</li>\n<li><strong>精妙设计</strong>：<strong>参数集分离</strong>、<strong>IDR帧</strong>、<strong>Slice划分</strong>和<strong>CABAC</strong>等特性共同造就了H.264在效率、鲁棒性和灵活性上的完美平衡。</li>\n</ol>\n<p>理解H.264的结构，不仅能帮助我们更好地处理视频数据（如封装、传输、解码问题定位），更能让我们体会到工程师们在标准制定中的智慧和远见。尽管如今H.265/HEVC、AV1等更先进的编码器已经出现，但H.264的基本设计思想和结构仍然深刻地影响着它们。</p>\n","cover":null,"images":["https://www.hardening-consulting.com/images/h264_bitstream.png"],"content":"<p>本文将带你深入H.264文件的内部，从宏观到微观，逐一剖析其各个组成部分的作用、相互关系以及一些精妙的设计哲学。<br>在了解H264之前需要有以下的一些基础知识：</p>\n<h2 id=\"宏观结构：从文件到帧\"><a href=\"#宏观结构：从文件到帧\" class=\"headerlink\" title=\"宏观结构：从文件到帧\"></a>宏观结构：从文件到帧</h2><p>一个H.264原始码流（<code>.h264</code>或<code>.264</code>文件）并不是一个简单的“视频文件”，它不包含音频、字幕等元信息。它是一个<strong>纯粹的、编码后的视频数据比特流</strong>。这个流的结构可以看作一个分层模型，如下图所示，理解这个结构是理解H.264的关键：</p>\n<p><img src=\"https://www.hardening-consulting.com/images/h264_bitstream.png\" alt=\"\"></p>\n<h3 id=\"网络抽象层单元-NAL-Unit\"><a href=\"#网络抽象层单元-NAL-Unit\" class=\"headerlink\" title=\"网络抽象层单元 (NAL Unit)\"></a>网络抽象层单元 (NAL Unit)</h3><p>H.264设计的一个核心思想是<strong>网络友好性</strong>。为了实现这一目标，整个码流被分割成一个个独立的包，称为 <strong>NAL Unit（网络抽象层单元）</strong>。每个NAL Unit都是一个自包含的数据包，包含一个头部和负载数据。这种设计使得H.流非常适合在容易产生包丢失和延迟的网络（如RTP/UDP）中传输，因为一个NAL Unit的丢失通常不会导致整个视频无法解码。</p>\n<h3 id=\"关键概念：帧-Frame-与片-Slice\"><a href=\"#关键概念：帧-Frame-与片-Slice\" class=\"headerlink\" title=\"关键概念：帧 (Frame) 与片 (Slice)\"></a>关键概念：帧 (Frame) 与片 (Slice)</h3><p>在视频编码中，一<strong>帧（Frame）</strong> 通常对应一张静态图片。H.264对一帧图像进行编码后，其数据可能会被装进<strong>一个或多个NAL Unit</strong>中。</p>\n<p>为什么是一或多个？这是因为一帧数据可以被分割成多个<strong>片（Slice）</strong>。每个Slice都是一个独立的编码单元，包含了一帧图像中的一部分宏块（Macroblock）。将一帧分割成多个Slice主要有两个好处：</p>\n<ol>\n<li><strong>错误恢复</strong>：在网络传输中，如果一个Slice丢失了，解码器仍然可以利用错误隐藏技术来近似恢复图像，而不是丢失整帧。</li>\n<li><strong>并行处理</strong>：多个Slice可以并行编码或解码，提高效率。</li>\n</ol>\n<h2 id=\"微观结构：NAL-Unit的内部世界\"><a href=\"#微观结构：NAL-Unit的内部世界\" class=\"headerlink\" title=\"微观结构：NAL Unit的内部世界\"></a>微观结构：NAL Unit的内部世界</h2><p>现在，让我们打开一个NAL Unit，看看它里面到底有什么。</p>\n<h3 id=\"NAL-Unit-Header（头部）\"><a href=\"#NAL-Unit-Header（头部）\" class=\"headerlink\" title=\"NAL Unit Header（头部）\"></a>NAL Unit Header（头部）</h3><p>每个NAL Unit都以一个1字节（可扩展为2字节）的头部开始。这个头部虽然小，但信息量巨大：</p>\n<ul>\n<li><strong>禁止位（F）</strong>：通常为0，如果为1表示该单元出错。</li>\n<li><strong>重要性指示位（NRI）</strong>：表示这个NAL Unit的重要性。值越大，解码器越需要优先保护它（如SPS/PPS的NRI值最高）。</li>\n<li><strong>类型（Type）</strong>：这是最关键的部分！它定义了该单元负载数据的类型。主要分为两大类：<ul>\n<li><strong>VCL（视频编码层）单元</strong>：真正携带编码视频数据的单元（如Slice）。</li>\n<li><strong>Non-VCL（非视频编码层）单元</strong>：携带元数据和控制信息的单元，是解码的“说明书”。</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"NAL-Unit-Payload（负载）\"><a href=\"#NAL-Unit-Payload（负载）\" class=\"headerlink\" title=\"NAL Unit Payload（负载）\"></a>NAL Unit Payload（负载）</h3><p>负载部分的数据内容完全由头部中的<strong>类型（Type）</strong> 决定。</p>\n<h4 id=\"关键的Non-VCL单元（元数据）\"><a href=\"#关键的Non-VCL单元（元数据）\" class=\"headerlink\" title=\"关键的Non-VCL单元（元数据）\"></a><strong>关键的Non-VCL单元（元数据）</strong></h4><p>这些单元不包含图像像素数据，但<strong>没有它们，VCL单元根本无法被解码</strong>。它们通常在视频流开始时发送一次，但如果解码器中途加入，也需要重新获取。</p>\n<p>  <strong>SPS（序列参数集 - Type 7）</strong></p>\n<pre><code>*   **作用**：包含了适用于**整个视频序列**的全局参数。它是解码器的“总纲”。\n*   **包含信息**：视频的档次、级别、分辨率（`pic_width_in_mbs_minus1`等）、帧率、色深、比特深度等。没有SPS，解码器连图像该解码成多大都不知道。\n</code></pre><p>   <strong>PPS（图像参数集 - Type 8）</strong></p>\n<pre><code>*   **作用**：包含了适用于**一幅或多幅图像**的解码参数。它更像是“章节细则”。\n*   **包含信息**：熵编码模式（CAVLC或CABAC）、量化参数等。PPS可以改变，从而在序列中实现不同的编码配置。\n</code></pre><p>   <strong>IDR（即时解码刷新 - 属于VCL，但特殊）</strong></p>\n<pre><code>*   **作用**：一个特殊的Slice（通常是I-Slice），它告诉解码器：“从这里开始，可以独立解码，不再需要参考之前的帧了。”\n*   **意义**：IDR帧是**随机访问和 seeking 的关键点**。当你拖动视频进度条时，播放器总是在寻找最近的IDR帧开始解码，因为它能清空之前的参考帧缓冲区，保证解码正确。\n</code></pre><h4 id=\"VCL单元（核心数据）\"><a href=\"#VCL单元（核心数据）\" class=\"headerlink\" title=\"VCL单元（核心数据）\"></a><strong>VCL单元（核心数据）</strong></h4><p>这些单元携带了实际的压缩视频数据，即Slice。</p>\n<p>  <strong>Slice Header（切片头）</strong></p>\n<pre><code>*   每个Slice都有自己的头，其中包含了当前Slice解码所需的**信息**：\n    *   引用哪个PPS（从而间接引用SPS）。\n    *   帧类型（I, P, B）。\n    *   量化参数。\n    *   根据帧类型，包含运动向量预测所需的信息。\n</code></pre><p>   <strong>Slice Data（切片数据）</strong></p>\n<pre><code>*   这是压缩数据的核心，由一系列**宏块（Macroblock）** 组成。\n*   **宏块**通常是16x16像素的编码单元，它包含了：\n    *   **预测信息**：对于I帧，是帧内预测模式；对于P/B帧，是运动向量（描述当前块是从参考帧的哪个位置移动过来的）。\n    *   **残差数据**：经过预测后，当前块与预测块之间的差值。这部分数据会经过**变换（DCT）、量化、熵编码（CAVLC/CABAC）**，从而获得极高的压缩率。\n</code></pre><h2 id=\"特殊设计点\"><a href=\"#特殊设计点\" class=\"headerlink\" title=\"特殊设计点\"></a>特殊设计点</h2><h3 id=\"3-1-参数集（SPS-PPS）机制\"><a href=\"#3-1-参数集（SPS-PPS）机制\" class=\"headerlink\" title=\"3.1 参数集（SPS/PPS）机制\"></a>3.1 参数集（SPS/PPS）机制</h3><p>这是H.264一个非常巧妙的设计。它将<strong>很少改变但至关重要的信息</strong>（SPS/PPS）与<strong>频繁变化的数据</strong>（Slice）分离开。</p>\n<ul>\n<li><strong>优点一：鲁棒性</strong>：即使丢失了一些Slice，只要SPS/PPS还在，解码器就能继续工作。</li>\n<li><strong>优点二：效率</strong>：无需在每一个Slice中都重复这些头部信息，大大节省了码流。</li>\n<li><strong>优点三：灵活性</strong>：一个码流中可以存在多个PPS，并在不同场景下切换使用。</li>\n</ul>\n<h3 id=\"3-2-I-P-B帧与GOP（图像组）\"><a href=\"#3-2-I-P-B帧与GOP（图像组）\" class=\"headerlink\" title=\"3.2 I, P, B帧与GOP（图像组）\"></a>3.2 I, P, B帧与GOP（图像组）</h3><ul>\n<li><strong>I帧（Intra）</strong>：自包含帧，仅使用本帧内的信息进行编码，不参考其他帧。它是压缩率最低但最关键的帧，是P帧和B帧的锚点。</li>\n<li><strong>P帧（Predicted）</strong>：参考前面的I帧或P帧进行运动补偿预测编码，压缩率高于I帧。</li>\n<li><strong>B帧（Bi-directional）</strong>：可以同时参考前面和后面的帧，获得最高的压缩率，但会带来编码延迟。</li>\n<li>一个<strong>GOP</strong>就是从上一个IDR帧到下一个IDR帧之前的所有帧序列。GOP长度越长，B/P帧越多，压缩率越高，但随机访问的间隔也越长。</li>\n</ul>\n<h3 id=\"3-3-熵编码：CAVLC-与-CABAC\"><a href=\"#3-3-熵编码：CAVLC-与-CABAC\" class=\"headerlink\" title=\"3.3 熵编码：CAVLC 与 CABAC\"></a>3.3 熵编码：CAVLC 与 CABAC</h3><p>这是压缩过程中的最后一步，将数据转换为二进制码流。</p>\n<ul>\n<li><strong>CAVLC（上下文自适应变长编码）</strong>：相对简单，压缩效率一般，用于Baseline等档次。</li>\n<li><strong>CABAC（上下文自适应二进制算术编码）</strong>：非常复杂，但压缩效率比CAVLC高出10%-20%，是Main和High档次效率高的主要原因之一。</li>\n</ul>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>H.264的结构是一个分层、模块化的杰作：</p>\n<ol>\n<li><strong>整体</strong>：码流由一个个<strong>NAL Unit</strong>组成，适合网络传输。</li>\n<li><strong>局部</strong>：NAL Unit分为<strong>VCL</strong>（携带Slice数据）和<strong>Non-VCL</strong>（携带SPS/PPS等元数据）。</li>\n<li><strong>核心</strong>：Slice数据由<strong>宏块</strong>组成，宏块包含了<strong>预测信息</strong>和<strong>残差数据</strong>，通过预测和变换编码实现压缩。</li>\n<li><strong>精妙设计</strong>：<strong>参数集分离</strong>、<strong>IDR帧</strong>、<strong>Slice划分</strong>和<strong>CABAC</strong>等特性共同造就了H.264在效率、鲁棒性和灵活性上的完美平衡。</li>\n</ol>\n<p>理解H.264的结构，不仅能帮助我们更好地处理视频数据（如封装、传输、解码问题定位），更能让我们体会到工程师们在标准制定中的智慧和远见。尽管如今H.265/HEVC、AV1等更先进的编码器已经出现，但H.264的基本设计思想和结构仍然深刻地影响着它们。</p>\n","categories":[],"tags":[{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2025/09/15/H264码流结构理解整理.json"},{"title":"实现一个自定义 FFmpeg Filter","slug":"实现一个自定义FFmpeg-Filter","date":"2024-03-07T02:58:00.000Z","updated":"2025-09-15T13:07:48.846Z","comments":true,"url":"2024/03/07/实现一个自定义FFmpeg-Filter/","excerpt":"<p>此前在做  ffmpeg+某个第三库作为 filter 的集成，第三库是做AE特效相关的，与 ffmpeg 结合能让视频渲染效果大大提升。整体流程将第三方库作为 ffmpeg 的一个filter 形式进行结合，其中就涉及到 ffmpeg 的 filter 开发，本文即 对ffmpeg 的滤镜开发流程作一个总结。本文以实现一个视频垂直翻转的 filter 为例，ffmpeg 源码基于<a href=\"https://github.com/FFmpeg/FFmpeg/tree/release/6.1\">FFmpeg6.1</a> </p>\n<h2 id=\"实现自定义-Filter-流程\"><a href=\"#实现自定义-Filter-流程\" class=\"headerlink\" title=\"实现自定义 Filter 流程\"></a>实现自定义 Filter 流程</h2><ul>\n<li><p>编写 filter.c 文件</p>\n<p>一般视频滤镜以 vf_ 为前缀，视频滤镜以 af_ 为前缀，放在libavfilter目录下，参考其他 filter 代码逻辑，模块化配置相关参数，本文例以 vf_flip.c 实现视频的上下翻转</p>\n</li>\n<li><p>在 <code>libavfilter/allfilters.c</code> 注册</p>\n<p>例如：extern const AVFilter ff_vf_flip;  <code>ff_vf_flip</code>就是在 <code>vf_flip.c</code>的 filter 注册名称</p>\n</li>\n<li><p>修改 <code>libavfilter/Makefile</code> 添加编译配置： </p>\n<p>例如：OBJS-$(CONFIG_FLIP_FILTER)                   += vf_flip.o</p>\n</li>\n<li><p>编译打包</p>\n</li>\n</ul>\n<h2 id=\"编写-filter-c-文件\"><a href=\"#编写-filter-c-文件\" class=\"headerlink\" title=\"编写 filter.c 文件\"></a>编写 filter.c 文件</h2><h3 id=\"AVFilter主体\"><a href=\"#AVFilter主体\" class=\"headerlink\" title=\"AVFilter主体\"></a>AVFilter主体</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">AVFilter</span> &#123;</span></span><br><span class=\"line\">  <span class=\"type\">const</span> <span class=\"type\">char</span> *name;</span><br><span class=\"line\">  <span class=\"type\">const</span> <span class=\"type\">char</span> *description;</span><br><span class=\"line\">  <span class=\"type\">const</span> AVFilterPad *inputs;</span><br><span class=\"line\">  <span class=\"type\">const</span> AVFilterPad *outputs;</span><br><span class=\"line\">  <span class=\"type\">const</span> AVClass *priv_class;</span><br><span class=\"line\">  <span class=\"type\">int</span> flags;</span><br><span class=\"line\">  <span class=\"type\">int</span> (*preinit)(AVFilterContext *ctx);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*init)(AVFilterContext *ctx);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*init_dict)(AVFilterContext *ctx, AVDictionary **options);</span><br><span class=\"line\">  <span class=\"type\">void</span> (*uninit)(AVFilterContext *ctx);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*query_formats)(AVFilterContext *);</span><br><span class=\"line\">  <span class=\"type\">int</span> priv_size;   </span><br><span class=\"line\">  <span class=\"type\">int</span> flags_internal; </span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">AVFilter</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">  <span class=\"type\">int</span> (*process_command)(AVFilterContext *, <span class=\"type\">const</span> <span class=\"type\">char</span> *cmd, <span class=\"type\">const</span> <span class=\"type\">char</span> *arg, <span class=\"type\">char</span> *res, <span class=\"type\">int</span> res_len, <span class=\"type\">int</span> flags);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*init_opaque)(AVFilterContext *ctx, <span class=\"type\">void</span> *opaque);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*activate)(AVFilterContext *ctx);</span><br><span class=\"line\">&#125; AVFilter;</span><br></pre></td></tr></table></figure>\n<p>具体里面的属性作用可以参考：<a href=\"https://www.cnblogs.com/TaigaCon/p/10171464.html\">[ffmpeg] 定制滤波器</a>，可以根据需求实现里面的相关函数，接下来以一个最简单的 Filter 和一个较复杂一点的 Filter 举例。</p>\n<h3 id=\"最简单的-AVFilter\"><a href=\"#最简单的-AVFilter\" class=\"headerlink\" title=\"最简单的 AVFilter\"></a>最简单的 AVFilter</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">const</span> AVClass *<span class=\"class\"><span class=\"keyword\">class</span>;</span></span><br><span class=\"line\">&#125; NoopContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">filter_frame</span><span class=\"params\">(AVFilterLink *link, AVFrame *frame)</span> &#123;</span><br><span class=\"line\">    av_log(<span class=\"literal\">NULL</span>, AV_LOG_INFO, <span class=\"string\">&quot;filter frame pts:%lld\\n&quot;</span>, frame-&gt;pts);</span><br><span class=\"line\">    NoopContext *noopContext = link-&gt;dst-&gt;priv;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ff_filter_frame(link-&gt;dst-&gt;outputs[<span class=\"number\">0</span>], frame);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad noop_inputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name         = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type         = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">                .filter_frame = filter_frame,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad noop_outputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">const</span> AVFilter ff_vf_noop = &#123;</span><br><span class=\"line\">        .name          = <span class=\"string\">&quot;noop&quot;</span>,</span><br><span class=\"line\">        .description   = NULL_IF_CONFIG_SMALL(<span class=\"string\">&quot;Pass the input video unchanged.&quot;</span>),</span><br><span class=\"line\">        .priv_size     = <span class=\"keyword\">sizeof</span>(NoopContext),</span><br><span class=\"line\">        FILTER_INPUTS(noop_inputs),</span><br><span class=\"line\">        FILTER_OUTPUTS(noop_outputs),</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>命令行运行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">ffmpeg -i test.mp4 -vf &quot;noop&quot; noop.mp4</span><br></pre></td></tr></table></figure>\n<p> 正常输出文件（对原片没有做任何更改）,这个 filter 的作用是将输入的视频帧不做任何处理地传递给下一个过滤器，在处理每帧的时候会打印处理的 PTS，麻雀虽小五脏俱全，它包含了一个 AVFilter 基础的结构：</p>\n<ol>\n<li><p><strong><code>NoopContext</code></strong></p>\n<p>这是一个简单的结构体，包含一个指向 AVClass 的指针。在这个例子中，实际上没有使用到 NoopContext 结构体的任何成员，因为这个过滤器没有需要存储的私有数据。</p>\n</li>\n<li><p><strong><code>filter_frame</code></strong> </p>\n<p>这个函数的作用是处理输入的视频帧。在这个例子中，它只是打印帧的 PTS（Presentation Time Stamp，显示时间戳）并将帧传递给下一个过滤器，不对帧做任何修改。</p>\n</li>\n<li><p><strong><code>noop_inputs</code> 和 <code>noop_outputs</code></strong></p>\n<p>这两个数组定义了过滤器的输入和输出 Pad。在这个例子中，输入 Pad 类型为 AVMEDIA_TYPE_VIDEO，并关联了 <code>filter_frame</code> 函数。输出 Pad 也是 AVMEDIA_TYPE_VIDEO 类型，但没有关联任何函数，因为输出直接由 <code>filter_frame</code> 函数处理。</p>\n</li>\n<li><p><strong><code>ff_vf_noop</code></strong></p>\n<p>这是一个 AVFilter 结构体实例，包含了过滤器的名称、描述、私有数据大小以及输入和输出 Pad。在这个例子中，过滤器的名称为 “noop”，描述为 “Pass the input video unchanged.”，这也就是在执行：<code>ffmpeg -filters</code> 看到的 Filter描述内容。</p>\n</li>\n</ol>\n<p>接下来看一个稍微复杂的一个 AVFilter，实现一个视频的上下翻转</p>\n<h3 id=\"复杂一点的-AVFilter\"><a href=\"#复杂一点的-AVFilter\" class=\"headerlink\" title=\"复杂一点的 AVFilter\"></a>复杂一点的 AVFilter</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">FlipContext</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">const</span> AVClass *<span class=\"class\"><span class=\"keyword\">class</span>;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> duration;</span><br><span class=\"line\">&#125; FlipContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OFFSET(x) offsetof(FlipContext, x)</span></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVOption flip_options[] = &#123;</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;duration&quot;</span>, <span class=\"string\">&quot;set flip duration&quot;</span>, OFFSET(duration), AV_OPT_TYPE_INT, &#123;.i64 = <span class=\"number\">0</span>&#125;, <span class=\"number\">0</span>, INT_MAX, .flags = AV_OPT_FLAG_FILTERING_PARAM&#125;,</span><br><span class=\"line\">        &#123;<span class=\"literal\">NULL</span>&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> av_cold <span class=\"type\">int</span> <span class=\"title function_\">flip_init</span><span class=\"params\">(AVFilterContext *ctx)</span> &#123;</span><br><span class=\"line\">    FlipContext *context = ctx-&gt;priv;</span><br><span class=\"line\">    av_log(<span class=\"literal\">NULL</span>, AV_LOG_ERROR, <span class=\"string\">&quot;Input duration: %d.\\n&quot;</span>, context-&gt;duration);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> av_cold <span class=\"type\">void</span> <span class=\"title function_\">flip_uninit</span><span class=\"params\">(AVFilterContext *ctx)</span> &#123;</span><br><span class=\"line\">    FlipContext *context = ctx-&gt;priv;</span><br><span class=\"line\">    <span class=\"comment\">// no-op 本例无需释放滤镜实例分配的内存、关闭文件、资源等</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 对输入的 AVFrame 进行翻转</span></span><br><span class=\"line\"><span class=\"type\">static</span> AVFrame *<span class=\"title function_\">flip_frame</span><span class=\"params\">(AVFilterContext *ctx, AVFrame *in_frame)</span> &#123;</span><br><span class=\"line\"> \t\tAVFilterLink *inlink = ctx-&gt;inputs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    FlipContext *s = ctx-&gt;priv;</span><br><span class=\"line\">    <span class=\"type\">int64_t</span> pts = in_frame-&gt;pts;</span><br><span class=\"line\">    <span class=\"comment\">// 将时间戳（pts）转化以秒为单位的时间戳</span></span><br><span class=\"line\">    <span class=\"type\">float</span> time_s = TS2T(pts, inlink-&gt;time_base);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (time_s &gt; s-&gt;duration) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 超过对应的时间则直接输出in_frame</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> in_frame;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 创建输出帧并分配内存</span></span><br><span class=\"line\">    AVFrame *out_frame = av_frame_alloc();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!out_frame) &#123;</span><br><span class=\"line\">        av_frame_free(&amp;in_frame);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out_frame;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 设置输出帧的属性</span></span><br><span class=\"line\">    out_frame-&gt;format = in_frame-&gt;format;</span><br><span class=\"line\">    out_frame-&gt;width = in_frame-&gt;width;</span><br><span class=\"line\">    out_frame-&gt;height = in_frame-&gt;height;</span><br><span class=\"line\">    out_frame-&gt;pts = in_frame-&gt;pts;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 分配输出帧的数据缓冲区</span></span><br><span class=\"line\">    <span class=\"type\">int</span> ret = av_frame_get_buffer(out_frame, <span class=\"number\">32</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        av_frame_free(&amp;in_frame);</span><br><span class=\"line\">        av_frame_free(&amp;out_frame);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out_frame;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 这个示例仅适用于 YUV 格式的视频。对于其他格式（如 RGB）</span></span><br><span class=\"line\">    <span class=\"comment\">// 翻转输入帧的数据到输出帧</span></span><br><span class=\"line\">    <span class=\"comment\">// 翻转了 Y 分量，然后翻转了 U 和 V 分量</span></span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *src_y = in_frame-&gt;data[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *src_u = in_frame-&gt;data[<span class=\"number\">1</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *src_v = in_frame-&gt;data[<span class=\"number\">2</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *dst_y = out_frame-&gt;data[<span class=\"number\">0</span>] + (in_frame-&gt;height - <span class=\"number\">1</span>) * out_frame-&gt;linesize[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *dst_u = out_frame-&gt;data[<span class=\"number\">1</span>] + (in_frame-&gt;height / <span class=\"number\">2</span> - <span class=\"number\">1</span>) * out_frame-&gt;linesize[<span class=\"number\">1</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *dst_v = out_frame-&gt;data[<span class=\"number\">2</span>] + (in_frame-&gt;height / <span class=\"number\">2</span> - <span class=\"number\">1</span>) * out_frame-&gt;linesize[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; in_frame-&gt;height; i++) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">memcpy</span>(dst_y, src_y, in_frame-&gt;width);</span><br><span class=\"line\">        src_y += in_frame-&gt;linesize[<span class=\"number\">0</span>];</span><br><span class=\"line\">        dst_y -= out_frame-&gt;linesize[<span class=\"number\">0</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; in_frame-&gt;height / <span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">            <span class=\"built_in\">memcpy</span>(dst_u, src_u, in_frame-&gt;width / <span class=\"number\">2</span>);</span><br><span class=\"line\">            <span class=\"built_in\">memcpy</span>(dst_v, src_v, in_frame-&gt;width / <span class=\"number\">2</span>);</span><br><span class=\"line\">            src_u += in_frame-&gt;linesize[<span class=\"number\">1</span>];</span><br><span class=\"line\">            src_v += in_frame-&gt;linesize[<span class=\"number\">2</span>];</span><br><span class=\"line\">            dst_u -= out_frame-&gt;linesize[<span class=\"number\">1</span>];</span><br><span class=\"line\">            dst_v -= out_frame-&gt;linesize[<span class=\"number\">2</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> out_frame;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">activate</span><span class=\"params\">(AVFilterContext *ctx)</span> &#123;</span><br><span class=\"line\">    AVFilterLink *inlink = ctx-&gt;inputs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    AVFilterLink *outlink = ctx-&gt;outputs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    AVFrame *in_frame = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    AVFrame *out_frame = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> ret = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 获取输入帧</span></span><br><span class=\"line\">    ret = ff_inlink_consume_frame(inlink, &amp;in_frame);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 如果有输入帧，进行翻转处理</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (in_frame) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 对输出帧进行上下翻转处理</span></span><br><span class=\"line\">        out_frame = flip_frame(ctx, in_frame);</span><br><span class=\"line\">        <span class=\"comment\">// 将处理后的帧放入输出缓冲区</span></span><br><span class=\"line\">        ret = ff_filter_frame(outlink, out_frame);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            av_frame_free(&amp;out_frame);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 如果没有输入帧，尝试请求一个新的输入帧</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!in_frame) &#123;</span><br><span class=\"line\">        ff_inlink_request_frame(inlink);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> status;</span><br><span class=\"line\">    <span class=\"type\">int64_t</span> pts;</span><br><span class=\"line\">    ret = ff_inlink_acknowledge_status(inlink, &amp;status, &amp;pts);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (status == AVERROR_EOF) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 输入链接已经结束，设置输出链接的状态为 EOF</span></span><br><span class=\"line\">        ff_outlink_set_status(outlink, AVERROR_EOF, pts);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">AVFILTER_DEFINE_CLASS(flip);</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad flip_inputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad flip_outputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">const</span> AVFilter ff_vf_flip = &#123;</span><br><span class=\"line\">        .name = <span class=\"string\">&quot;flip&quot;</span>,</span><br><span class=\"line\">        .description = NULL_IF_CONFIG_SMALL(<span class=\"string\">&quot;Flip the input video.&quot;</span>),</span><br><span class=\"line\">        .priv_size = <span class=\"keyword\">sizeof</span>(FlipContext),</span><br><span class=\"line\">        .priv_class = &amp;flip_class,</span><br><span class=\"line\">        .activate      = activate,</span><br><span class=\"line\">        .init = flip_init,</span><br><span class=\"line\">        .uninit = flip_uninit,</span><br><span class=\"line\">        FILTER_INPUTS(flip_inputs),</span><br><span class=\"line\">        FILTER_OUTPUTS(flip_outputs),</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>命令行运行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">ffmpeg -i test.mp4 -filter_complex &quot;[0:v]flip=duration=5[out];&quot; -map &quot;[out]&quot; flip.mp4</span><br></pre></td></tr></table></figure>\n<p> 得到渲染好的视频，前5s是上下翻转的，后面的内容正常。</p>\n<p>相比于最简单的 AVFilter 多了几个实现：</p>\n<ol>\n<li><p><strong><code>AVOption flip_options</code></strong></p>\n<p>用于设置翻转持续时间的选项，外部命令配置可选输入<code>duration=5</code>，会自动对数据合法性进行校验。参数类型为 <code>AV_OPT_TYPE_INT</code>，默认值为 0，取值范围为 0 到 <code>INT_MAX</code>。<code>.flags</code> 设置为 <code>AV_OPT_FLAG_FILTERING_PARAM</code>，表示这是一个过滤参数。</p>\n</li>\n<li><p><strong><code>.priv_class</code></strong>  </p>\n<p>配置的<code>flip_class</code>实际是通过 <code>AVFILTER_DEFINE_CLASS(flip);</code> 宏实现的一个声明：见：<a href=\"https://github.com/FFmpeg/FFmpeg/blob/release/6.1/libavfilter/internal.h#L311\">internal.h#AVFILTER_DEFINE_CLASS_EXT</a></p>\n</li>\n<li><p><strong>`</strong>init<code>&amp;</code>uninit`**</p>\n<p>滤镜在初始化或者释放资源的时候将会调用</p>\n</li>\n<li><p><strong><code>activate</code></strong></p>\n<p>这个函数首先获取输入帧，然后调用 <code>flip_frame</code> 函数进行翻转操作，并将处理后的帧放入输出链接。如果没有输入帧，它会请求一个新的输入帧。最后，它会确认输入链接的状态，并根据需要设置输出链接的状态。</p>\n</li>\n</ol>\n<p>这个例子相比最简单的 filter 使用了 <code>activate</code> 函数 用于帧渲染，而不是使用 <code>filter_frame</code>去渲染，这两个方法有什么区别于联系呢？查看：<a href=\"##filter_frame(\">filter_frame和activate方法</a>和activate()函数)</p>\n<p>也能通过 <code>filter_frame</code>实现，对代码部分逻辑更新更改：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad flip_inputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">                .filter_frame = filter_frame, <span class=\"comment\">//添加filter_frame 实现</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">const</span> AVFilter ff_vf_flip = &#123;</span><br><span class=\"line\">       ……</span><br><span class=\"line\">        .priv_class = &amp;flip_class,</span><br><span class=\"line\">       <span class=\"comment\">// .activate      = activate,</span></span><br><span class=\"line\">        .init = flip_init,</span><br><span class=\"line\">       ……</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">filter_frame</span><span class=\"params\">(AVFilterLink *inlink, AVFrame *in)</span> &#123;</span><br><span class=\"line\">    AVFilterContext *ctx = inlink-&gt;dst;</span><br><span class=\"line\">    FlipContext *s = ctx-&gt;priv;</span><br><span class=\"line\">    AVFilterLink *outlink = ctx-&gt;outputs[<span class=\"number\">0</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int64_t</span> pts = in-&gt;pts;</span><br><span class=\"line\">    <span class=\"comment\">// 将时间戳（pts）转化以秒为单位的时间戳</span></span><br><span class=\"line\">    <span class=\"type\">float</span> time_s = TS2T(pts, inlink-&gt;time_base);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (time_s &gt; s-&gt;duration) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 超过对应的时间则直接输出in_frame</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ff_filter_frame(outlink, in);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        av_log(<span class=\"literal\">NULL</span>, AV_LOG_ERROR, <span class=\"string\">&quot;time_s s: %f.\\n&quot;</span>, time_s);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    AVFrame *out = flip_frame(ctx, in);</span><br><span class=\"line\">    <span class=\"comment\">// 释放输入帧</span></span><br><span class=\"line\">    av_frame_free(&amp;in);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将输出帧传递给下一个滤镜</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ff_filter_frame(outlink, out);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>命令行运行，得到的输出结果是一样的。</p>\n<h2 id=\"filter-frame-和activate-函数\"><a href=\"#filter-frame-和activate-函数\" class=\"headerlink\" title=\"filter_frame()和activate()函数\"></a>filter_frame()和activate()函数</h2><p>对于这点查了相关资料，看看源码相关的实现</p>\n<p>参考：<a href=\"https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html\">https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html</a></p>\n<blockquote>\n<p>The purpose of these rules is to ensure that frames flow in the filter graph without getting stuck and accumulating somewhere. Simple filters that output one frame for each input frame should not have to worry about it. There are two design for filters:one using the  <a href=\"https://www.ffmpeg.org/doxygen/5.0/vsink__nullsink_8c.html#aaa9a0e0f9de1464941d86a984cf77d37\">filter_frame</a>() and <a href=\"https://www.ffmpeg.org/doxygen/5.0/vsrc__mptestsrc_8c.html#a72949c8fcad3f201712a3569fc6888cb\">request_frame</a>() callbacks and the other using the activate() callback. The design using filter_frame() and request_frame() is legacy, but it is suitable for filters that have a single input and process one frame at a time. New filters with several inputs, that treat several frames at a time or that require a special treatment at EOF should probably use the design using activate(). activate ———— This method is called when something must be done in a filter</p>\n</blockquote>\n<p>大意，实现滤镜有两种实现方式：</p>\n<ul>\n<li><p><strong><code>filter_frame()</code></strong></p>\n<p>可以被认为是历史遗留产物。在早期的 AVFilter 设计中，<code>filter_frame()</code> 和 <code>request_frame()</code> 是主要用于处理输入帧和请求输出帧的回调函数。这种设计适用于简单的过滤器，例如单输入且每次处理一个帧的过滤器。</p>\n</li>\n<li><p><strong><code>activate()</code></strong></p>\n<p>随着 ffmpeg 和 AVFilter 的发展，处理需求变得越来越复杂，例如需要处理多个输入、一次处理多个帧或在文件结束（EOF）时进行特殊处理等。为了满足这些需求，引入了 <code>activate()</code> 函数，它提供了更灵活和强大的处理能力。因此，虽然 <code>filter_frame()</code> 在某些简单场景下仍然可以使用，但对于新的或复杂的过滤器，建议使用 <code>activate()</code> 函数。</p>\n</li>\n</ul>\n<p>如果两个方法都实现了，那他们谁会先执行呢？</p>\n<p>对应的源码处理逻辑： <a href=\"https://github.com/FFmpeg/FFmpeg/blob/release/6.1/libavfilter/avfilter.c#L1322\">avfilter.c</a></p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ff_filter_activate</span><span class=\"params\">(AVFilterContext *filter)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> ret;</span><br><span class=\"line\">\t\t……</span><br><span class=\"line\">    ret = filter-&gt;filter-&gt;activate ? filter-&gt;filter-&gt;activate(filter) :</span><br><span class=\"line\">          ff_filter_activate_default(filter);</span><br><span class=\"line\">  \t……</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果配置了activate() 函数则执行，否则执行 ff_filter_activate_default()-&gt;ff_filter_frame_to_filter()-&gt;ff_filter_frame_framed() 最终执行到配置的 filter_frame() 方法。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">ff_filter_frame_framed</span><span class=\"params\">(AVFilterLink *link, AVFrame *frame)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> (*filter_frame)(AVFilterLink *, AVFrame *);</span><br><span class=\"line\">    AVFilterContext *dstctx = link-&gt;dst;</span><br><span class=\"line\">    AVFilterPad *dst = link-&gt;dstpad;</span><br><span class=\"line\">    <span class=\"type\">int</span> ret;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!(filter_frame = dst-&gt;filter_frame))</span><br><span class=\"line\">        filter_frame = default_filter_frame;</span><br><span class=\"line\">    ……</span><br><span class=\"line\">    ret = filter_frame(link, frame);  <span class=\"comment\">// 最终调用到的地方</span></span><br><span class=\"line\">    link-&gt;frame_count_out++;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">fail:</span><br><span class=\"line\">    ……</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文介绍了 FFmpeg 滤镜开发的整体流程，如何编写 filter.c 文件，并以一个最简单的 AVFilter 和一个较为复杂的 AVFilter 为例，解析了滤镜开发的具体步骤和代码实现，并介绍了 filter_frame() 和 activate() 函数的区别与联系。</p>\n<p>在滤镜开发过程中，需要注意的是，filter_frame() 和 activate() 函数的使用取决于滤镜的复杂性。对于简单的滤镜，可以使用 filter_frame() 函数；而对于需要处理多个输入、一次处理多个帧或在文件结束（EOF）时进行特殊处理的复杂滤镜，建议使用 activate() 函数。</p>\n<p>文中的源码可以查看：<a href=\"https://github.com/VomPom/FFmpeg/commit/9176f58ae60e0b70e5708b25017f374deac9fae7\">add most simplest  AVFilter and a simple video flip filter.</a></p>\n<h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><p><a href=\"https://www.cnblogs.com/TaigaCon/p/10171464.html\">https://www.cnblogs.com/TaigaCon/p/10171464.html</a></p>\n<p><a href=\"https://www.cnblogs.com/ranson7zop/p/7728639.html\">https://www.cnblogs.com/ranson7zop/p/7728639.html</a></p>\n<p><a href=\"https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html\">https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html</a></p>\n","cover":null,"images":[],"content":"<p>此前在做  ffmpeg+某个第三库作为 filter 的集成，第三库是做AE特效相关的，与 ffmpeg 结合能让视频渲染效果大大提升。整体流程将第三方库作为 ffmpeg 的一个filter 形式进行结合，其中就涉及到 ffmpeg 的 filter 开发，本文即 对ffmpeg 的滤镜开发流程作一个总结。本文以实现一个视频垂直翻转的 filter 为例，ffmpeg 源码基于<a href=\"https://github.com/FFmpeg/FFmpeg/tree/release/6.1\">FFmpeg6.1</a> </p>\n<h2 id=\"实现自定义-Filter-流程\"><a href=\"#实现自定义-Filter-流程\" class=\"headerlink\" title=\"实现自定义 Filter 流程\"></a>实现自定义 Filter 流程</h2><ul>\n<li><p>编写 filter.c 文件</p>\n<p>一般视频滤镜以 vf_ 为前缀，视频滤镜以 af_ 为前缀，放在libavfilter目录下，参考其他 filter 代码逻辑，模块化配置相关参数，本文例以 vf_flip.c 实现视频的上下翻转</p>\n</li>\n<li><p>在 <code>libavfilter/allfilters.c</code> 注册</p>\n<p>例如：extern const AVFilter ff_vf_flip;  <code>ff_vf_flip</code>就是在 <code>vf_flip.c</code>的 filter 注册名称</p>\n</li>\n<li><p>修改 <code>libavfilter/Makefile</code> 添加编译配置： </p>\n<p>例如：OBJS-$(CONFIG_FLIP_FILTER)                   += vf_flip.o</p>\n</li>\n<li><p>编译打包</p>\n</li>\n</ul>\n<h2 id=\"编写-filter-c-文件\"><a href=\"#编写-filter-c-文件\" class=\"headerlink\" title=\"编写 filter.c 文件\"></a>编写 filter.c 文件</h2><h3 id=\"AVFilter主体\"><a href=\"#AVFilter主体\" class=\"headerlink\" title=\"AVFilter主体\"></a>AVFilter主体</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">AVFilter</span> &#123;</span></span><br><span class=\"line\">  <span class=\"type\">const</span> <span class=\"type\">char</span> *name;</span><br><span class=\"line\">  <span class=\"type\">const</span> <span class=\"type\">char</span> *description;</span><br><span class=\"line\">  <span class=\"type\">const</span> AVFilterPad *inputs;</span><br><span class=\"line\">  <span class=\"type\">const</span> AVFilterPad *outputs;</span><br><span class=\"line\">  <span class=\"type\">const</span> AVClass *priv_class;</span><br><span class=\"line\">  <span class=\"type\">int</span> flags;</span><br><span class=\"line\">  <span class=\"type\">int</span> (*preinit)(AVFilterContext *ctx);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*init)(AVFilterContext *ctx);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*init_dict)(AVFilterContext *ctx, AVDictionary **options);</span><br><span class=\"line\">  <span class=\"type\">void</span> (*uninit)(AVFilterContext *ctx);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*query_formats)(AVFilterContext *);</span><br><span class=\"line\">  <span class=\"type\">int</span> priv_size;   </span><br><span class=\"line\">  <span class=\"type\">int</span> flags_internal; </span><br><span class=\"line\">  <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">AVFilter</span> *<span class=\"title\">next</span>;</span></span><br><span class=\"line\">  <span class=\"type\">int</span> (*process_command)(AVFilterContext *, <span class=\"type\">const</span> <span class=\"type\">char</span> *cmd, <span class=\"type\">const</span> <span class=\"type\">char</span> *arg, <span class=\"type\">char</span> *res, <span class=\"type\">int</span> res_len, <span class=\"type\">int</span> flags);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*init_opaque)(AVFilterContext *ctx, <span class=\"type\">void</span> *opaque);</span><br><span class=\"line\">  <span class=\"type\">int</span> (*activate)(AVFilterContext *ctx);</span><br><span class=\"line\">&#125; AVFilter;</span><br></pre></td></tr></table></figure>\n<p>具体里面的属性作用可以参考：<a href=\"https://www.cnblogs.com/TaigaCon/p/10171464.html\">[ffmpeg] 定制滤波器</a>，可以根据需求实现里面的相关函数，接下来以一个最简单的 Filter 和一个较复杂一点的 Filter 举例。</p>\n<h3 id=\"最简单的-AVFilter\"><a href=\"#最简单的-AVFilter\" class=\"headerlink\" title=\"最简单的 AVFilter\"></a>最简单的 AVFilter</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">const</span> AVClass *<span class=\"class\"><span class=\"keyword\">class</span>;</span></span><br><span class=\"line\">&#125; NoopContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">filter_frame</span><span class=\"params\">(AVFilterLink *link, AVFrame *frame)</span> &#123;</span><br><span class=\"line\">    av_log(<span class=\"literal\">NULL</span>, AV_LOG_INFO, <span class=\"string\">&quot;filter frame pts:%lld\\n&quot;</span>, frame-&gt;pts);</span><br><span class=\"line\">    NoopContext *noopContext = link-&gt;dst-&gt;priv;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ff_filter_frame(link-&gt;dst-&gt;outputs[<span class=\"number\">0</span>], frame);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad noop_inputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name         = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type         = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">                .filter_frame = filter_frame,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad noop_outputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">const</span> AVFilter ff_vf_noop = &#123;</span><br><span class=\"line\">        .name          = <span class=\"string\">&quot;noop&quot;</span>,</span><br><span class=\"line\">        .description   = NULL_IF_CONFIG_SMALL(<span class=\"string\">&quot;Pass the input video unchanged.&quot;</span>),</span><br><span class=\"line\">        .priv_size     = <span class=\"keyword\">sizeof</span>(NoopContext),</span><br><span class=\"line\">        FILTER_INPUTS(noop_inputs),</span><br><span class=\"line\">        FILTER_OUTPUTS(noop_outputs),</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>命令行运行：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">ffmpeg -i test.mp4 -vf &quot;noop&quot; noop.mp4</span><br></pre></td></tr></table></figure>\n<p> 正常输出文件（对原片没有做任何更改）,这个 filter 的作用是将输入的视频帧不做任何处理地传递给下一个过滤器，在处理每帧的时候会打印处理的 PTS，麻雀虽小五脏俱全，它包含了一个 AVFilter 基础的结构：</p>\n<ol>\n<li><p><strong><code>NoopContext</code></strong></p>\n<p>这是一个简单的结构体，包含一个指向 AVClass 的指针。在这个例子中，实际上没有使用到 NoopContext 结构体的任何成员，因为这个过滤器没有需要存储的私有数据。</p>\n</li>\n<li><p><strong><code>filter_frame</code></strong> </p>\n<p>这个函数的作用是处理输入的视频帧。在这个例子中，它只是打印帧的 PTS（Presentation Time Stamp，显示时间戳）并将帧传递给下一个过滤器，不对帧做任何修改。</p>\n</li>\n<li><p><strong><code>noop_inputs</code> 和 <code>noop_outputs</code></strong></p>\n<p>这两个数组定义了过滤器的输入和输出 Pad。在这个例子中，输入 Pad 类型为 AVMEDIA_TYPE_VIDEO，并关联了 <code>filter_frame</code> 函数。输出 Pad 也是 AVMEDIA_TYPE_VIDEO 类型，但没有关联任何函数，因为输出直接由 <code>filter_frame</code> 函数处理。</p>\n</li>\n<li><p><strong><code>ff_vf_noop</code></strong></p>\n<p>这是一个 AVFilter 结构体实例，包含了过滤器的名称、描述、私有数据大小以及输入和输出 Pad。在这个例子中，过滤器的名称为 “noop”，描述为 “Pass the input video unchanged.”，这也就是在执行：<code>ffmpeg -filters</code> 看到的 Filter描述内容。</p>\n</li>\n</ol>\n<p>接下来看一个稍微复杂的一个 AVFilter，实现一个视频的上下翻转</p>\n<h3 id=\"复杂一点的-AVFilter\"><a href=\"#复杂一点的-AVFilter\" class=\"headerlink\" title=\"复杂一点的 AVFilter\"></a>复杂一点的 AVFilter</h3><figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> <span class=\"class\"><span class=\"keyword\">struct</span> <span class=\"title\">FlipContext</span> &#123;</span></span><br><span class=\"line\">    <span class=\"type\">const</span> AVClass *<span class=\"class\"><span class=\"keyword\">class</span>;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> duration;</span><br><span class=\"line\">&#125; FlipContext;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">define</span> OFFSET(x) offsetof(FlipContext, x)</span></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVOption flip_options[] = &#123;</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;duration&quot;</span>, <span class=\"string\">&quot;set flip duration&quot;</span>, OFFSET(duration), AV_OPT_TYPE_INT, &#123;.i64 = <span class=\"number\">0</span>&#125;, <span class=\"number\">0</span>, INT_MAX, .flags = AV_OPT_FLAG_FILTERING_PARAM&#125;,</span><br><span class=\"line\">        &#123;<span class=\"literal\">NULL</span>&#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> av_cold <span class=\"type\">int</span> <span class=\"title function_\">flip_init</span><span class=\"params\">(AVFilterContext *ctx)</span> &#123;</span><br><span class=\"line\">    FlipContext *context = ctx-&gt;priv;</span><br><span class=\"line\">    av_log(<span class=\"literal\">NULL</span>, AV_LOG_ERROR, <span class=\"string\">&quot;Input duration: %d.\\n&quot;</span>, context-&gt;duration);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> av_cold <span class=\"type\">void</span> <span class=\"title function_\">flip_uninit</span><span class=\"params\">(AVFilterContext *ctx)</span> &#123;</span><br><span class=\"line\">    FlipContext *context = ctx-&gt;priv;</span><br><span class=\"line\">    <span class=\"comment\">// no-op 本例无需释放滤镜实例分配的内存、关闭文件、资源等</span></span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 对输入的 AVFrame 进行翻转</span></span><br><span class=\"line\"><span class=\"type\">static</span> AVFrame *<span class=\"title function_\">flip_frame</span><span class=\"params\">(AVFilterContext *ctx, AVFrame *in_frame)</span> &#123;</span><br><span class=\"line\"> \t\tAVFilterLink *inlink = ctx-&gt;inputs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    FlipContext *s = ctx-&gt;priv;</span><br><span class=\"line\">    <span class=\"type\">int64_t</span> pts = in_frame-&gt;pts;</span><br><span class=\"line\">    <span class=\"comment\">// 将时间戳（pts）转化以秒为单位的时间戳</span></span><br><span class=\"line\">    <span class=\"type\">float</span> time_s = TS2T(pts, inlink-&gt;time_base);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (time_s &gt; s-&gt;duration) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 超过对应的时间则直接输出in_frame</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> in_frame;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 创建输出帧并分配内存</span></span><br><span class=\"line\">    AVFrame *out_frame = av_frame_alloc();</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!out_frame) &#123;</span><br><span class=\"line\">        av_frame_free(&amp;in_frame);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out_frame;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 设置输出帧的属性</span></span><br><span class=\"line\">    out_frame-&gt;format = in_frame-&gt;format;</span><br><span class=\"line\">    out_frame-&gt;width = in_frame-&gt;width;</span><br><span class=\"line\">    out_frame-&gt;height = in_frame-&gt;height;</span><br><span class=\"line\">    out_frame-&gt;pts = in_frame-&gt;pts;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 分配输出帧的数据缓冲区</span></span><br><span class=\"line\">    <span class=\"type\">int</span> ret = av_frame_get_buffer(out_frame, <span class=\"number\">32</span>);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        av_frame_free(&amp;in_frame);</span><br><span class=\"line\">        av_frame_free(&amp;out_frame);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> out_frame;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 这个示例仅适用于 YUV 格式的视频。对于其他格式（如 RGB）</span></span><br><span class=\"line\">    <span class=\"comment\">// 翻转输入帧的数据到输出帧</span></span><br><span class=\"line\">    <span class=\"comment\">// 翻转了 Y 分量，然后翻转了 U 和 V 分量</span></span><br><span class=\"line\">    <span class=\"comment\">//</span></span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *src_y = in_frame-&gt;data[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *src_u = in_frame-&gt;data[<span class=\"number\">1</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *src_v = in_frame-&gt;data[<span class=\"number\">2</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *dst_y = out_frame-&gt;data[<span class=\"number\">0</span>] + (in_frame-&gt;height - <span class=\"number\">1</span>) * out_frame-&gt;linesize[<span class=\"number\">0</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *dst_u = out_frame-&gt;data[<span class=\"number\">1</span>] + (in_frame-&gt;height / <span class=\"number\">2</span> - <span class=\"number\">1</span>) * out_frame-&gt;linesize[<span class=\"number\">1</span>];</span><br><span class=\"line\">    <span class=\"type\">uint8_t</span> *dst_v = out_frame-&gt;data[<span class=\"number\">2</span>] + (in_frame-&gt;height / <span class=\"number\">2</span> - <span class=\"number\">1</span>) * out_frame-&gt;linesize[<span class=\"number\">2</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; in_frame-&gt;height; i++) &#123;</span><br><span class=\"line\">        <span class=\"built_in\">memcpy</span>(dst_y, src_y, in_frame-&gt;width);</span><br><span class=\"line\">        src_y += in_frame-&gt;linesize[<span class=\"number\">0</span>];</span><br><span class=\"line\">        dst_y -= out_frame-&gt;linesize[<span class=\"number\">0</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; in_frame-&gt;height / <span class=\"number\">2</span>) &#123;</span><br><span class=\"line\">            <span class=\"built_in\">memcpy</span>(dst_u, src_u, in_frame-&gt;width / <span class=\"number\">2</span>);</span><br><span class=\"line\">            <span class=\"built_in\">memcpy</span>(dst_v, src_v, in_frame-&gt;width / <span class=\"number\">2</span>);</span><br><span class=\"line\">            src_u += in_frame-&gt;linesize[<span class=\"number\">1</span>];</span><br><span class=\"line\">            src_v += in_frame-&gt;linesize[<span class=\"number\">2</span>];</span><br><span class=\"line\">            dst_u -= out_frame-&gt;linesize[<span class=\"number\">1</span>];</span><br><span class=\"line\">            dst_v -= out_frame-&gt;linesize[<span class=\"number\">2</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> out_frame;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">activate</span><span class=\"params\">(AVFilterContext *ctx)</span> &#123;</span><br><span class=\"line\">    AVFilterLink *inlink = ctx-&gt;inputs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    AVFilterLink *outlink = ctx-&gt;outputs[<span class=\"number\">0</span>];</span><br><span class=\"line\">    AVFrame *in_frame = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    AVFrame *out_frame = <span class=\"literal\">NULL</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> ret = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"comment\">// 获取输入帧</span></span><br><span class=\"line\">    ret = ff_inlink_consume_frame(inlink, &amp;in_frame);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 如果有输入帧，进行翻转处理</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (in_frame) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 对输出帧进行上下翻转处理</span></span><br><span class=\"line\">        out_frame = flip_frame(ctx, in_frame);</span><br><span class=\"line\">        <span class=\"comment\">// 将处理后的帧放入输出缓冲区</span></span><br><span class=\"line\">        ret = ff_filter_frame(outlink, out_frame);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>) &#123;</span><br><span class=\"line\">            av_frame_free(&amp;out_frame);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"comment\">// 如果没有输入帧，尝试请求一个新的输入帧</span></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!in_frame) &#123;</span><br><span class=\"line\">        ff_inlink_request_frame(inlink);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> status;</span><br><span class=\"line\">    <span class=\"type\">int64_t</span> pts;</span><br><span class=\"line\">    ret = ff_inlink_acknowledge_status(inlink, &amp;status, &amp;pts);</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (ret &lt; <span class=\"number\">0</span>)</span><br><span class=\"line\">        <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (status == AVERROR_EOF) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 输入链接已经结束，设置输出链接的状态为 EOF</span></span><br><span class=\"line\">        ff_outlink_set_status(outlink, AVERROR_EOF, pts);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">AVFILTER_DEFINE_CLASS(flip);</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad flip_inputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad flip_outputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"><span class=\"type\">const</span> AVFilter ff_vf_flip = &#123;</span><br><span class=\"line\">        .name = <span class=\"string\">&quot;flip&quot;</span>,</span><br><span class=\"line\">        .description = NULL_IF_CONFIG_SMALL(<span class=\"string\">&quot;Flip the input video.&quot;</span>),</span><br><span class=\"line\">        .priv_size = <span class=\"keyword\">sizeof</span>(FlipContext),</span><br><span class=\"line\">        .priv_class = &amp;flip_class,</span><br><span class=\"line\">        .activate      = activate,</span><br><span class=\"line\">        .init = flip_init,</span><br><span class=\"line\">        .uninit = flip_uninit,</span><br><span class=\"line\">        FILTER_INPUTS(flip_inputs),</span><br><span class=\"line\">        FILTER_OUTPUTS(flip_outputs),</span><br><span class=\"line\">&#125;;</span><br></pre></td></tr></table></figure>\n<p>命令行运行：</p>\n<figure class=\"highlight shell\"><table><tr><td class=\"code\"><pre><span class=\"line\">ffmpeg -i test.mp4 -filter_complex &quot;[0:v]flip=duration=5[out];&quot; -map &quot;[out]&quot; flip.mp4</span><br></pre></td></tr></table></figure>\n<p> 得到渲染好的视频，前5s是上下翻转的，后面的内容正常。</p>\n<p>相比于最简单的 AVFilter 多了几个实现：</p>\n<ol>\n<li><p><strong><code>AVOption flip_options</code></strong></p>\n<p>用于设置翻转持续时间的选项，外部命令配置可选输入<code>duration=5</code>，会自动对数据合法性进行校验。参数类型为 <code>AV_OPT_TYPE_INT</code>，默认值为 0，取值范围为 0 到 <code>INT_MAX</code>。<code>.flags</code> 设置为 <code>AV_OPT_FLAG_FILTERING_PARAM</code>，表示这是一个过滤参数。</p>\n</li>\n<li><p><strong><code>.priv_class</code></strong>  </p>\n<p>配置的<code>flip_class</code>实际是通过 <code>AVFILTER_DEFINE_CLASS(flip);</code> 宏实现的一个声明：见：<a href=\"https://github.com/FFmpeg/FFmpeg/blob/release/6.1/libavfilter/internal.h#L311\">internal.h#AVFILTER_DEFINE_CLASS_EXT</a></p>\n</li>\n<li><p><strong>`</strong>init<code>&amp;</code>uninit`**</p>\n<p>滤镜在初始化或者释放资源的时候将会调用</p>\n</li>\n<li><p><strong><code>activate</code></strong></p>\n<p>这个函数首先获取输入帧，然后调用 <code>flip_frame</code> 函数进行翻转操作，并将处理后的帧放入输出链接。如果没有输入帧，它会请求一个新的输入帧。最后，它会确认输入链接的状态，并根据需要设置输出链接的状态。</p>\n</li>\n</ol>\n<p>这个例子相比最简单的 filter 使用了 <code>activate</code> 函数 用于帧渲染，而不是使用 <code>filter_frame</code>去渲染，这两个方法有什么区别于联系呢？查看：<a href=\"##filter_frame(\">filter_frame和activate方法</a>和activate()函数)</p>\n<p>也能通过 <code>filter_frame</code>实现，对代码部分逻辑更新更改：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">const</span> AVFilterPad flip_inputs[] = &#123;</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">                .name = <span class=\"string\">&quot;default&quot;</span>,</span><br><span class=\"line\">                .type = AVMEDIA_TYPE_VIDEO,</span><br><span class=\"line\">                .filter_frame = filter_frame, <span class=\"comment\">//添加filter_frame 实现</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">const</span> AVFilter ff_vf_flip = &#123;</span><br><span class=\"line\">       ……</span><br><span class=\"line\">        .priv_class = &amp;flip_class,</span><br><span class=\"line\">       <span class=\"comment\">// .activate      = activate,</span></span><br><span class=\"line\">        .init = flip_init,</span><br><span class=\"line\">       ……</span><br><span class=\"line\">&#125;;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">filter_frame</span><span class=\"params\">(AVFilterLink *inlink, AVFrame *in)</span> &#123;</span><br><span class=\"line\">    AVFilterContext *ctx = inlink-&gt;dst;</span><br><span class=\"line\">    FlipContext *s = ctx-&gt;priv;</span><br><span class=\"line\">    AVFilterLink *outlink = ctx-&gt;outputs[<span class=\"number\">0</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int64_t</span> pts = in-&gt;pts;</span><br><span class=\"line\">    <span class=\"comment\">// 将时间戳（pts）转化以秒为单位的时间戳</span></span><br><span class=\"line\">    <span class=\"type\">float</span> time_s = TS2T(pts, inlink-&gt;time_base);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (time_s &gt; s-&gt;duration) &#123;</span><br><span class=\"line\">        <span class=\"comment\">// 超过对应的时间则直接输出in_frame</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> ff_filter_frame(outlink, in);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        av_log(<span class=\"literal\">NULL</span>, AV_LOG_ERROR, <span class=\"string\">&quot;time_s s: %f.\\n&quot;</span>, time_s);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    AVFrame *out = flip_frame(ctx, in);</span><br><span class=\"line\">    <span class=\"comment\">// 释放输入帧</span></span><br><span class=\"line\">    av_frame_free(&amp;in);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 将输出帧传递给下一个滤镜</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> ff_filter_frame(outlink, out);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>命令行运行，得到的输出结果是一样的。</p>\n<h2 id=\"filter-frame-和activate-函数\"><a href=\"#filter-frame-和activate-函数\" class=\"headerlink\" title=\"filter_frame()和activate()函数\"></a>filter_frame()和activate()函数</h2><p>对于这点查了相关资料，看看源码相关的实现</p>\n<p>参考：<a href=\"https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html\">https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html</a></p>\n<blockquote>\n<p>The purpose of these rules is to ensure that frames flow in the filter graph without getting stuck and accumulating somewhere. Simple filters that output one frame for each input frame should not have to worry about it. There are two design for filters:one using the  <a href=\"https://www.ffmpeg.org/doxygen/5.0/vsink__nullsink_8c.html#aaa9a0e0f9de1464941d86a984cf77d37\">filter_frame</a>() and <a href=\"https://www.ffmpeg.org/doxygen/5.0/vsrc__mptestsrc_8c.html#a72949c8fcad3f201712a3569fc6888cb\">request_frame</a>() callbacks and the other using the activate() callback. The design using filter_frame() and request_frame() is legacy, but it is suitable for filters that have a single input and process one frame at a time. New filters with several inputs, that treat several frames at a time or that require a special treatment at EOF should probably use the design using activate(). activate ———— This method is called when something must be done in a filter</p>\n</blockquote>\n<p>大意，实现滤镜有两种实现方式：</p>\n<ul>\n<li><p><strong><code>filter_frame()</code></strong></p>\n<p>可以被认为是历史遗留产物。在早期的 AVFilter 设计中，<code>filter_frame()</code> 和 <code>request_frame()</code> 是主要用于处理输入帧和请求输出帧的回调函数。这种设计适用于简单的过滤器，例如单输入且每次处理一个帧的过滤器。</p>\n</li>\n<li><p><strong><code>activate()</code></strong></p>\n<p>随着 ffmpeg 和 AVFilter 的发展，处理需求变得越来越复杂，例如需要处理多个输入、一次处理多个帧或在文件结束（EOF）时进行特殊处理等。为了满足这些需求，引入了 <code>activate()</code> 函数，它提供了更灵活和强大的处理能力。因此，虽然 <code>filter_frame()</code> 在某些简单场景下仍然可以使用，但对于新的或复杂的过滤器，建议使用 <code>activate()</code> 函数。</p>\n</li>\n</ul>\n<p>如果两个方法都实现了，那他们谁会先执行呢？</p>\n<p>对应的源码处理逻辑： <a href=\"https://github.com/FFmpeg/FFmpeg/blob/release/6.1/libavfilter/avfilter.c#L1322\">avfilter.c</a></p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> <span class=\"title function_\">ff_filter_activate</span><span class=\"params\">(AVFilterContext *filter)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> ret;</span><br><span class=\"line\">\t\t……</span><br><span class=\"line\">    ret = filter-&gt;filter-&gt;activate ? filter-&gt;filter-&gt;activate(filter) :</span><br><span class=\"line\">          ff_filter_activate_default(filter);</span><br><span class=\"line\">  \t……</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>如果配置了activate() 函数则执行，否则执行 ff_filter_activate_default()-&gt;ff_filter_frame_to_filter()-&gt;ff_filter_frame_framed() 最终执行到配置的 filter_frame() 方法。</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">ff_filter_frame_framed</span><span class=\"params\">(AVFilterLink *link, AVFrame *frame)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> (*filter_frame)(AVFilterLink *, AVFrame *);</span><br><span class=\"line\">    AVFilterContext *dstctx = link-&gt;dst;</span><br><span class=\"line\">    AVFilterPad *dst = link-&gt;dstpad;</span><br><span class=\"line\">    <span class=\"type\">int</span> ret;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!(filter_frame = dst-&gt;filter_frame))</span><br><span class=\"line\">        filter_frame = default_filter_frame;</span><br><span class=\"line\">    ……</span><br><span class=\"line\">    ret = filter_frame(link, frame);  <span class=\"comment\">// 最终调用到的地方</span></span><br><span class=\"line\">    link-&gt;frame_count_out++;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ret;</span><br><span class=\"line\">fail:</span><br><span class=\"line\">    ……</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文介绍了 FFmpeg 滤镜开发的整体流程，如何编写 filter.c 文件，并以一个最简单的 AVFilter 和一个较为复杂的 AVFilter 为例，解析了滤镜开发的具体步骤和代码实现，并介绍了 filter_frame() 和 activate() 函数的区别与联系。</p>\n<p>在滤镜开发过程中，需要注意的是，filter_frame() 和 activate() 函数的使用取决于滤镜的复杂性。对于简单的滤镜，可以使用 filter_frame() 函数；而对于需要处理多个输入、一次处理多个帧或在文件结束（EOF）时进行特殊处理的复杂滤镜，建议使用 activate() 函数。</p>\n<p>文中的源码可以查看：<a href=\"https://github.com/VomPom/FFmpeg/commit/9176f58ae60e0b70e5708b25017f374deac9fae7\">add most simplest  AVFilter and a simple video flip filter.</a></p>\n<h3 id=\"参考：\"><a href=\"#参考：\" class=\"headerlink\" title=\"参考：\"></a>参考：</h3><p><a href=\"https://www.cnblogs.com/TaigaCon/p/10171464.html\">https://www.cnblogs.com/TaigaCon/p/10171464.html</a></p>\n<p><a href=\"https://www.cnblogs.com/ranson7zop/p/7728639.html\">https://www.cnblogs.com/ranson7zop/p/7728639.html</a></p>\n<p><a href=\"https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html\">https://www.ffmpeg.org/doxygen/5.0/filter__design_8txt.html</a></p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"FFmpeg","slug":"FFmpeg","api":"api/tags/FFmpeg.json"},{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2024/03/07/实现一个自定义FFmpeg-Filter.json"},{"title":"[转]《数字化视频技术概述》","slug":"转-《数字化视频技术概述》","date":"2023-09-15T12:11:00.000Z","updated":"2025-09-15T13:30:00.946Z","comments":true,"url":"2023/09/15/转-《数字化视频技术概述》/","excerpt":"<p>自己从事在音视频领域的边缘，音视频领域存在太多的专业术语，一下子搞懂是不可能的，这个项目 <a href=\"https://github.com/leandromoreira/digital_video_introduction\">digital_video_introduction</a> 来自 github 的一个开源项目，目前接近有 16k 的 star 了，从最基础的图像概念开始，逐步深入到视频、编解码器、传输和流媒体，适合反复学习阅读。<br>主要内容包括：<br>图像基础： 介绍了图像是如何被计算机理解和存储的，包括像素、分辨率、色彩空间（如 RGB 和 YUV）以及色度采样（如 4:2:0）。<br>视频： 将视频定义为一系列帧在时间上的连续，并引出了帧率（FPS）的概念。<br>编解码器（Codec）： 详细介绍了视频压缩的原理，包括帧内预测、帧间预测（I、P、B 帧）、运动补偿、宏块、熵编码、量化和变换。文章特别提到了 FFmpeg，并提供了如何使用它进行视频编码和转码的实例。<br>容器格式（Container）： 解释了容器格式（如 MP4、MKV）的作用，它们用于封装视频流、音频流和元数据。</p>\n<p>原文链接: <a href=\"https://github.com/leandromoreira/digital_video_introduction\">https://github.com/leandromoreira/digital_video_introduction</a></p>\n","cover":null,"images":[],"content":"<p>自己从事在音视频领域的边缘，音视频领域存在太多的专业术语，一下子搞懂是不可能的，这个项目 <a href=\"https://github.com/leandromoreira/digital_video_introduction\">digital_video_introduction</a> 来自 github 的一个开源项目，目前接近有 16k 的 star 了，从最基础的图像概念开始，逐步深入到视频、编解码器、传输和流媒体，适合反复学习阅读。<br>主要内容包括：<br>图像基础： 介绍了图像是如何被计算机理解和存储的，包括像素、分辨率、色彩空间（如 RGB 和 YUV）以及色度采样（如 4:2:0）。<br>视频： 将视频定义为一系列帧在时间上的连续，并引出了帧率（FPS）的概念。<br>编解码器（Codec）： 详细介绍了视频压缩的原理，包括帧内预测、帧间预测（I、P、B 帧）、运动补偿、宏块、熵编码、量化和变换。文章特别提到了 FFmpeg，并提供了如何使用它进行视频编码和转码的实例。<br>容器格式（Container）： 解释了容器格式（如 MP4、MKV）的作用，它们用于封装视频流、音频流和元数据。</p>\n<p>原文链接: <a href=\"https://github.com/leandromoreira/digital_video_introduction\">https://github.com/leandromoreira/digital_video_introduction</a></p>\n","categories":[],"tags":[{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2023/09/15/转-《数字化视频技术概述》.json"},{"title":"[转]OpenGL黑屏及渲染不出来的常见原因总结","slug":"转-OpenGL黑屏及渲染不出来的常见原因总结","date":"2023-01-15T04:18:00.000Z","updated":"2025-09-15T13:25:10.075Z","comments":true,"url":"2023/01/15/转-OpenGL黑屏及渲染不出来的常见原因总结/","excerpt":"<p>最近在做 Unitiy 与原生渲染相关的研究学习，对于OpenGL这块自己也是接触不多，有很多的坑需要自己去踩，做的过程中遇到最多的问题是：<strong>渲染黑屏</strong> 在掘金上搜到这篇文章<a href=\"https://juejin.cn/post/6844903910742687751\">《OpenGL黑屏及渲染不出来的常见原因总结》</a>很不错，于是记录转载过来，方便日后学习以及问题排查。</p>\n<p>原文链接：<br><a href=\"https://juejin.cn/post/6844903910742687751\">https://juejin.cn/post/6844903910742687751</a></p>\n<h2 id=\"原文\"><a href=\"#原文\" class=\"headerlink\" title=\"原文\"></a>原文</h2><p>做OpenGL开发的同学，想必一定碰到过黑屏的问题，特别是刚接触OpenGL的同学，可能会觉得黑屏问题让人相当头疼，因为OpenGL的查错没有一般编程时那么简单，我们通常是利用glGetError()这个API来获取错误码，但这个方法获取的错误是调用这个方法时，已经产生的错误，它有可能是很久之前产生的，这样查越来还是比较不方便的，而且，有些黑屏以及渲染不出来的情况下，glGetError()也不会报任何错。</p>\n<p>在给大家总结常见的黑屏原因之前，我们先来铺垫一下基础知识，其实屏幕也是一块frame buffer，但它比较特殊，是0号<code>frame buffer</code>，我们如果自己申请frame buffer的话，得到的id是大于0的。那么frame buffer它就会有自己的颜色，如果不特意设置的话，它就是黑色的，因此如果我们渲染操作未正确执行，什么也没渲染出来，自然看到了底色的黑色。</p>\n<p>我们也可以通过<code>glClearColor()+glClear()</code>来设置消除颜色及执行消除操作，来将一个frame buffer清成某种颜色。因此，如果你将frame buffer清成了别的颜色，但其它渲染操作未正确执行，你有可能也不是黑屏，而是你设置的消除颜色，这里也一并总结了，统成为黑屏，同时也包括其它一些不正确的情形。<br>如果不是渲染到屏幕上，是渲染到一个离屏的frame buffer上，同样也会遇到各种黑掉或者渲染不出来的情况，有些原因会同时导致上屏和离屏都黑，有些只影响其中一种情况。</p>\n<p>下面给大家总结一下：</p>\n<ul>\n<li><strong>调用线程的Context不正确</strong></li>\n</ul>\n<p>OpenGL的API在调用时需要有正确的上下文，在Android中称为<code>EGL Context</code>，IOS中是<code>EAGL Context</code>，其它平台有其它平台的叫法，但原理类似。一个线程需要跟EGL Context绑定才能正确使用OpenGL的API，否则调用不会有任何效果，具体可参考我的一篇文章：<a href=\"https://juejin.cn/post/6844903858380996616\">《OpenGL ES 高级进阶：EGL及GL线程》</a>。</p>\n<blockquote>\n<p>【转载注】这也是我碰到的第一个坑，因为从 Unity 调用到 Android 侧的时候，gl渲染环境没有在一个地方。我花了很大的精力去排查前面shader相关的渲染问题，一直没有注意这个问题，所以浪费了大量时间。我个人认为这个问题需要像使用一门新语言的时候要先确保它能打印出”Hello World”一样，是整个流程的前提。</p>\n</blockquote>\n<ul>\n<li><strong>GL Program不正确</strong></li>\n</ul>\n<p>OpenGL渲染需要通过GL Program，它就是一个程序，和我们的普通程序是一个道理，只不过它是运行在GPU上的，如果它不正确了，那自然就渲染不出正确的结果，常见的不正确原因为shader编译失败，通常是因为语法错误，可以用glGetShaderInfoLog()来在编译之后查看相关shader信息，以及在Link后用glGetProgramInfoLog()查看相关program信息，如果得到的信息为空，则说明没有错。</p>\n<ul>\n<li><strong>没有use program</strong></li>\n</ul>\n<p>渲染前需要通过glUseProgran设置本次渲染所用的program，如果未设置则无法执行到对应的shader，自然无法渲染出来。</p>\n<ul>\n<li><strong>未调用glDrawXXX()</strong></li>\n</ul>\n<p>要渲染出来东西，必须调用glDrawXXX()，一般很少出现没调的情况，一般都是低级失误，最好也排查一下。</p>\n<p>对于底层是多buffer实现的surface，渲染后未进行swap buffer<br>常见的是双buffer，此时有一个back buffer和一个front buffer，front buffer是正在显存的这个，back buffer是正在渲染的，如果draw call后没有swap buffer，那back buffer不会呈现出来，因此渲染不出来，这里是特定上屏，如果渲染不是要上屏，则无需考虑这个问题。</p>\n<ul>\n<li><strong>frame buffer的attachment不正确</strong></li>\n</ul>\n<p>在离屏渲染情况下，当我们要渲染到一个frame buffer上，这个frame buffer必须正确绑定了attachment，否则相当于frame buffer是个空壳，它没有任何可用于承载渲染结果的空间。</p>\n<ul>\n<li><strong>顶点attribute值设置错误</strong></li>\n</ul>\n<p>顶点关系到渲染到什么位置，如果设置错误导致渲染的位置在可视范围之外，那么就看不到了，这里的范围是什么呢？如果直接用NDC坐标渲染，那就是-1~1,如果是用世界坐标来渲染，那就要看具体设置的投影矩阵，详细原理可参考我的另一篇文章《OpenGL 3D渲染技术：坐标系及矩阵变换》。</p>\n<p>attribute未启用<br>我们通过想要设置一个attribute的值，需要获取这个attribute的location，并通过glVertexAttribPointer()给它设置值，但别忘了需要使用glGetAttribLocation()来启用这个location，不然设置了也没有用，默认是不启用的。</p>\n<ul>\n<li><strong>VAO/VBO未绑定或者绑定错误</strong></li>\n</ul>\n<p>如果是用VAO/VBO的方式渲染，在渲染前要绑定正确的VAO/VBO，否则等于没指定或者指定错了顶点，就渲染不出来了。</p>\n<ul>\n<li><strong>VAO/VBO方式渲染之后未重置，后面接着用非VAO/VBO方式渲染</strong></li>\n</ul>\n<p>在用VAO/VBO方式渲染之后如果未重置，那么顶点绑定的还是VAO/VBO指定的顶点，此时如果再用普通的glVertexAttribPointer()的方式指定顶点渲染，那用法上会冲突，因为VAO/VBO的方式要求glVertexAttribPointer()函数不指定顶点数据，而普通用法中glVertexAttribPointer()又要指定顶点数据，此时容易造成顶点混乱，渲染结果不正确。</p>\n<p>View Port设置错误<br>View Port即视口，可以理解成我们通过一个窗口去看见OpenGL世界坐标系里渲染的景物，就像我们通过窗口看到室外的景物一样，如果这个窗口没设置或者设置不正确，也会导致看不到东西，一般情况下，我们会将它设置为surface的大小，这样渲染出来的东西就刚好填满这个surface。</p>\n<ul>\n<li><strong>没有渲染到0号frame buffer</strong></li>\n</ul>\n<p>有时候渲染操作有很多步，想做完这些步骤后，再将做好的结果显示的屏幕上，这时就会用一些frame buffer来做离屏渲染，但在最后一步渲染到屏幕上时，需要将frame buffer绑定回0号，才能上屏。</p>\n<ul>\n<li><strong>渲染了一个不正确的纹理</strong></li>\n</ul>\n<p>例如我们希望对一个纹理做一些处理然后渲染出来，但如果这个纹理本身是不正确的，例如前面的步骤出了一些错，导致给过来的纹理id不正确，比如是0，或者纹理id是正确的，但这个纹理是全黑的或者空的，也会导致黑屏。</p>\n<ul>\n<li><strong>glDrawXXX()方法传递的顶点数不正确</strong></li>\n</ul>\n<p>我们在调用glDrawXXX()，会设置顶点数组的开始位置和数量，如果设置不正确，导致传递的顶点是0个，也会导致渲染不出来任何东西。</p>\n<ul>\n<li><strong>顶点buffer的position不正确</strong></li>\n</ul>\n<p>这一点主要是针对java及kotlin，glVertexAttribPointer()接受数据时是通过一个buffer，而我们往buffer是put数据后，buffer的position会相应地往后移动，因此在调用glVertexAttribPointer()之前，记得将position设回到0，否则它将从末尾开始取数据，当然就取不到了。</p>\n<ul>\n<li><strong>面剔除的原因</strong></li>\n</ul>\n<p>如果开启了cull face，那么会按你指定的cull方式来剔除指定顶点旋转顺序的三角面片，如果视线方向看过去的刚好被剔除了，自然就看不见了。</p>\n<ul>\n<li><strong>未开启颜色混合渲染了有透明度的纹理</strong></li>\n</ul>\n<p>OpenGL默认是不开启颜色混合的，这会导致透明的部分通常会被渲染成黑色，而不是透出下面的颜色，具体可以参数我的一篇文章：《OpenGL ES 高级进阶：颜色混合》。</p>\n<p>作者：程序员kenney</p>\n<p>链接：<a href=\"https://juejin.cn/post/6844903910742687751\">https://juejin.cn/post/6844903910742687751</a></p>\n<p>来源：稀土掘金</p>\n","cover":null,"images":[],"content":"<p>最近在做 Unitiy 与原生渲染相关的研究学习，对于OpenGL这块自己也是接触不多，有很多的坑需要自己去踩，做的过程中遇到最多的问题是：<strong>渲染黑屏</strong> 在掘金上搜到这篇文章<a href=\"https://juejin.cn/post/6844903910742687751\">《OpenGL黑屏及渲染不出来的常见原因总结》</a>很不错，于是记录转载过来，方便日后学习以及问题排查。</p>\n<p>原文链接：<br><a href=\"https://juejin.cn/post/6844903910742687751\">https://juejin.cn/post/6844903910742687751</a></p>\n<h2 id=\"原文\"><a href=\"#原文\" class=\"headerlink\" title=\"原文\"></a>原文</h2><p>做OpenGL开发的同学，想必一定碰到过黑屏的问题，特别是刚接触OpenGL的同学，可能会觉得黑屏问题让人相当头疼，因为OpenGL的查错没有一般编程时那么简单，我们通常是利用glGetError()这个API来获取错误码，但这个方法获取的错误是调用这个方法时，已经产生的错误，它有可能是很久之前产生的，这样查越来还是比较不方便的，而且，有些黑屏以及渲染不出来的情况下，glGetError()也不会报任何错。</p>\n<p>在给大家总结常见的黑屏原因之前，我们先来铺垫一下基础知识，其实屏幕也是一块frame buffer，但它比较特殊，是0号<code>frame buffer</code>，我们如果自己申请frame buffer的话，得到的id是大于0的。那么frame buffer它就会有自己的颜色，如果不特意设置的话，它就是黑色的，因此如果我们渲染操作未正确执行，什么也没渲染出来，自然看到了底色的黑色。</p>\n<p>我们也可以通过<code>glClearColor()+glClear()</code>来设置消除颜色及执行消除操作，来将一个frame buffer清成某种颜色。因此，如果你将frame buffer清成了别的颜色，但其它渲染操作未正确执行，你有可能也不是黑屏，而是你设置的消除颜色，这里也一并总结了，统成为黑屏，同时也包括其它一些不正确的情形。<br>如果不是渲染到屏幕上，是渲染到一个离屏的frame buffer上，同样也会遇到各种黑掉或者渲染不出来的情况，有些原因会同时导致上屏和离屏都黑，有些只影响其中一种情况。</p>\n<p>下面给大家总结一下：</p>\n<ul>\n<li><strong>调用线程的Context不正确</strong></li>\n</ul>\n<p>OpenGL的API在调用时需要有正确的上下文，在Android中称为<code>EGL Context</code>，IOS中是<code>EAGL Context</code>，其它平台有其它平台的叫法，但原理类似。一个线程需要跟EGL Context绑定才能正确使用OpenGL的API，否则调用不会有任何效果，具体可参考我的一篇文章：<a href=\"https://juejin.cn/post/6844903858380996616\">《OpenGL ES 高级进阶：EGL及GL线程》</a>。</p>\n<blockquote>\n<p>【转载注】这也是我碰到的第一个坑，因为从 Unity 调用到 Android 侧的时候，gl渲染环境没有在一个地方。我花了很大的精力去排查前面shader相关的渲染问题，一直没有注意这个问题，所以浪费了大量时间。我个人认为这个问题需要像使用一门新语言的时候要先确保它能打印出”Hello World”一样，是整个流程的前提。</p>\n</blockquote>\n<ul>\n<li><strong>GL Program不正确</strong></li>\n</ul>\n<p>OpenGL渲染需要通过GL Program，它就是一个程序，和我们的普通程序是一个道理，只不过它是运行在GPU上的，如果它不正确了，那自然就渲染不出正确的结果，常见的不正确原因为shader编译失败，通常是因为语法错误，可以用glGetShaderInfoLog()来在编译之后查看相关shader信息，以及在Link后用glGetProgramInfoLog()查看相关program信息，如果得到的信息为空，则说明没有错。</p>\n<ul>\n<li><strong>没有use program</strong></li>\n</ul>\n<p>渲染前需要通过glUseProgran设置本次渲染所用的program，如果未设置则无法执行到对应的shader，自然无法渲染出来。</p>\n<ul>\n<li><strong>未调用glDrawXXX()</strong></li>\n</ul>\n<p>要渲染出来东西，必须调用glDrawXXX()，一般很少出现没调的情况，一般都是低级失误，最好也排查一下。</p>\n<p>对于底层是多buffer实现的surface，渲染后未进行swap buffer<br>常见的是双buffer，此时有一个back buffer和一个front buffer，front buffer是正在显存的这个，back buffer是正在渲染的，如果draw call后没有swap buffer，那back buffer不会呈现出来，因此渲染不出来，这里是特定上屏，如果渲染不是要上屏，则无需考虑这个问题。</p>\n<ul>\n<li><strong>frame buffer的attachment不正确</strong></li>\n</ul>\n<p>在离屏渲染情况下，当我们要渲染到一个frame buffer上，这个frame buffer必须正确绑定了attachment，否则相当于frame buffer是个空壳，它没有任何可用于承载渲染结果的空间。</p>\n<ul>\n<li><strong>顶点attribute值设置错误</strong></li>\n</ul>\n<p>顶点关系到渲染到什么位置，如果设置错误导致渲染的位置在可视范围之外，那么就看不到了，这里的范围是什么呢？如果直接用NDC坐标渲染，那就是-1~1,如果是用世界坐标来渲染，那就要看具体设置的投影矩阵，详细原理可参考我的另一篇文章《OpenGL 3D渲染技术：坐标系及矩阵变换》。</p>\n<p>attribute未启用<br>我们通过想要设置一个attribute的值，需要获取这个attribute的location，并通过glVertexAttribPointer()给它设置值，但别忘了需要使用glGetAttribLocation()来启用这个location，不然设置了也没有用，默认是不启用的。</p>\n<ul>\n<li><strong>VAO/VBO未绑定或者绑定错误</strong></li>\n</ul>\n<p>如果是用VAO/VBO的方式渲染，在渲染前要绑定正确的VAO/VBO，否则等于没指定或者指定错了顶点，就渲染不出来了。</p>\n<ul>\n<li><strong>VAO/VBO方式渲染之后未重置，后面接着用非VAO/VBO方式渲染</strong></li>\n</ul>\n<p>在用VAO/VBO方式渲染之后如果未重置，那么顶点绑定的还是VAO/VBO指定的顶点，此时如果再用普通的glVertexAttribPointer()的方式指定顶点渲染，那用法上会冲突，因为VAO/VBO的方式要求glVertexAttribPointer()函数不指定顶点数据，而普通用法中glVertexAttribPointer()又要指定顶点数据，此时容易造成顶点混乱，渲染结果不正确。</p>\n<p>View Port设置错误<br>View Port即视口，可以理解成我们通过一个窗口去看见OpenGL世界坐标系里渲染的景物，就像我们通过窗口看到室外的景物一样，如果这个窗口没设置或者设置不正确，也会导致看不到东西，一般情况下，我们会将它设置为surface的大小，这样渲染出来的东西就刚好填满这个surface。</p>\n<ul>\n<li><strong>没有渲染到0号frame buffer</strong></li>\n</ul>\n<p>有时候渲染操作有很多步，想做完这些步骤后，再将做好的结果显示的屏幕上，这时就会用一些frame buffer来做离屏渲染，但在最后一步渲染到屏幕上时，需要将frame buffer绑定回0号，才能上屏。</p>\n<ul>\n<li><strong>渲染了一个不正确的纹理</strong></li>\n</ul>\n<p>例如我们希望对一个纹理做一些处理然后渲染出来，但如果这个纹理本身是不正确的，例如前面的步骤出了一些错，导致给过来的纹理id不正确，比如是0，或者纹理id是正确的，但这个纹理是全黑的或者空的，也会导致黑屏。</p>\n<ul>\n<li><strong>glDrawXXX()方法传递的顶点数不正确</strong></li>\n</ul>\n<p>我们在调用glDrawXXX()，会设置顶点数组的开始位置和数量，如果设置不正确，导致传递的顶点是0个，也会导致渲染不出来任何东西。</p>\n<ul>\n<li><strong>顶点buffer的position不正确</strong></li>\n</ul>\n<p>这一点主要是针对java及kotlin，glVertexAttribPointer()接受数据时是通过一个buffer，而我们往buffer是put数据后，buffer的position会相应地往后移动，因此在调用glVertexAttribPointer()之前，记得将position设回到0，否则它将从末尾开始取数据，当然就取不到了。</p>\n<ul>\n<li><strong>面剔除的原因</strong></li>\n</ul>\n<p>如果开启了cull face，那么会按你指定的cull方式来剔除指定顶点旋转顺序的三角面片，如果视线方向看过去的刚好被剔除了，自然就看不见了。</p>\n<ul>\n<li><strong>未开启颜色混合渲染了有透明度的纹理</strong></li>\n</ul>\n<p>OpenGL默认是不开启颜色混合的，这会导致透明的部分通常会被渲染成黑色，而不是透出下面的颜色，具体可以参数我的一篇文章：《OpenGL ES 高级进阶：颜色混合》。</p>\n<p>作者：程序员kenney</p>\n<p>链接：<a href=\"https://juejin.cn/post/6844903910742687751\">https://juejin.cn/post/6844903910742687751</a></p>\n<p>来源：稀土掘金</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"},{"name":"OpenGL","slug":"OpenGL","api":"api/tags/OpenGL.json"}],"api":"api/posts/2023/01/15/转-OpenGL黑屏及渲染不出来的常见原因总结.json"},{"title":"3D LUT 滤镜 shader 源码分析","slug":"LUT-Shader-源码分析","date":"2022-11-24T03:36:00.000Z","updated":"2025-09-15T13:11:04.747Z","comments":true,"url":"2022/11/24/LUT-Shader-源码分析/","excerpt":"<p>最近在做滤镜相关的渲染学习，目前大部分 LUT 滤镜代码实现都是参考由 GPUImage 提供的 LookupFilter 的逻辑，整个代码实现不多。参考网上的博文也有各种解释，参考了大量博文之后终于理解了，所以自己重新整理了一份，方便以后阅读理解，对整体代码的实现过程结合LUT的原理进行一个简单整理。</p>\n<h2 id=\"GPUImageLookupFilter-shader-源码\"><a href=\"#GPUImageLookupFilter-shader-源码\" class=\"headerlink\" title=\"GPUImageLookupFilter shader 源码\"></a>GPUImageLookupFilter shader 源码</h2><figure class=\"highlight glsl\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate;      </span><br><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture;  <span class=\"comment\">// 目标纹理，对应原始资源</span></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture2; <span class=\"comment\">// 查找表纹理，对应LUT图片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"keyword\">lowp</span> <span class=\"type\">float</span> intensity;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> main()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//获取原始图层颜色</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec4</span> textureColor = <span class=\"built_in\">texture2D</span>(inputImageTexture, textureCoordinate);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取蓝色通道颜色，textureColor.b 的范围为(0,1)，blueColor 范围为(0,63) </span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">float</span> blueColor = textureColor.b * <span class=\"number\">63.0</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad1为查找颜色所在左边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad1;</span><br><span class=\"line\">    quad1.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">floor</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad1.x = <span class=\"built_in\">floor</span>(blueColor) - (quad1.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad2为查找颜色所在右边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad2;</span><br><span class=\"line\">    quad2.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">ceil</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad2.x = <span class=\"built_in\">ceil</span>(blueColor) - (quad2.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取到左边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos1;</span><br><span class=\"line\">    texPos1.x = (quad1.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos1.y = (quad1.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">   <span class=\"comment\">//获取到右边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos2;</span><br><span class=\"line\">    texPos2.x = (quad2.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos2.y = (quad2.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取对应位置纹理的颜色 RGBA 值</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor1 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos1);</span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor2 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos2);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//真正的颜色是 newColor1 和 newColor2 的混合</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor = <span class=\"built_in\">mix</span>(newColor1, newColor2, <span class=\"built_in\">fract</span>(blueColor));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">gl_FragColor</span> = <span class=\"built_in\">mix</span>(textureColor, <span class=\"type\">vec4</span>(newColor.rgb, textureColor.w), intensity);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>整个源码的主要逻辑为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>。上面注释已将具体的实现过程描述清楚，但与我们的 LUT 图片割裂，接下来结合 LUT 的实现原理以及具体的数据来形象地描述整个实现流程。</p>\n<p>假设我们输入的参数为：<br>textureColor = ver4(.0, .0, 0.5, 1.0)</p>\n<h2 id=\"查找颜色所在位置的小正方形\"><a href=\"#查找颜色所在位置的小正方形\" class=\"headerlink\" title=\"查找颜色所在位置的小正方形\"></a>查找颜色所在位置的小正方形</h2><p>我们知道LUT有64个小正方形，目标是为了找到对应小正方形里面的对应的颜色，我们需要先确认是第几个小正方形，正是通过 textureColor.b * 63 查找</p>\n<p>带入<code>blueColor</code> -&gt; textureColor.b = 0.5<br>对 <code>textureColor.b * 63.0</code> = 31.5</p>\n<p>也就是说我们需要第 [31.5] 位置小正方形，但是索引(从0-63共64个)都是正数，对于 31.5 索引 我们该怎么确定是 31 还是第 32 个呢？GPUImage给出的一种插值方式就是两个都要，然后进行一次混合，从而使得值能够俊均匀的在两个小正方形色块中。</p>\n<p>具体逻辑为：</p>\n<p>quad1.y = floor(floor(blueColor) / 8.0) = 3，确定为小方块在纵坐标索引3，也就是第4行。</p>\n<img src=\"https://cdn.julis.wang/blog/img/f2e1e14bc30c4e438664b90fa5ad8103.png?imageView2/2/w/500\">\n<p><code>quad1.x = floor(blueColor) - (quad1.y * 8.0) = 31 - 24 = 7</code></p>\n<p>也就确定了小方块为(3,7) 也就是第4排第8个。</p>\n<img src=\"https://cdn.julis.wang/blog/img/bc8a47389a654dbeb5e4fc5e5584d1f9.png?imageView2/2/w/500\">\n<p>同理，对于第2个小方块确定的位置为(4,0) 也就是第5排第1个。</p>\n<p> <code>quad2.y = floor(ceil(blueColor) / 8.0) = 4</code></p>\n<p> <code>quad2.x = ceil(blueColor) - (quad2.y * 8.0)= 0</code></p>\n<h2 id=\"查找小正方形内的具体颜色\"><a href=\"#查找小正方形内的具体颜色\" class=\"headerlink\" title=\"查找小正方形内的具体颜色\"></a>查找小正方形内的具体颜色</h2><p>已经获取到对应的方块了，接下来需要确定方块内的像素的位置了。一般一个LUT的大小为 512x512，由8x8小方块构成，也就是每个方块的的像素为64x64，如下图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/a4d6f390dd7b41fab75b568b37fb1e08.png?imageView2/2/w/500\">\n<p>计算x坐标的逻辑为：</p>\n<p><code>texPos1.x = (quad1.x * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.r)</code></p>\n<p>这一段是相对比较难理解的，我们可以分几部分进行理解：</p>\n<p>第一部分：<strong>(quad1.x * 0.125)</strong></p>\n<p>  我们得到 quad1.x = 7，也就是第8列，*0.125将坐标转化在(0,1)之间，也就是得到在01坐标系内如图红线的位置。</p>\n<img src=\"https://cdn.julis.wang/blog/img/56098332877d4724beae4000c4fdf5fa.png?imageView2/2/w/500\">\n<p>第二部分：<strong>((0.125 - 1.0/512.0) * textureColor.r)</strong></p>\n<p>我们可以把它当成 <code>(63.0/512.0)* textureColor.r</code> , <code>63.0/512.0</code>代表着一个512x512中每个小方块的64份数据（为什么是63？别忘了0的存在），textureColor.r 数据在 0-1之间，这样就能确认在第一部分结果基础之上的偏移值。</p>\n<img src=\"https://cdn.julis.wang/blog/img/7b3cba46937a4be4b0e45bc3077d3aad.png?imageView2/2/w/500\">\n<p>第三部分：<strong>0.5/512.0</strong></p>\n<p>这一部分主要是 +0.5 做四舍五入运算，为保证第512行取到的是511.5/512，第1行取到的是 0.5/512.0。</p>\n<p>同理，计算y的坐标，以及计算另一个小正方形内的位置是一样的。</p>\n<p>最后在通过对从两个小正方形获取到的颜色进行 mix，并返回给着色器，GPU再对原始图像进行每一个像素点绘制，从而实现滤镜的效果。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>LUT 对应的 Shader 执行过程主要为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>，整个流程都比较好理解，但代码相对而言比较难理解，网上看了很多其他的大佬写的一些文章，最开始自己看的时候也是很难理解的，后面终于悟了，所以想通过自己的理解，尽力更形象地解释（虽然可能也没有很形象），如果还有什么疑问，欢迎一起交流学习。</p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/f2e1e14bc30c4e438664b90fa5ad8103.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/bc8a47389a654dbeb5e4fc5e5584d1f9.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/a4d6f390dd7b41fab75b568b37fb1e08.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/56098332877d4724beae4000c4fdf5fa.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/7b3cba46937a4be4b0e45bc3077d3aad.png?imageView2/2/w/500"],"content":"<p>最近在做滤镜相关的渲染学习，目前大部分 LUT 滤镜代码实现都是参考由 GPUImage 提供的 LookupFilter 的逻辑，整个代码实现不多。参考网上的博文也有各种解释，参考了大量博文之后终于理解了，所以自己重新整理了一份，方便以后阅读理解，对整体代码的实现过程结合LUT的原理进行一个简单整理。</p>\n<h2 id=\"GPUImageLookupFilter-shader-源码\"><a href=\"#GPUImageLookupFilter-shader-源码\" class=\"headerlink\" title=\"GPUImageLookupFilter shader 源码\"></a>GPUImageLookupFilter shader 源码</h2><figure class=\"highlight glsl\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate;      </span><br><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture;  <span class=\"comment\">// 目标纹理，对应原始资源</span></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture2; <span class=\"comment\">// 查找表纹理，对应LUT图片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"keyword\">lowp</span> <span class=\"type\">float</span> intensity;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> main()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//获取原始图层颜色</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec4</span> textureColor = <span class=\"built_in\">texture2D</span>(inputImageTexture, textureCoordinate);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取蓝色通道颜色，textureColor.b 的范围为(0,1)，blueColor 范围为(0,63) </span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">float</span> blueColor = textureColor.b * <span class=\"number\">63.0</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad1为查找颜色所在左边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad1;</span><br><span class=\"line\">    quad1.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">floor</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad1.x = <span class=\"built_in\">floor</span>(blueColor) - (quad1.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad2为查找颜色所在右边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad2;</span><br><span class=\"line\">    quad2.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">ceil</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad2.x = <span class=\"built_in\">ceil</span>(blueColor) - (quad2.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取到左边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos1;</span><br><span class=\"line\">    texPos1.x = (quad1.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos1.y = (quad1.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">   <span class=\"comment\">//获取到右边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos2;</span><br><span class=\"line\">    texPos2.x = (quad2.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos2.y = (quad2.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取对应位置纹理的颜色 RGBA 值</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor1 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos1);</span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor2 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos2);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//真正的颜色是 newColor1 和 newColor2 的混合</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor = <span class=\"built_in\">mix</span>(newColor1, newColor2, <span class=\"built_in\">fract</span>(blueColor));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">gl_FragColor</span> = <span class=\"built_in\">mix</span>(textureColor, <span class=\"type\">vec4</span>(newColor.rgb, textureColor.w), intensity);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>整个源码的主要逻辑为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>。上面注释已将具体的实现过程描述清楚，但与我们的 LUT 图片割裂，接下来结合 LUT 的实现原理以及具体的数据来形象地描述整个实现流程。</p>\n<p>假设我们输入的参数为：<br>textureColor = ver4(.0, .0, 0.5, 1.0)</p>\n<h2 id=\"查找颜色所在位置的小正方形\"><a href=\"#查找颜色所在位置的小正方形\" class=\"headerlink\" title=\"查找颜色所在位置的小正方形\"></a>查找颜色所在位置的小正方形</h2><p>我们知道LUT有64个小正方形，目标是为了找到对应小正方形里面的对应的颜色，我们需要先确认是第几个小正方形，正是通过 textureColor.b * 63 查找</p>\n<p>带入<code>blueColor</code> -&gt; textureColor.b = 0.5<br>对 <code>textureColor.b * 63.0</code> = 31.5</p>\n<p>也就是说我们需要第 [31.5] 位置小正方形，但是索引(从0-63共64个)都是正数，对于 31.5 索引 我们该怎么确定是 31 还是第 32 个呢？GPUImage给出的一种插值方式就是两个都要，然后进行一次混合，从而使得值能够俊均匀的在两个小正方形色块中。</p>\n<p>具体逻辑为：</p>\n<p>quad1.y = floor(floor(blueColor) / 8.0) = 3，确定为小方块在纵坐标索引3，也就是第4行。</p>\n<img src=\"https://cdn.julis.wang/blog/img/f2e1e14bc30c4e438664b90fa5ad8103.png?imageView2/2/w/500\">\n<p><code>quad1.x = floor(blueColor) - (quad1.y * 8.0) = 31 - 24 = 7</code></p>\n<p>也就确定了小方块为(3,7) 也就是第4排第8个。</p>\n<img src=\"https://cdn.julis.wang/blog/img/bc8a47389a654dbeb5e4fc5e5584d1f9.png?imageView2/2/w/500\">\n<p>同理，对于第2个小方块确定的位置为(4,0) 也就是第5排第1个。</p>\n<p> <code>quad2.y = floor(ceil(blueColor) / 8.0) = 4</code></p>\n<p> <code>quad2.x = ceil(blueColor) - (quad2.y * 8.0)= 0</code></p>\n<h2 id=\"查找小正方形内的具体颜色\"><a href=\"#查找小正方形内的具体颜色\" class=\"headerlink\" title=\"查找小正方形内的具体颜色\"></a>查找小正方形内的具体颜色</h2><p>已经获取到对应的方块了，接下来需要确定方块内的像素的位置了。一般一个LUT的大小为 512x512，由8x8小方块构成，也就是每个方块的的像素为64x64，如下图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/a4d6f390dd7b41fab75b568b37fb1e08.png?imageView2/2/w/500\">\n<p>计算x坐标的逻辑为：</p>\n<p><code>texPos1.x = (quad1.x * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.r)</code></p>\n<p>这一段是相对比较难理解的，我们可以分几部分进行理解：</p>\n<p>第一部分：<strong>(quad1.x * 0.125)</strong></p>\n<p>  我们得到 quad1.x = 7，也就是第8列，*0.125将坐标转化在(0,1)之间，也就是得到在01坐标系内如图红线的位置。</p>\n<img src=\"https://cdn.julis.wang/blog/img/56098332877d4724beae4000c4fdf5fa.png?imageView2/2/w/500\">\n<p>第二部分：<strong>((0.125 - 1.0/512.0) * textureColor.r)</strong></p>\n<p>我们可以把它当成 <code>(63.0/512.0)* textureColor.r</code> , <code>63.0/512.0</code>代表着一个512x512中每个小方块的64份数据（为什么是63？别忘了0的存在），textureColor.r 数据在 0-1之间，这样就能确认在第一部分结果基础之上的偏移值。</p>\n<img src=\"https://cdn.julis.wang/blog/img/7b3cba46937a4be4b0e45bc3077d3aad.png?imageView2/2/w/500\">\n<p>第三部分：<strong>0.5/512.0</strong></p>\n<p>这一部分主要是 +0.5 做四舍五入运算，为保证第512行取到的是511.5/512，第1行取到的是 0.5/512.0。</p>\n<p>同理，计算y的坐标，以及计算另一个小正方形内的位置是一样的。</p>\n<p>最后在通过对从两个小正方形获取到的颜色进行 mix，并返回给着色器，GPU再对原始图像进行每一个像素点绘制，从而实现滤镜的效果。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>LUT 对应的 Shader 执行过程主要为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>，整个流程都比较好理解，但代码相对而言比较难理解，网上看了很多其他的大佬写的一些文章，最开始自己看的时候也是很难理解的，后面终于悟了，所以想通过自己的理解，尽力更形象地解释（虽然可能也没有很形象），如果还有什么疑问，欢迎一起交流学习。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2022/11/24/LUT-Shader-源码分析.json"},{"title":"Unity 实现利用 Andorid 能力进行视频渲染播放","slug":"Unity-实现利用-Andorid-能力进行视频渲染播放","date":"2022-10-25T02:00:00.000Z","updated":"2025-09-15T13:10:48.829Z","comments":true,"url":"2022/10/25/Unity-实现利用-Andorid-能力进行视频渲染播放/","excerpt":"<p>在 Unity 中使用 Android 侧提供的视频渲染相关的能力，有两种方案可选：</p>\n<p>第一种是将渲染播放页单独做一个页面，在 Unity事件交互的时候打开对应 Activity 页面，或者获取到 Unity 创建的 Acitivity 动态添加 View。</p>\n<p>第二种是只借助 Android 的渲染能力，将数据渲染到 Unity 的控件上。</p>\n<p>两种方案各有优劣，第一种大大地减少了播放器相关的开发工作量，整个页面逻辑可以实现复用，但是交互页面的话 iOS/Android 需要写两套。第二种实现成本相对较高，但是交互可以由 Unity 侧进行，只是播放器使用封装好的 plugin 进行，能达到交互相对较统一，本文也主要是讲述该方案的实现。</p>\n<h2 id=\"Android-平台基本播放逻辑\"><a href=\"#Android-平台基本播放逻辑\" class=\"headerlink\" title=\"Android 平台基本播放逻辑\"></a>Android 平台基本播放逻辑</h2><p>在正式开发改造之前，对 Android 侧的一个播放器渲染流程进行简单的介绍，以 MediaPlayer 为例，利用 MediaPlayer 进行视频解码渲染，并将视频最后输出到 SurfaceView 上,一次播放器视频渲染到View上的的主要代码流程为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initPlayer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MediaPlayer</span> <span class=\"variable\">mediaPlayer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MediaPlayer</span>();</span><br><span class=\"line\">    <span class=\"type\">SurfaceView</span> <span class=\"variable\">surfaceView</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceView</span>(activity);</span><br><span class=\"line\">    surfaceHolder = surfaceView.getHolder();</span><br><span class=\"line\">    surfaceHolder.addCallback(^ &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">surfaceCreated</span><span class=\"params\">(SurfaceHolder holder)</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">Surface</span> <span class=\"variable\">surface</span> <span class=\"operator\">=</span> holder.getSurface();</span><br><span class=\"line\">            mediaPlayer.setSurface(surface);</span><br><span class=\"line\">            mediaPlayer.prepareAsync();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         ……</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    mediaPlayer.setDataSource(URI...);</span><br><span class=\"line\">    mediaPlayer.setOnPreparedListener(mp -&gt; mp.start());   </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<p>对于渲染 <code>mediaPlayer.setSurface(surface)</code> 设为播放器解码数据的接受器，Surface 来自于 SurfaceView。</p>\n<p>播放器是将数据图形绘制在 Surface 对象上，Surface中会关联一个 BufferQueue 用于提供图像数据缓存，SurfaceFlinger 会把 Surface 对应的图像层混合在一起，将其输出到 FrameBuffer 中（Framebuffer就是一块内存区域，它通常是显示驱动的内部缓冲区在内存中的映射），最后在屏幕上看到合成的图像。</p>\n<p>整个流程引入外部大佬的一张图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg\">\n<h2 id=\"Unity-中的一些改造\"><a href=\"#Unity-中的一些改造\" class=\"headerlink\" title=\"Unity 中的一些改造\"></a>Unity 中的一些改造</h2><p>上面的流程最终是通过播放器解码渲染到 SurfaceView 上，当然，你可以通过获取到 UnityPlayer 对应的 Acitivity 将这个 SurfaceView 动态添加到当前界面，实现“在 Unity 中利用 Android 能力进行视频渲染”。</p>\n<p>所以需要对其进行改造，我们的目的是实现 Android 播放器数据渲染到 Untiy 的组件中。实现这一过程需要借助 FBO(Frame Buffer Object) 的能力。</p>\n<h3 id=\"（一）FBO\"><a href=\"#（一）FBO\" class=\"headerlink\" title=\"（一）FBO\"></a>（一）FBO</h3><p>在 OpenGL 渲染管线中几何数据和纹理经过变换和一些测试处理，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存(framebuffer)，OpenGL渲染管线的最终位置是在帧缓冲区中，默认情况下 OpenGL 使用的是窗口系统提供的帧缓冲区。</p>\n<p>但有些场景是不想要直接渲染到窗口上的(例如加视频特效)，于是 OpenGL 提供了一种方式来创建额外的帧缓冲区对象(FBO)。使用帧缓冲区对象，OpenGL 可以将原先绘制到窗口提供的帧缓冲区重定向到 FBO 之中。FBO本身不是一块内存，没有空间，真正存储东西，可实际读写的是依附于FBO的东西：纹理(texture)和渲染缓存(renderbuffer)，依附的方式，是一个二维数组来管理，结构如图所示：</p>\n<p><img src=\"https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&amp;width=380\" alt=\"\"></p>\n<h3 id=\"（二）具体实现\"><a href=\"#（二）具体实现\" class=\"headerlink\" title=\"（二）具体实现\"></a>（二）具体实现</h3><p>使用 FBO 我们可以将渲染目标渲染到其他的空间，我们目的是将播放器解码后的数据渲染到 Unity 控件的纹理空间中。<br>渲染播放器将输出到 FBO 中，FBO 指向 Unity 控件数据的输入，从而实现：Android 的播放器输出数据显示到 Unity 的控件中。</p>\n<h3 id=\"（三）从渲染输出数据到外部纹理\"><a href=\"#（三）从渲染输出数据到外部纹理\" class=\"headerlink\" title=\"（三）从渲染输出数据到外部纹理\"></a>（三）从渲染输出数据到外部纹理</h3><p>由于 <code>mediaPlayer.setSurface(surface)</code> 对应的 Surface 来源于 SurafaceView，会直接渲染到屏幕上，这里我们需要使用 构造一个新的 SurfaceTexture 以将图像流式传输到给定的 OpenGL 纹理;</p>\n<p>要获取到播放器渲染得数据，需要借助 SurfaceTexture ，SurfaceTexture 是Surface 和 OpenGL ES 纹理的结合，其对图像流的处理并不直接显示，而是从图像流中捕获帧作为 OpenGL 的外部纹理，图像流来自相机预览和视频解码。</p>\n<p>SurfaceTexture 创建的 Surface 是数据的生产者，而 SurfaceTexture 是对应的消费者，Surface 接收媒体数据并将数据发送到 SurfaceTexture，当调用 updateTexImage 的时候，创建SurfaceTexture 的纹理对象相应的内容将更新为最新图像帧，也就是会将图像帧转换为 GL 纹理，并将该纹理绑定到 GL_TEXTURE_EXTERNAL_OES 纹理对象上。具体实现逻辑参考：<a href=\"https://juejin.cn/post/7012517274768179236\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">SurfaceTexture</span> <span class=\"variable\">surfaceTexture</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceTexture</span>(videoTextureId);</span><br><span class=\"line\">player.setUpSurface(<span class=\"keyword\">new</span> <span class=\"title class_\">Surface</span>(surfaceTexture), width, height);</span><br><span class=\"line\">surfaceTexture.setDefaultBufferSize(width, height);</span><br><span class=\"line\">surfaceTexture.setOnFrameAvailableListener(surfaceTexture -&gt; &#123;……&#125;);;</span><br></pre></td></tr></table></figure>\n<p>其中 videoTextureId 来源于创建的 OES 纹理：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createOESTextureID</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span>[] texture = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"comment\">// 创建纹理对象，一个容器对象，保存渲染所需要的纹理数据，例如：图像数据</span></span><br><span class=\"line\">        <span class=\"comment\">//在OpenGL 中纹理对象是一个无符号整数，是一个纹理对象的句柄</span></span><br><span class=\"line\">        GLES30.glGenTextures(texture.length, texture, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 绑定纹理ID到纹理单元的纹理目标上</span></span><br><span class=\"line\">        GLES30.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, texture[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 设置纹理参数</span></span><br><span class=\"line\">        ……</span><br><span class=\"line\"></span><br><span class=\"line\">        GLES30.glGenerateMipmap(GLES11Ext.GL_TEXTURE_EXTERNAL_OES);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> texture[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"（四）FBO纹理数据到-Unity-的纹理数据\"><a href=\"#（四）FBO纹理数据到-Unity-的纹理数据\" class=\"headerlink\" title=\"（四）FBO纹理数据到 Unity 的纹理数据\"></a>（四）FBO纹理数据到 Unity 的纹理数据</h3><p>学习了解到Unity中可以使用 RawImage 或者 quad 等相关控件可以显示纹理，这里以 RawImage 为例。在 Unity 脚本编写初始化的逻辑，构造一个 Texture2D 对象，将句柄传递到 Android，并赋值给 RawImage，并将texture id 传递到 Android 平台，完成一次渲染的重定向。<br><figure class=\"highlight c#\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">InitPlayer</span>()</span></span><br><span class=\"line\"> &#123;    </span><br><span class=\"line\">    Texture2D texture2D = <span class=\"keyword\">new</span> Texture2D(width, height, TextureFormat.RGB24, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>);</span><br><span class=\"line\">    androidObj.Call(<span class=\"string\">&quot;init&quot;</span>, (<span class=\"built_in\">int</span>)texture2D.GetNativeTexturePtr(), width, height);</span><br><span class=\"line\">    RawImage.texture = texture2D;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>创建FBO<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createFBO</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>[] fbo = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">    GLES30.glGenFramebuffers(fbo.length, fbo, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fbo[<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>为<code>SurfaceTexture</code> 设置了 <code>OnFrameAvailableListener</code> 后，当有新的图形流数据生成之后，就可以通过  <code>mSurfaceTexture.updateTexImage()</code> 将当前图片流更新到纹理所关联的OpenGLES中纹理，并绘制 FBO.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">publc <span class=\"keyword\">void</span> <span class=\"title function_\">draw</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 绑定 FrameBuffer 到当前的绘制环境上， 后续 GL 绘制都会到这个 framebuffer</span></span><br><span class=\"line\">    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fbo[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//2.把一个2D纹理作为帧缓冲区附着</span></span><br><span class=\"line\">    <span class=\"comment\">//即所有渲染操作的结果将会被储存在 unityTextureId 对应的纹理图像中</span></span><br><span class=\"line\">    GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, unityTextureId, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//绑定指定纹理到当前激活的纹理单元</span></span><br><span class=\"line\">    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, videoTextureId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//…… 省略 Opengl 绘制的常规流程</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这一步是最关键的，实现了将 FBO 的输出指向 Unity 里面创建的纹理，也就实现了 Android 渲染与 Unity 之间的数据打通。</p>\n<p>这里的 unityTextureId 来源于在 Unity 中初始化的 <code>(int)texture2D.GetNativeTexturePtr()</code>值。</p>\n<p>整体的流程为：</p>\n<img src=\"https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png\">\n<p>效果图：</p>\n<img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif\">\n<p>图中播放视频区域为 Unity 的 RawImage 控件，渲染的视频通过 Pag 等相关素材由渲染SDK合成。</p>\n<p>如图所示，视频画面正常地进行渲染，图中有两个区域展示了视频画面，上面的使用的 Quad 组件，下面是用的 RawImage，流程都一直，只是在 Unity 使用 Texture2D 的时候通过 <code>Quad.mainTexture = texture2D</code> 赋值。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲了 Unity 利用 Android 提供的能力进行视频相关的特效渲染的方案，总体正常运行。还需要一些优化，例如对 <code>Multithreaded Rendering</code>配置还未支持，以及一些逻辑可能受限于游戏侧的配置，例如图形渲染的配置使用的 OpenGL3.0，如果使用 OpenGL2.0 或者 Vulkan，还需要单独调整相关逻辑。</p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg","https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&amp;width=380","https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png","https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif"],"content":"<p>在 Unity 中使用 Android 侧提供的视频渲染相关的能力，有两种方案可选：</p>\n<p>第一种是将渲染播放页单独做一个页面，在 Unity事件交互的时候打开对应 Activity 页面，或者获取到 Unity 创建的 Acitivity 动态添加 View。</p>\n<p>第二种是只借助 Android 的渲染能力，将数据渲染到 Unity 的控件上。</p>\n<p>两种方案各有优劣，第一种大大地减少了播放器相关的开发工作量，整个页面逻辑可以实现复用，但是交互页面的话 iOS/Android 需要写两套。第二种实现成本相对较高，但是交互可以由 Unity 侧进行，只是播放器使用封装好的 plugin 进行，能达到交互相对较统一，本文也主要是讲述该方案的实现。</p>\n<h2 id=\"Android-平台基本播放逻辑\"><a href=\"#Android-平台基本播放逻辑\" class=\"headerlink\" title=\"Android 平台基本播放逻辑\"></a>Android 平台基本播放逻辑</h2><p>在正式开发改造之前，对 Android 侧的一个播放器渲染流程进行简单的介绍，以 MediaPlayer 为例，利用 MediaPlayer 进行视频解码渲染，并将视频最后输出到 SurfaceView 上,一次播放器视频渲染到View上的的主要代码流程为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initPlayer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MediaPlayer</span> <span class=\"variable\">mediaPlayer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MediaPlayer</span>();</span><br><span class=\"line\">    <span class=\"type\">SurfaceView</span> <span class=\"variable\">surfaceView</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceView</span>(activity);</span><br><span class=\"line\">    surfaceHolder = surfaceView.getHolder();</span><br><span class=\"line\">    surfaceHolder.addCallback(^ &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">surfaceCreated</span><span class=\"params\">(SurfaceHolder holder)</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">Surface</span> <span class=\"variable\">surface</span> <span class=\"operator\">=</span> holder.getSurface();</span><br><span class=\"line\">            mediaPlayer.setSurface(surface);</span><br><span class=\"line\">            mediaPlayer.prepareAsync();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         ……</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    mediaPlayer.setDataSource(URI...);</span><br><span class=\"line\">    mediaPlayer.setOnPreparedListener(mp -&gt; mp.start());   </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n<p>对于渲染 <code>mediaPlayer.setSurface(surface)</code> 设为播放器解码数据的接受器，Surface 来自于 SurfaceView。</p>\n<p>播放器是将数据图形绘制在 Surface 对象上，Surface中会关联一个 BufferQueue 用于提供图像数据缓存，SurfaceFlinger 会把 Surface 对应的图像层混合在一起，将其输出到 FrameBuffer 中（Framebuffer就是一块内存区域，它通常是显示驱动的内部缓冲区在内存中的映射），最后在屏幕上看到合成的图像。</p>\n<p>整个流程引入外部大佬的一张图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg\">\n<h2 id=\"Unity-中的一些改造\"><a href=\"#Unity-中的一些改造\" class=\"headerlink\" title=\"Unity 中的一些改造\"></a>Unity 中的一些改造</h2><p>上面的流程最终是通过播放器解码渲染到 SurfaceView 上，当然，你可以通过获取到 UnityPlayer 对应的 Acitivity 将这个 SurfaceView 动态添加到当前界面，实现“在 Unity 中利用 Android 能力进行视频渲染”。</p>\n<p>所以需要对其进行改造，我们的目的是实现 Android 播放器数据渲染到 Untiy 的组件中。实现这一过程需要借助 FBO(Frame Buffer Object) 的能力。</p>\n<h3 id=\"（一）FBO\"><a href=\"#（一）FBO\" class=\"headerlink\" title=\"（一）FBO\"></a>（一）FBO</h3><p>在 OpenGL 渲染管线中几何数据和纹理经过变换和一些测试处理，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存(framebuffer)，OpenGL渲染管线的最终位置是在帧缓冲区中，默认情况下 OpenGL 使用的是窗口系统提供的帧缓冲区。</p>\n<p>但有些场景是不想要直接渲染到窗口上的(例如加视频特效)，于是 OpenGL 提供了一种方式来创建额外的帧缓冲区对象(FBO)。使用帧缓冲区对象，OpenGL 可以将原先绘制到窗口提供的帧缓冲区重定向到 FBO 之中。FBO本身不是一块内存，没有空间，真正存储东西，可实际读写的是依附于FBO的东西：纹理(texture)和渲染缓存(renderbuffer)，依附的方式，是一个二维数组来管理，结构如图所示：</p>\n<p><img src=\"https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&amp;width=380\" alt=\"\"></p>\n<h3 id=\"（二）具体实现\"><a href=\"#（二）具体实现\" class=\"headerlink\" title=\"（二）具体实现\"></a>（二）具体实现</h3><p>使用 FBO 我们可以将渲染目标渲染到其他的空间，我们目的是将播放器解码后的数据渲染到 Unity 控件的纹理空间中。<br>渲染播放器将输出到 FBO 中，FBO 指向 Unity 控件数据的输入，从而实现：Android 的播放器输出数据显示到 Unity 的控件中。</p>\n<h3 id=\"（三）从渲染输出数据到外部纹理\"><a href=\"#（三）从渲染输出数据到外部纹理\" class=\"headerlink\" title=\"（三）从渲染输出数据到外部纹理\"></a>（三）从渲染输出数据到外部纹理</h3><p>由于 <code>mediaPlayer.setSurface(surface)</code> 对应的 Surface 来源于 SurafaceView，会直接渲染到屏幕上，这里我们需要使用 构造一个新的 SurfaceTexture 以将图像流式传输到给定的 OpenGL 纹理;</p>\n<p>要获取到播放器渲染得数据，需要借助 SurfaceTexture ，SurfaceTexture 是Surface 和 OpenGL ES 纹理的结合，其对图像流的处理并不直接显示，而是从图像流中捕获帧作为 OpenGL 的外部纹理，图像流来自相机预览和视频解码。</p>\n<p>SurfaceTexture 创建的 Surface 是数据的生产者，而 SurfaceTexture 是对应的消费者，Surface 接收媒体数据并将数据发送到 SurfaceTexture，当调用 updateTexImage 的时候，创建SurfaceTexture 的纹理对象相应的内容将更新为最新图像帧，也就是会将图像帧转换为 GL 纹理，并将该纹理绑定到 GL_TEXTURE_EXTERNAL_OES 纹理对象上。具体实现逻辑参考：<a href=\"https://juejin.cn/post/7012517274768179236\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">SurfaceTexture</span> <span class=\"variable\">surfaceTexture</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceTexture</span>(videoTextureId);</span><br><span class=\"line\">player.setUpSurface(<span class=\"keyword\">new</span> <span class=\"title class_\">Surface</span>(surfaceTexture), width, height);</span><br><span class=\"line\">surfaceTexture.setDefaultBufferSize(width, height);</span><br><span class=\"line\">surfaceTexture.setOnFrameAvailableListener(surfaceTexture -&gt; &#123;……&#125;);;</span><br></pre></td></tr></table></figure>\n<p>其中 videoTextureId 来源于创建的 OES 纹理：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createOESTextureID</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span>[] texture = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"comment\">// 创建纹理对象，一个容器对象，保存渲染所需要的纹理数据，例如：图像数据</span></span><br><span class=\"line\">        <span class=\"comment\">//在OpenGL 中纹理对象是一个无符号整数，是一个纹理对象的句柄</span></span><br><span class=\"line\">        GLES30.glGenTextures(texture.length, texture, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 绑定纹理ID到纹理单元的纹理目标上</span></span><br><span class=\"line\">        GLES30.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, texture[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 设置纹理参数</span></span><br><span class=\"line\">        ……</span><br><span class=\"line\"></span><br><span class=\"line\">        GLES30.glGenerateMipmap(GLES11Ext.GL_TEXTURE_EXTERNAL_OES);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> texture[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n<h3 id=\"（四）FBO纹理数据到-Unity-的纹理数据\"><a href=\"#（四）FBO纹理数据到-Unity-的纹理数据\" class=\"headerlink\" title=\"（四）FBO纹理数据到 Unity 的纹理数据\"></a>（四）FBO纹理数据到 Unity 的纹理数据</h3><p>学习了解到Unity中可以使用 RawImage 或者 quad 等相关控件可以显示纹理，这里以 RawImage 为例。在 Unity 脚本编写初始化的逻辑，构造一个 Texture2D 对象，将句柄传递到 Android，并赋值给 RawImage，并将texture id 传递到 Android 平台，完成一次渲染的重定向。<br><figure class=\"highlight c#\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">InitPlayer</span>()</span></span><br><span class=\"line\"> &#123;    </span><br><span class=\"line\">    Texture2D texture2D = <span class=\"keyword\">new</span> Texture2D(width, height, TextureFormat.RGB24, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>);</span><br><span class=\"line\">    androidObj.Call(<span class=\"string\">&quot;init&quot;</span>, (<span class=\"built_in\">int</span>)texture2D.GetNativeTexturePtr(), width, height);</span><br><span class=\"line\">    RawImage.texture = texture2D;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>创建FBO<br><figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createFBO</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>[] fbo = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">    GLES30.glGenFramebuffers(fbo.length, fbo, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fbo[<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure></p>\n<p>为<code>SurfaceTexture</code> 设置了 <code>OnFrameAvailableListener</code> 后，当有新的图形流数据生成之后，就可以通过  <code>mSurfaceTexture.updateTexImage()</code> 将当前图片流更新到纹理所关联的OpenGLES中纹理，并绘制 FBO.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">publc <span class=\"keyword\">void</span> <span class=\"title function_\">draw</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 绑定 FrameBuffer 到当前的绘制环境上， 后续 GL 绘制都会到这个 framebuffer</span></span><br><span class=\"line\">    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fbo[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//2.把一个2D纹理作为帧缓冲区附着</span></span><br><span class=\"line\">    <span class=\"comment\">//即所有渲染操作的结果将会被储存在 unityTextureId 对应的纹理图像中</span></span><br><span class=\"line\">    GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, unityTextureId, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//绑定指定纹理到当前激活的纹理单元</span></span><br><span class=\"line\">    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, videoTextureId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//…… 省略 Opengl 绘制的常规流程</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>这一步是最关键的，实现了将 FBO 的输出指向 Unity 里面创建的纹理，也就实现了 Android 渲染与 Unity 之间的数据打通。</p>\n<p>这里的 unityTextureId 来源于在 Unity 中初始化的 <code>(int)texture2D.GetNativeTexturePtr()</code>值。</p>\n<p>整体的流程为：</p>\n<img src=\"https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png\">\n<p>效果图：</p>\n<img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif\">\n<p>图中播放视频区域为 Unity 的 RawImage 控件，渲染的视频通过 Pag 等相关素材由渲染SDK合成。</p>\n<p>如图所示，视频画面正常地进行渲染，图中有两个区域展示了视频画面，上面的使用的 Quad 组件，下面是用的 RawImage，流程都一直，只是在 Unity 使用 Texture2D 的时候通过 <code>Quad.mainTexture = texture2D</code> 赋值。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲了 Unity 利用 Android 提供的能力进行视频相关的特效渲染的方案，总体正常运行。还需要一些优化，例如对 <code>Multithreaded Rendering</code>配置还未支持，以及一些逻辑可能受限于游戏侧的配置，例如图形渲染的配置使用的 OpenGL3.0，如果使用 OpenGL2.0 或者 Vulkan，还需要单独调整相关逻辑。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"},{"name":"Unity","slug":"Unity","api":"api/tags/Unity.json"}],"api":"api/posts/2022/10/25/Unity-实现利用-Andorid-能力进行视频渲染播放.json"},{"title":"FFmpeg之AVFrame转Android Bitmap","slug":"FFmpeg之AVFrame转Android-Bitmap","date":"2022-05-22T13:03:00.000Z","updated":"2025-09-15T13:06:47.170Z","comments":true,"url":"2022/05/22/FFmpeg之AVFrame转Android-Bitmap/","excerpt":"<p>此前很多工作都设计到使用 FFmpeg 对视频帧进行获取，在 FFmpeg 解码视频文件获取到帧数据结构是 <code>AVFrame</code>, 对于应用层我们没有办法直接拿到进行数据处理，需要转换为 Android 平台特有的处理结构。而我是需要对应的帧图片数据，那么在 Android 侧需要将其转化为 <code>Bitmap</code> ,之前整理的过程中发现了这篇<a href=\"https://segmentfault.com/a/1190000016674715?utm_source=sf-similar-article\">《Android音视频开发】从AVFrame到MediaFrame数组(二)》</a>博客文章 ，觉得写得很不错，非常精简，适合我的需求，于是对齐进行整理，并标注一下自己在过程中遇到的一些坑点。</p>\n<h2 id=\"Native层创建Bitmap\"><a href=\"#Native层创建Bitmap\" class=\"headerlink\" title=\"Native层创建Bitmap\"></a>Native层创建Bitmap</h2><p><code>Bitmap</code> 是对 <a href=\"https://docs.microsoft.com/en-us/dotnet/api/skiasharp.skbitmap?view=skiasharp-2.80.2\">SkBitmap</a> 的包装。具体说来， Bitmap 的实现包括 Java 层和 JNI 层，JNI 层依赖 Skia，<code>SkBitmap</code> 本质上可简单理解为内存中的一个字节数组</p>\n<p>想要生成 <code>Bitmap</code>,  我们首先需要构造一个 <code>Bitmap</code> 对象，Java层有很多种方式可以生成Bitmap对象，最简单的方式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Bitmap.createBitmap(width,height,<span class=\"keyword\">new</span> <span class=\"title class_\">Bitmap</span>.Config.ARGB_8888)</span><br></pre></td></tr></table></figure>\n<p>由于整个 <code>FFmpeg</code>的操作在 JNI 侧进行，对应的操作需要使用 <code>JNIEnv</code>  进行相关的调用，主要逻辑如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">jobject <span class=\"title\">create_bitmap</span><span class=\"params\">(JNIEnv *env, <span class=\"type\">int</span> width, <span class=\"type\">int</span> height)</span> </span>&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 找到 Bitmap.class 和 该类中的 createBitmap 方法</span></span><br><span class=\"line\">    jclass clz_bitmap = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_bitmap = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_bitmap, <span class=\"string\">&quot;createBitmap&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;(IILandroid/graphics/Bitmap$Config;)Landroid/graphics/Bitmap;&quot;</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 配置 Bitmap</span></span><br><span class=\"line\">    jstring str_config = env-&gt;<span class=\"built_in\">NewStringUTF</span>(<span class=\"string\">&quot;ARGB_8888&quot;</span>);</span><br><span class=\"line\">    jclass clz_config = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap$Config&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_config = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_config, <span class=\"string\">&quot;valueOf&quot;</span>, <span class=\"string\">&quot;(Ljava/lang/String;)Landroid/graphics/Bitmap$Config;&quot;</span>);</span><br><span class=\"line\">    jobject obj_config = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(clz_config, mtd_config, str_config);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 创建 Bitmap 对象</span></span><br><span class=\"line\">    jobject bitmap = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(</span><br><span class=\"line\">            clz_bitmap, mtd_bitmap, width, height, obj_config);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> bitmap;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"获取Bitmap像素数据地址，并锁定\"><a href=\"#获取Bitmap像素数据地址，并锁定\" class=\"headerlink\" title=\"获取Bitmap像素数据地址，并锁定\"></a>获取Bitmap像素数据地址，并锁定</h2><figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *addr_pixels;</span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br></pre></td></tr></table></figure>\n<p>解释一下这两句话：</p>\n<blockquote>\n<p>第一句的作用声明并定义一个指向任意类型的指针变量，名称是addr_pixels。我们定义它的目的，是让它指向bitmap像素数据(即:<br>addr_pixels的值为bitmap像素数据的地址)。注意哦，这时候，addr_pixels的值是一个随机的值(假定此时为：0x01)，由系统分配，它还不指向bitmap像素数据。<br>第二句话的作用就是将bitmap的像素数据地址赋值给addr_pixels，此时它的值被修改(假定为：0x002)。并且锁定该地址，保证不会被移动。【注：地址不会被移动这里我也不太懂什么意思，有兴趣的可以去查看该方法的API文档】<br>【注：】此时的bitmap由像素数据的地址，但是该地址内还没有任何像素数据哦，或者说它的像素数据为\\0</p>\n</blockquote>\n<p>到这里，我们已经有了源像素数据在AVFrame中，有了目的像素数据地址addr_pixels，那么接下来的任务就是将AVFrame中的像素数据写入到addr_pixels指向的那片内存中去。</p>\n<h2 id=\"向Bitmap中写入像素数据\"><a href=\"#向Bitmap中写入像素数据\" class=\"headerlink\" title=\"向Bitmap中写入像素数据\"></a>向Bitmap中写入像素数据</h2><p>这里要说一下，我们获取到的AVFrame的像素格式通常是YUV格式的，而Bitmap的像素格式通常是RGB格式的。因此我们需要将YUV格式的像素数据转换成RGB格式进行存储。而RGB的存储空间Bitmap不是已经给我门提供好了吗？嘿嘿，直接用就OK了，那现在问题就是YUV如何转换成RGB呢？<br>关于YUV和RGB之间的转换，我知道的有三种方式：</p>\n<ul>\n<li>通过公式换算</li>\n<li>FFmpeg提供的libswscale</li>\n<li>Google提供的libyuv<br>这里我们选择libyuv因为它的性能好、使用简单。</li>\n</ul>\n<p>说它使用简单，到底有多简单，嘿，一个函数就够了！！</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">libyuv::<span class=\"built_in\">I420ToABGR</span>(frame-&gt;data[<span class=\"number\">0</span>], frame-&gt;linesize[<span class=\"number\">0</span>], <span class=\"comment\">// Y</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">1</span>], frame-&gt;linesize[<span class=\"number\">1</span>], <span class=\"comment\">// U</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">2</span>], frame-&gt;linesize[<span class=\"number\">2</span>], <span class=\"comment\">// V</span></span><br><span class=\"line\">                   (<span class=\"type\">uint8_t</span> *) addr_pixels, linesize,  <span class=\"comment\">// RGBA</span></span><br><span class=\"line\">                   frame-&gt;width, frame-&gt;height);</span><br></pre></td></tr></table></figure>\n<p>解释一下这个函数：</p>\n<ul>\n<li>I420ToABGR: I420表示的是YUV420P格式，ABGR表示的RGBA格式(execuse me?? 是的，你没看错，Google说RGBA格式的数据在底层的存储方式是ABGR，顺序反过来，看下libyuv源码的函数注释就知道了)</li>\n<li>frame-&gt;data&amp;linesize: 这些个参数表示的是源YUV数据，上面有标注</li>\n<li>(uint8_t *) addr_pixels: 嘿，这个就是说往这块空间里写入像素数据啦</li>\n<li>linesize: 这个表示的是该图片一行数据的字节大小，Bitmap按照RBGA格式存储，也就是说一个像素是4个字节，那么一行共有：frame-&gt;width 个像素，所以：<br>linesize = frame-&gt; width * 4</li>\n</ul>\n<p>【注：】关于这一小块功能的实现，可能其他地方你会看到这样的写法，他们用了如下接口：</p>\n<p>// 思路</p>\n<blockquote>\n<p>是：新建一个AVFrame(RGB格式)，通过av_image_fill_arrays来实现AVFrame(RGB)中像素数据和Bitmap像素数据的关联，也就是让AVFrame(RGB)像素数据指针等于addr_pixels<br>pRGBFrame = av_frame_alloc() av_image_get_buffer_size()<br>av_image_fill_arrays() /<em><br>我也是写到这里的时候，才想到这个问题，为什么要这样用呢，直接使用addr_pixels不是也一样可以么？<br>不过大家都这么用，应该是有它不可替代的使用场景的。因此这里也说一下av_image_fill_arrays这个函数。\n</em>/</p>\n<p>// TODO: 解释下这个函数的作用 av_image_fill_arrays(dst_data, dst_linesize,<br>src_data, pix_fmt, width, height, align); 它的作用就是</p>\n<ol>\n<li>根据src_data，设置dst_data，事实上根据现象或者自己去调试，可以发现dst_data的值就是src_data的值(我印象中好像值是相同的，这会我忘了，后面我再验证下)</li>\n<li>根据pix_fmt, width, height设置linesize的值，其实linesize的计算就和我上面给出的那个公式是一样子的值</li>\n</ol>\n</blockquote>\n<p>OK, 函数执行完毕，我们Bitmap就有了像素数据，下面就是把Bitmap上传给Java层</p>\n<h2 id=\"Native回调Java接口\"><a href=\"#Native回调Java接口\" class=\"headerlink\" title=\"Native回调Java接口\"></a>Native回调Java接口</h2><p>说下Java层</p>\n<p>有一个MainActivity.java用于界面的显示<br>有一个JNIHelper.java用于Java层和Native层的沟通</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">JNIHelper</span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Java层接收到Bitmap后，可以开始搞事情了</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Native层的回调代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">jclass clz = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;me/oogh/xplayer/JNIHelper&quot;</span>);</span><br><span class=\"line\">jmethodID method = env-&gt;<span class=\"built_in\">GetMethodID</span>(clz, <span class=\"string\">&quot;onReceived&quot;</span>, <span class=\"string\">&quot;(Landroid/graphics/Bitmap;)V&quot;</span>);</span><br><span class=\"line\">env-&gt;<span class=\"built_in\">CallVoidMethod</span>(obj, method, bitmap);</span><br></pre></td></tr></table></figure>\n<h2 id=\"AndroidBitmap-lockPixels-方法\"><a href=\"#AndroidBitmap-lockPixels-方法\" class=\"headerlink\" title=\"AndroidBitmap_lockPixels 方法\"></a>AndroidBitmap_lockPixels 方法</h2><p>以上就是整个文章的内容，使用起来也是 no problem!  但使用过程中遇到的问题就是内存回收的问题，最开始使用的时候并没有过多关注JNI层 <code>AndroidBitmap_lockPixels</code>这个方法，以至于后来我在处理Bitmap内存回收上遇到了一些问题。 <code>AndroidBitmap_lockPixels</code> 与之对应还有一个   <code>AndroidBitmap_unlockPixels</code></p>\n<p><code>AndroidBitmap_lockPixels</code> “函数作用锁定了像素缓存以确保像素的内存不会被移动”，这句话看起来好像挺难理解，但是我们在 Java层面有与之类似的操作，那就是 <code>SurfaceHolder.lockCanvas()</code>，还记得我们在绘制的过程中需要先使用 <code>lockCanvas</code> 锁定画布，返回的画布对象<code>Canvas</code>然后使用 <code>unlockCanvasAndPost(Canvas canvas)</code> 结束锁定画布，并提交改变。<code>AndroidBitmap_lockPixels</code> 与  <code>AndroidBitmap_unlockPixels</code>做的是类似的事情，都是锁住一块内存区域，保证其安全。</p>\n<p>回到上面说的内存回收的问题，由于自己使用失误，流程大概是这样：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br><span class=\"line\"><span class=\"comment\">//在两者之间，将生成好的 Bitmap Obj 回调到Java层</span></span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_unlockPixels</span>(env, bitmap);</span><br></pre></td></tr></table></figure>\n<p>然后在Java层有这样的逻辑：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//……一些业务逻辑</span></span><br><span class=\"line\">    <span class=\"comment\">//我们习惯性对bitmap使用recycle对其数据进行回收……</span></span><br><span class=\"line\">    bitmap.recycle()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>但是我发现使用了 <code>bitmap.recycle()</code>与不使用，内存中 Native区域仍然占了一大部分，后来在<code>AndroidBitmap_lockPixels</code>的注释才发现不对的地方：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Given a java bitmap object, attempt to lock the pixel address.</span></span><br><span class=\"line\"><span class=\"comment\"> * Locking will ensure that the memory for the pixels will not move</span></span><br><span class=\"line\"><span class=\"comment\"> * until the unlockPixels call, and ensure that, if the pixels had been</span></span><br><span class=\"line\"><span class=\"comment\"> * previously purged, they will have been restored.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this call succeeds, it must be balanced by a call to</span></span><br><span class=\"line\"><span class=\"comment\"> * AndroidBitmap_unlockPixels, after which time the address of the pixels should</span></span><br><span class=\"line\"><span class=\"comment\"> * no longer be used.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this succeeds, *addrPtr will be set to the pixel address. If the call</span></span><br><span class=\"line\"><span class=\"comment\"> * fails, addrPtr will be ignored.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">AndroidBitmap_lockPixels</span><span class=\"params\">(JNIEnv* env, jobject jbitmap, <span class=\"type\">void</span>** addrPtr)</span></span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>其中：</p>\n<blockquote>\n<p><strong>if the pixels had been previously purged, they will have been restored.</strong></p>\n</blockquote>\n<p>也就是说在<code>AndroidBitmap_unlockPixels</code> 调用之前，如果像素数据被销毁了，他们会被恢复！至于为什么会被恢复，这个就需要之后再进行研究了。</p>\n<p>后来对逻辑进行更改，将 Bitmap.recycle()的逻辑移动到 AndroidBitmap_unlockPixels之后。</p>\n","cover":null,"images":[],"content":"<p>此前很多工作都设计到使用 FFmpeg 对视频帧进行获取，在 FFmpeg 解码视频文件获取到帧数据结构是 <code>AVFrame</code>, 对于应用层我们没有办法直接拿到进行数据处理，需要转换为 Android 平台特有的处理结构。而我是需要对应的帧图片数据，那么在 Android 侧需要将其转化为 <code>Bitmap</code> ,之前整理的过程中发现了这篇<a href=\"https://segmentfault.com/a/1190000016674715?utm_source=sf-similar-article\">《Android音视频开发】从AVFrame到MediaFrame数组(二)》</a>博客文章 ，觉得写得很不错，非常精简，适合我的需求，于是对齐进行整理，并标注一下自己在过程中遇到的一些坑点。</p>\n<h2 id=\"Native层创建Bitmap\"><a href=\"#Native层创建Bitmap\" class=\"headerlink\" title=\"Native层创建Bitmap\"></a>Native层创建Bitmap</h2><p><code>Bitmap</code> 是对 <a href=\"https://docs.microsoft.com/en-us/dotnet/api/skiasharp.skbitmap?view=skiasharp-2.80.2\">SkBitmap</a> 的包装。具体说来， Bitmap 的实现包括 Java 层和 JNI 层，JNI 层依赖 Skia，<code>SkBitmap</code> 本质上可简单理解为内存中的一个字节数组</p>\n<p>想要生成 <code>Bitmap</code>,  我们首先需要构造一个 <code>Bitmap</code> 对象，Java层有很多种方式可以生成Bitmap对象，最简单的方式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Bitmap.createBitmap(width,height,<span class=\"keyword\">new</span> <span class=\"title class_\">Bitmap</span>.Config.ARGB_8888)</span><br></pre></td></tr></table></figure>\n<p>由于整个 <code>FFmpeg</code>的操作在 JNI 侧进行，对应的操作需要使用 <code>JNIEnv</code>  进行相关的调用，主要逻辑如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">jobject <span class=\"title\">create_bitmap</span><span class=\"params\">(JNIEnv *env, <span class=\"type\">int</span> width, <span class=\"type\">int</span> height)</span> </span>&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 找到 Bitmap.class 和 该类中的 createBitmap 方法</span></span><br><span class=\"line\">    jclass clz_bitmap = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_bitmap = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_bitmap, <span class=\"string\">&quot;createBitmap&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;(IILandroid/graphics/Bitmap$Config;)Landroid/graphics/Bitmap;&quot;</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 配置 Bitmap</span></span><br><span class=\"line\">    jstring str_config = env-&gt;<span class=\"built_in\">NewStringUTF</span>(<span class=\"string\">&quot;ARGB_8888&quot;</span>);</span><br><span class=\"line\">    jclass clz_config = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap$Config&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_config = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_config, <span class=\"string\">&quot;valueOf&quot;</span>, <span class=\"string\">&quot;(Ljava/lang/String;)Landroid/graphics/Bitmap$Config;&quot;</span>);</span><br><span class=\"line\">    jobject obj_config = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(clz_config, mtd_config, str_config);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 创建 Bitmap 对象</span></span><br><span class=\"line\">    jobject bitmap = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(</span><br><span class=\"line\">            clz_bitmap, mtd_bitmap, width, height, obj_config);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> bitmap;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<h2 id=\"获取Bitmap像素数据地址，并锁定\"><a href=\"#获取Bitmap像素数据地址，并锁定\" class=\"headerlink\" title=\"获取Bitmap像素数据地址，并锁定\"></a>获取Bitmap像素数据地址，并锁定</h2><figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *addr_pixels;</span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br></pre></td></tr></table></figure>\n<p>解释一下这两句话：</p>\n<blockquote>\n<p>第一句的作用声明并定义一个指向任意类型的指针变量，名称是addr_pixels。我们定义它的目的，是让它指向bitmap像素数据(即:<br>addr_pixels的值为bitmap像素数据的地址)。注意哦，这时候，addr_pixels的值是一个随机的值(假定此时为：0x01)，由系统分配，它还不指向bitmap像素数据。<br>第二句话的作用就是将bitmap的像素数据地址赋值给addr_pixels，此时它的值被修改(假定为：0x002)。并且锁定该地址，保证不会被移动。【注：地址不会被移动这里我也不太懂什么意思，有兴趣的可以去查看该方法的API文档】<br>【注：】此时的bitmap由像素数据的地址，但是该地址内还没有任何像素数据哦，或者说它的像素数据为\\0</p>\n</blockquote>\n<p>到这里，我们已经有了源像素数据在AVFrame中，有了目的像素数据地址addr_pixels，那么接下来的任务就是将AVFrame中的像素数据写入到addr_pixels指向的那片内存中去。</p>\n<h2 id=\"向Bitmap中写入像素数据\"><a href=\"#向Bitmap中写入像素数据\" class=\"headerlink\" title=\"向Bitmap中写入像素数据\"></a>向Bitmap中写入像素数据</h2><p>这里要说一下，我们获取到的AVFrame的像素格式通常是YUV格式的，而Bitmap的像素格式通常是RGB格式的。因此我们需要将YUV格式的像素数据转换成RGB格式进行存储。而RGB的存储空间Bitmap不是已经给我门提供好了吗？嘿嘿，直接用就OK了，那现在问题就是YUV如何转换成RGB呢？<br>关于YUV和RGB之间的转换，我知道的有三种方式：</p>\n<ul>\n<li>通过公式换算</li>\n<li>FFmpeg提供的libswscale</li>\n<li>Google提供的libyuv<br>这里我们选择libyuv因为它的性能好、使用简单。</li>\n</ul>\n<p>说它使用简单，到底有多简单，嘿，一个函数就够了！！</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">libyuv::<span class=\"built_in\">I420ToABGR</span>(frame-&gt;data[<span class=\"number\">0</span>], frame-&gt;linesize[<span class=\"number\">0</span>], <span class=\"comment\">// Y</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">1</span>], frame-&gt;linesize[<span class=\"number\">1</span>], <span class=\"comment\">// U</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">2</span>], frame-&gt;linesize[<span class=\"number\">2</span>], <span class=\"comment\">// V</span></span><br><span class=\"line\">                   (<span class=\"type\">uint8_t</span> *) addr_pixels, linesize,  <span class=\"comment\">// RGBA</span></span><br><span class=\"line\">                   frame-&gt;width, frame-&gt;height);</span><br></pre></td></tr></table></figure>\n<p>解释一下这个函数：</p>\n<ul>\n<li>I420ToABGR: I420表示的是YUV420P格式，ABGR表示的RGBA格式(execuse me?? 是的，你没看错，Google说RGBA格式的数据在底层的存储方式是ABGR，顺序反过来，看下libyuv源码的函数注释就知道了)</li>\n<li>frame-&gt;data&amp;linesize: 这些个参数表示的是源YUV数据，上面有标注</li>\n<li>(uint8_t *) addr_pixels: 嘿，这个就是说往这块空间里写入像素数据啦</li>\n<li>linesize: 这个表示的是该图片一行数据的字节大小，Bitmap按照RBGA格式存储，也就是说一个像素是4个字节，那么一行共有：frame-&gt;width 个像素，所以：<br>linesize = frame-&gt; width * 4</li>\n</ul>\n<p>【注：】关于这一小块功能的实现，可能其他地方你会看到这样的写法，他们用了如下接口：</p>\n<p>// 思路</p>\n<blockquote>\n<p>是：新建一个AVFrame(RGB格式)，通过av_image_fill_arrays来实现AVFrame(RGB)中像素数据和Bitmap像素数据的关联，也就是让AVFrame(RGB)像素数据指针等于addr_pixels<br>pRGBFrame = av_frame_alloc() av_image_get_buffer_size()<br>av_image_fill_arrays() /<em><br>我也是写到这里的时候，才想到这个问题，为什么要这样用呢，直接使用addr_pixels不是也一样可以么？<br>不过大家都这么用，应该是有它不可替代的使用场景的。因此这里也说一下av_image_fill_arrays这个函数。\n</em>/</p>\n<p>// TODO: 解释下这个函数的作用 av_image_fill_arrays(dst_data, dst_linesize,<br>src_data, pix_fmt, width, height, align); 它的作用就是</p>\n<ol>\n<li>根据src_data，设置dst_data，事实上根据现象或者自己去调试，可以发现dst_data的值就是src_data的值(我印象中好像值是相同的，这会我忘了，后面我再验证下)</li>\n<li>根据pix_fmt, width, height设置linesize的值，其实linesize的计算就和我上面给出的那个公式是一样子的值</li>\n</ol>\n</blockquote>\n<p>OK, 函数执行完毕，我们Bitmap就有了像素数据，下面就是把Bitmap上传给Java层</p>\n<h2 id=\"Native回调Java接口\"><a href=\"#Native回调Java接口\" class=\"headerlink\" title=\"Native回调Java接口\"></a>Native回调Java接口</h2><p>说下Java层</p>\n<p>有一个MainActivity.java用于界面的显示<br>有一个JNIHelper.java用于Java层和Native层的沟通</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">JNIHelper</span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Java层接收到Bitmap后，可以开始搞事情了</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>Native层的回调代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">jclass clz = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;me/oogh/xplayer/JNIHelper&quot;</span>);</span><br><span class=\"line\">jmethodID method = env-&gt;<span class=\"built_in\">GetMethodID</span>(clz, <span class=\"string\">&quot;onReceived&quot;</span>, <span class=\"string\">&quot;(Landroid/graphics/Bitmap;)V&quot;</span>);</span><br><span class=\"line\">env-&gt;<span class=\"built_in\">CallVoidMethod</span>(obj, method, bitmap);</span><br></pre></td></tr></table></figure>\n<h2 id=\"AndroidBitmap-lockPixels-方法\"><a href=\"#AndroidBitmap-lockPixels-方法\" class=\"headerlink\" title=\"AndroidBitmap_lockPixels 方法\"></a>AndroidBitmap_lockPixels 方法</h2><p>以上就是整个文章的内容，使用起来也是 no problem!  但使用过程中遇到的问题就是内存回收的问题，最开始使用的时候并没有过多关注JNI层 <code>AndroidBitmap_lockPixels</code>这个方法，以至于后来我在处理Bitmap内存回收上遇到了一些问题。 <code>AndroidBitmap_lockPixels</code> 与之对应还有一个   <code>AndroidBitmap_unlockPixels</code></p>\n<p><code>AndroidBitmap_lockPixels</code> “函数作用锁定了像素缓存以确保像素的内存不会被移动”，这句话看起来好像挺难理解，但是我们在 Java层面有与之类似的操作，那就是 <code>SurfaceHolder.lockCanvas()</code>，还记得我们在绘制的过程中需要先使用 <code>lockCanvas</code> 锁定画布，返回的画布对象<code>Canvas</code>然后使用 <code>unlockCanvasAndPost(Canvas canvas)</code> 结束锁定画布，并提交改变。<code>AndroidBitmap_lockPixels</code> 与  <code>AndroidBitmap_unlockPixels</code>做的是类似的事情，都是锁住一块内存区域，保证其安全。</p>\n<p>回到上面说的内存回收的问题，由于自己使用失误，流程大概是这样：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br><span class=\"line\"><span class=\"comment\">//在两者之间，将生成好的 Bitmap Obj 回调到Java层</span></span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_unlockPixels</span>(env, bitmap);</span><br></pre></td></tr></table></figure>\n<p>然后在Java层有这样的逻辑：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//……一些业务逻辑</span></span><br><span class=\"line\">    <span class=\"comment\">//我们习惯性对bitmap使用recycle对其数据进行回收……</span></span><br><span class=\"line\">    bitmap.recycle()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>但是我发现使用了 <code>bitmap.recycle()</code>与不使用，内存中 Native区域仍然占了一大部分，后来在<code>AndroidBitmap_lockPixels</code>的注释才发现不对的地方：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Given a java bitmap object, attempt to lock the pixel address.</span></span><br><span class=\"line\"><span class=\"comment\"> * Locking will ensure that the memory for the pixels will not move</span></span><br><span class=\"line\"><span class=\"comment\"> * until the unlockPixels call, and ensure that, if the pixels had been</span></span><br><span class=\"line\"><span class=\"comment\"> * previously purged, they will have been restored.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this call succeeds, it must be balanced by a call to</span></span><br><span class=\"line\"><span class=\"comment\"> * AndroidBitmap_unlockPixels, after which time the address of the pixels should</span></span><br><span class=\"line\"><span class=\"comment\"> * no longer be used.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this succeeds, *addrPtr will be set to the pixel address. If the call</span></span><br><span class=\"line\"><span class=\"comment\"> * fails, addrPtr will be ignored.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">AndroidBitmap_lockPixels</span><span class=\"params\">(JNIEnv* env, jobject jbitmap, <span class=\"type\">void</span>** addrPtr)</span></span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>其中：</p>\n<blockquote>\n<p><strong>if the pixels had been previously purged, they will have been restored.</strong></p>\n</blockquote>\n<p>也就是说在<code>AndroidBitmap_unlockPixels</code> 调用之前，如果像素数据被销毁了，他们会被恢复！至于为什么会被恢复，这个就需要之后再进行研究了。</p>\n<p>后来对逻辑进行更改，将 Bitmap.recycle()的逻辑移动到 AndroidBitmap_unlockPixels之后。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"FFmpeg","slug":"FFmpeg","api":"api/tags/FFmpeg.json"},{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2022/05/22/FFmpeg之AVFrame转Android-Bitmap.json"},{"title":"Android音视频-初识FFmpeg","slug":"音视频-初识FFmpeg","date":"2021-11-14T01:49:00.000Z","updated":"2025-09-15T13:07:56.100Z","comments":true,"url":"2021/11/14/音视频-初识FFmpeg/","excerpt":"<p>已经很久没有写过技术博客了，这段时间加入了新公司，主要时间花在熟悉新业务的技术上。而新的业务主要跟音视频相关，关于音视频的尝试在加入新公司之前，自己有做相关demo的尝试与学习，可以参看<a href=\"https://github.com/VomPom/JProject/tree/master/app/src/main/java/wang/julis/jproject/example/media\">音视频相关学习demo</a>。当然，那都是自己“想当然”学习的一些东西，虽然实际工作中并没有派上太大的用处，但让我对音视频相关的基础知识有了一定的概念，对后面的技术尝试做了铺垫。第一个技术挑战比较大的就是进行：<strong>视频抽帧</strong>，关于视频抽帧网上有很多很多文章进行讲解，但……我始终没有找到一个效率很高的解决方案。直到我遇见了 ffmpeg，仿佛打开了新世界的大门……</p>\n<h2 id=\"关于FFmpeg\"><a href=\"#关于FFmpeg\" class=\"headerlink\" title=\"关于FFmpeg\"></a>关于FFmpeg</h2><p>刚接触 ffmpeg 时，我一脸懵逼，完全不知道该怎么做，也不知道在哪里开始进行学习，后来在<a href=\"https://blog.csdn.net/leixiaohua1020\">雷霄骅大神的博客</a>中渐渐找到了感觉，膜拜！不过雷神的博客代码是基于老版本的 ffmpeg api，推荐搭配<a href=\"https://github.com/FFmpeg/FFmpeg/tree/master/doc/examples\">官方example</a>，先跑通雷声的博客，再对照官方的例子对进行api相关接口的修改。</p>\n<p>当然，想要使用 ffmpeg编写代码之前，我们首先要做的是对 FFmpeg 进行so库编译，这一步也是难倒了众多的英雄好汉，引用<a href=\"https://juejin.cn/post/6844904039524597773\">FFmpeg so库编译</a>作者的话：</p>\n<blockquote>\n<p>为什么FFmpeg让人觉得很难搞？<br>我想主要是因为迈出第一步就很困难，连so库都编译不出来，后面的都是扯淡了。</p>\n</blockquote>\n<p>参考<a href=\"https://juejin.cn/post/6844904039524597773\">FFmpeg so库编译</a>文章能成功地打包出 ffmpeg.so，接下来就是添加在项目中运行。</p>\n<h2 id=\"踏上-FFmpeg-音视频之路\"><a href=\"#踏上-FFmpeg-音视频之路\" class=\"headerlink\" title=\"踏上 FFmpeg 音视频之路\"></a>踏上 FFmpeg 音视频之路</h2><p>关于音视频等开发，无论是做特效渲染还是做视频播放，那么最重要也是最基本的步骤就是：<strong>音视频解码</strong></p>\n<p>众所周知的是视频是由一帧帧视频帧(图片)/音频帧编码组合而成</p>\n<p>视频解码要做的就是解码出视频文件中的每一帧，我们以:<strong>将视频转化为一帧帧的图片</strong>作为例进行学习。</p>\n<h2 id=\"FFmpeg-提取视频每一帧图像\"><a href=\"#FFmpeg-提取视频每一帧图像\" class=\"headerlink\" title=\"FFmpeg 提取视频每一帧图像\"></a>FFmpeg 提取视频每一帧图像</h2><p>在学习之前，我们思考一个问题：抛开 ffmpeg，如果让你去设计一个提取的代码，n你会怎么设计？</p>\n<p>因为视频是以文件流的形式存在，我相信很多人一上来就能想到这样的结构：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (!EOF) &#123; <span class=\"comment\">//当文件流没有结束</span></span><br><span class=\"line\">    <span class=\"type\">Stream</span> <span class=\"variable\">stream</span> <span class=\"operator\">=</span> getStream(); <span class=\"comment\">//获取一定区域的stream</span></span><br><span class=\"line\">    <span class=\"type\">Frame</span> <span class=\"variable\">steam</span> <span class=\"operator\">=</span> getFrame(stream); <span class=\"comment\">//Stream转化为视频帧</span></span><br><span class=\"line\">    <span class=\"type\">Picture</span> <span class=\"variable\">picture</span> <span class=\"operator\">=</span> decodeFrame(steam); <span class=\"comment\">//将视频帧转化为 .jpeg等格式图片</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>的确是这样的，这里是给出一份ffmpeg提取视频帧图片的核心逻辑：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">AVFrame frame = av_frame_alloc(); </span><br><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (av_read_frame(fmt_ctx, &amp;avpkt) &gt;= <span class=\"number\">0</span>) &#123; <span class=\"comment\">// Return the next frame of a stream.</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (avpkt.stream_index == video_stream_index) &#123; <span class=\"comment\">//标识该AVPacket所属的视频/音频流。</span></span><br><span class=\"line\">            avcodec_send_packet(codeCtx, &amp;avpkt); <span class=\"comment\">//Supply raw packet data as input to a decoder.</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> (avcodec_receive_frame(codeCtx, frame) == <span class=\"number\">0</span>) &#123; <span class=\"comment\">//Return decoded output data from a decoder.</span></span><br><span class=\"line\">                <span class=\"built_in\">snprintf</span>(buf, <span class=\"keyword\">sizeof</span>(buf), <span class=\"string\">&quot;%s/frame-%d.jpg&quot;</span>, out_filename, frame_count);</span><br><span class=\"line\">                saveJpg(frame, buf);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            frame_count++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        av_packet_unref(&amp;avpkt);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        LOGE(<span class=\"string\">&quot;//Exit&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的代码块就是 ffmpeg 进行视频解码最核心的逻辑了，主要的注释也贴在了代码上，完整代码请查看<a href=\"https://github.com/VomPom/FFmpegLearn/blob/main/app/src/main/cpp/func/video_to_jpeg.cpp\">video_to_jpeg.cpp</a>，查看完整的代码后，会感觉到很惊讶：为什么这么复杂？特别是前面的初始化操作。放心，ffmpeg就像一套组合拳，有固定不变的套路，写一次就足够了，了解了其中的流程，之后理解起来就会很容易了。</p>\n<p>上面的代码我们还可以做一些其他处理，比如只获取关键帧、查找指定时间戳位置的帧、视频按2s一帧进行抽取、视频不保存为jpeg文件转化为Java的bitmap？</p>\n<p>这些实现需求也都是基于上述核心模块进行修改：</p>\n<p>如果<strong>想只获取关键帧</strong>，可以利用<code>AVFrame</code>对象的属性<code>AVFrame-&gt;key_frame</code>进行判断。</p>\n<p><strong>查找指定时间戳位置的帧</strong>：利用 <code>av_seek_frame</code>查找到指定帧时间最近的关键帧，然后依次进行编码，直到<code>pts</code>与目标时间相近</p>\n<p><strong>视频按2s一帧进行抽取</strong>：简单的操作可以去获取视频fps，比如视频25fps，可以使用一个计数器判断<code>if(frame_count%25==0)</code>,这时候则是刚好1s。当然这样子性能不太好。如果需要追求性能，那么也可以利用<code>av_seek_frame</code>，查找目标时间附近，然后循环进行解码直到目标时间。</p>\n<p><strong>视频不保存为jpeg文件转化为Java的Bitmap</strong>：只需要对最终获取的 <code>AVFrame</code>做不一样的操作进行了，获取到对应的buffer，再利用jni调用构造 Java 的 bitmap 对象。</p>\n<p>可以做的还有很多……</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>提取视频图片这个功能只是 FFmpeg 强大功能的九牛一毛，需要探究的还有很多很多……</p>\n<p>如果能跑起来 FFmpeg 最简单的例子，已经迈出了很大一步了，但如果要理解其中的原理，还需要更多的基础知识，以及像<code>AVPacket</code>、<code>AVFrame</code>、 <code>AVCodec</code> ……每一个类的数据结构，以及实现都需要仔细研究。</p>\n<p>自己在网上找到的 FFmpeg 相关的教程，以及自己想要去实现的功能的资源太少，很多东西都需要自己去摸索。有时候我总在怀疑：<strong>为什么这么基础且很实用的功能没有现成的轮子？</strong> 这可能也是现在音视频相关开发的现状吧，成熟可用的轮子相对而言较少，以及相关技术的分享可能不太好做。既然没有，那就靠自己一点点积累吧。</p>\n<p>学习之路，任重而道远呐。</p>\n","cover":null,"images":[],"content":"<p>已经很久没有写过技术博客了，这段时间加入了新公司，主要时间花在熟悉新业务的技术上。而新的业务主要跟音视频相关，关于音视频的尝试在加入新公司之前，自己有做相关demo的尝试与学习，可以参看<a href=\"https://github.com/VomPom/JProject/tree/master/app/src/main/java/wang/julis/jproject/example/media\">音视频相关学习demo</a>。当然，那都是自己“想当然”学习的一些东西，虽然实际工作中并没有派上太大的用处，但让我对音视频相关的基础知识有了一定的概念，对后面的技术尝试做了铺垫。第一个技术挑战比较大的就是进行：<strong>视频抽帧</strong>，关于视频抽帧网上有很多很多文章进行讲解，但……我始终没有找到一个效率很高的解决方案。直到我遇见了 ffmpeg，仿佛打开了新世界的大门……</p>\n<h2 id=\"关于FFmpeg\"><a href=\"#关于FFmpeg\" class=\"headerlink\" title=\"关于FFmpeg\"></a>关于FFmpeg</h2><p>刚接触 ffmpeg 时，我一脸懵逼，完全不知道该怎么做，也不知道在哪里开始进行学习，后来在<a href=\"https://blog.csdn.net/leixiaohua1020\">雷霄骅大神的博客</a>中渐渐找到了感觉，膜拜！不过雷神的博客代码是基于老版本的 ffmpeg api，推荐搭配<a href=\"https://github.com/FFmpeg/FFmpeg/tree/master/doc/examples\">官方example</a>，先跑通雷声的博客，再对照官方的例子对进行api相关接口的修改。</p>\n<p>当然，想要使用 ffmpeg编写代码之前，我们首先要做的是对 FFmpeg 进行so库编译，这一步也是难倒了众多的英雄好汉，引用<a href=\"https://juejin.cn/post/6844904039524597773\">FFmpeg so库编译</a>作者的话：</p>\n<blockquote>\n<p>为什么FFmpeg让人觉得很难搞？<br>我想主要是因为迈出第一步就很困难，连so库都编译不出来，后面的都是扯淡了。</p>\n</blockquote>\n<p>参考<a href=\"https://juejin.cn/post/6844904039524597773\">FFmpeg so库编译</a>文章能成功地打包出 ffmpeg.so，接下来就是添加在项目中运行。</p>\n<h2 id=\"踏上-FFmpeg-音视频之路\"><a href=\"#踏上-FFmpeg-音视频之路\" class=\"headerlink\" title=\"踏上 FFmpeg 音视频之路\"></a>踏上 FFmpeg 音视频之路</h2><p>关于音视频等开发，无论是做特效渲染还是做视频播放，那么最重要也是最基本的步骤就是：<strong>音视频解码</strong></p>\n<p>众所周知的是视频是由一帧帧视频帧(图片)/音频帧编码组合而成</p>\n<p>视频解码要做的就是解码出视频文件中的每一帧，我们以:<strong>将视频转化为一帧帧的图片</strong>作为例进行学习。</p>\n<h2 id=\"FFmpeg-提取视频每一帧图像\"><a href=\"#FFmpeg-提取视频每一帧图像\" class=\"headerlink\" title=\"FFmpeg 提取视频每一帧图像\"></a>FFmpeg 提取视频每一帧图像</h2><p>在学习之前，我们思考一个问题：抛开 ffmpeg，如果让你去设计一个提取的代码，n你会怎么设计？</p>\n<p>因为视频是以文件流的形式存在，我相信很多人一上来就能想到这样的结构：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span> (!EOF) &#123; <span class=\"comment\">//当文件流没有结束</span></span><br><span class=\"line\">    <span class=\"type\">Stream</span> <span class=\"variable\">stream</span> <span class=\"operator\">=</span> getStream(); <span class=\"comment\">//获取一定区域的stream</span></span><br><span class=\"line\">    <span class=\"type\">Frame</span> <span class=\"variable\">steam</span> <span class=\"operator\">=</span> getFrame(stream); <span class=\"comment\">//Stream转化为视频帧</span></span><br><span class=\"line\">    <span class=\"type\">Picture</span> <span class=\"variable\">picture</span> <span class=\"operator\">=</span> decodeFrame(steam); <span class=\"comment\">//将视频帧转化为 .jpeg等格式图片</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>的确是这样的，这里是给出一份ffmpeg提取视频帧图片的核心逻辑：</p>\n<figure class=\"highlight c\"><table><tr><td class=\"code\"><pre><span class=\"line\">AVFrame frame = av_frame_alloc(); </span><br><span class=\"line\"><span class=\"keyword\">while</span> (<span class=\"literal\">true</span>) &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (av_read_frame(fmt_ctx, &amp;avpkt) &gt;= <span class=\"number\">0</span>) &#123; <span class=\"comment\">// Return the next frame of a stream.</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (avpkt.stream_index == video_stream_index) &#123; <span class=\"comment\">//标识该AVPacket所属的视频/音频流。</span></span><br><span class=\"line\">            avcodec_send_packet(codeCtx, &amp;avpkt); <span class=\"comment\">//Supply raw packet data as input to a decoder.</span></span><br><span class=\"line\">            <span class=\"keyword\">while</span> (avcodec_receive_frame(codeCtx, frame) == <span class=\"number\">0</span>) &#123; <span class=\"comment\">//Return decoded output data from a decoder.</span></span><br><span class=\"line\">                <span class=\"built_in\">snprintf</span>(buf, <span class=\"keyword\">sizeof</span>(buf), <span class=\"string\">&quot;%s/frame-%d.jpg&quot;</span>, out_filename, frame_count);</span><br><span class=\"line\">                saveJpg(frame, buf);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            frame_count++;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        av_packet_unref(&amp;avpkt);</span><br><span class=\"line\">    &#125; <span class=\"keyword\">else</span> &#123;</span><br><span class=\"line\">        LOGE(<span class=\"string\">&quot;//Exit&quot;</span>);</span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>上面的代码块就是 ffmpeg 进行视频解码最核心的逻辑了，主要的注释也贴在了代码上，完整代码请查看<a href=\"https://github.com/VomPom/FFmpegLearn/blob/main/app/src/main/cpp/func/video_to_jpeg.cpp\">video_to_jpeg.cpp</a>，查看完整的代码后，会感觉到很惊讶：为什么这么复杂？特别是前面的初始化操作。放心，ffmpeg就像一套组合拳，有固定不变的套路，写一次就足够了，了解了其中的流程，之后理解起来就会很容易了。</p>\n<p>上面的代码我们还可以做一些其他处理，比如只获取关键帧、查找指定时间戳位置的帧、视频按2s一帧进行抽取、视频不保存为jpeg文件转化为Java的bitmap？</p>\n<p>这些实现需求也都是基于上述核心模块进行修改：</p>\n<p>如果<strong>想只获取关键帧</strong>，可以利用<code>AVFrame</code>对象的属性<code>AVFrame-&gt;key_frame</code>进行判断。</p>\n<p><strong>查找指定时间戳位置的帧</strong>：利用 <code>av_seek_frame</code>查找到指定帧时间最近的关键帧，然后依次进行编码，直到<code>pts</code>与目标时间相近</p>\n<p><strong>视频按2s一帧进行抽取</strong>：简单的操作可以去获取视频fps，比如视频25fps，可以使用一个计数器判断<code>if(frame_count%25==0)</code>,这时候则是刚好1s。当然这样子性能不太好。如果需要追求性能，那么也可以利用<code>av_seek_frame</code>，查找目标时间附近，然后循环进行解码直到目标时间。</p>\n<p><strong>视频不保存为jpeg文件转化为Java的Bitmap</strong>：只需要对最终获取的 <code>AVFrame</code>做不一样的操作进行了，获取到对应的buffer，再利用jni调用构造 Java 的 bitmap 对象。</p>\n<p>可以做的还有很多……</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>提取视频图片这个功能只是 FFmpeg 强大功能的九牛一毛，需要探究的还有很多很多……</p>\n<p>如果能跑起来 FFmpeg 最简单的例子，已经迈出了很大一步了，但如果要理解其中的原理，还需要更多的基础知识，以及像<code>AVPacket</code>、<code>AVFrame</code>、 <code>AVCodec</code> ……每一个类的数据结构，以及实现都需要仔细研究。</p>\n<p>自己在网上找到的 FFmpeg 相关的教程，以及自己想要去实现的功能的资源太少，很多东西都需要自己去摸索。有时候我总在怀疑：<strong>为什么这么基础且很实用的功能没有现成的轮子？</strong> 这可能也是现在音视频相关开发的现状吧，成熟可用的轮子相对而言较少，以及相关技术的分享可能不太好做。既然没有，那就靠自己一点点积累吧。</p>\n<p>学习之路，任重而道远呐。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"FFmpeg","slug":"FFmpeg","api":"api/tags/FFmpeg.json"},{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2021/11/14/音视频-初识FFmpeg.json"},{"title":"基于AndroidVideoCache的预加载","slug":"基于AndroidVideoCache的预加载","date":"2020-07-06T11:09:00.000Z","updated":"2025-09-15T13:07:12.918Z","comments":true,"url":"2020/07/06/基于AndroidVideoCache的预加载/","excerpt":"<p>最近有做需求关于视频缓存，了解到相关的开源库<a href=\"https://www.jianshu.com/p/dfc18278b053\">AndroidVideoCache</a>，\b一款市面上相对比较流行的视频缓存框架，而我想利用该框架进行视频缓存的处理，并且希望能够支持预加载。然而该框架作者在18年就已经停止了维护，所以留下了无限的编程空间给其他程序员，对于视频预加载，只搜到一篇<a href=\"https://www.jianshu.com/p/dfc18278b053\">《AndroidVideoCache源码详解以及改造系列-源码篇》</a>，然而点进该作者的博客列表，说好的预加载呢？？？后面也没有了下文，搜遍全网好像没有做AndroidVideoCache的预加载相关的事情，那么这样子的话……自己干吧。</p>\n<p>首先需要明白AndroidVideoCache的实现原理，推荐查看<a href=\"https://www.jianshu.com/p/4745de02dcdc\">《AndroidVideoCache-视频边播放边缓存的代理策略》</a>这里不再赘述。</p>\n<p>其实预加载的思路很简单，在进行一个播放视频后，再返回接下来需要预加载的视频url，启用后台线程去请求下载数据，不过中间涉及的细节逻辑比较多。</p>\n<h2 id=\"一、实现方案\"><a href=\"#一、实现方案\" class=\"headerlink\" title=\"一、实现方案\"></a>一、实现方案</h2><p>主要逻辑为：</p>\n<p>1、后台开启一个线程去请求并预加载一部分的数据</p>\n<p>2、可能需要预加载的数据大于&gt;1，利用队列先进入的先进行加载，加上前面的条件 使用HandlerThread再适合不过了。</p>\n<p>我们首先定义好需要去处理的任务情况：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">preload</span><span class=\"params\">( String method，Call call)</span> &#123;</span><br><span class=\"line\">       <span class=\"keyword\">switch</span> (method) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">case</span> <span class=\"string\">&quot;addPreloadURL&quot;</span>:</span><br><span class=\"line\">               addPreloadURL(call); <span class=\"comment\">//添加url到预加载队列</span></span><br><span class=\"line\">               <span class=\"keyword\">break</span>;</span><br><span class=\"line\">           <span class=\"keyword\">case</span> <span class=\"string\">&quot;cancelPreloadURLIfNeeded&quot;</span>:</span><br><span class=\"line\">               cancelPreloadURLIfNeeded(call); <span class=\"comment\">//取消对应的url预加载（因为可能是立马需要播放这个视频，那么就不需要预加载了）</span></span><br><span class=\"line\">               <span class=\"keyword\">break</span>;</span><br><span class=\"line\">           <span class=\"keyword\">case</span> <span class=\"string\">&quot;cancelAnyPreloads&quot;</span>: </span><br><span class=\"line\">               cancelAnyPreLoads();<span class=\"comment\">//取消所有的预加载，主要是方便管理任务</span></span><br><span class=\"line\">               <span class=\"keyword\">break</span>;</span><br><span class=\"line\">           <span class=\"keyword\">default</span>:</span><br><span class=\"line\">           </span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>那么对于每次的预加载逻辑基本上是这样的方法执行顺序：</p>\n<p> cancelPreloadURLIfNeeded()-&gt;addPreloadURL();   //取消对应url加载的任务，因为有可能该url不需要再进行预加载了（参考抖音，当用户瞬间下滑几个视频，那么很多视频就需要跳过了不需要再进行预加载）</p>\n<p> cancelAnyPreLoads()-&gt;addPreloadURL();   //取消对应url加载的任务（这时候需要立马播放最新的视频，那么就应该让出网速给该视频），之后再添加新一轮的预加载url。</p>\n<p>接下来具体的处理逻辑VideoPreLoader类，我直接放上所有的代码逻辑吧,为方便观察删除了一部分不太重要的逻辑，其实总体流程也比较简单。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">VideoPreLoader</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> Handler handler;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> HandlerThread handlerThread;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> List&lt;String&gt; cancelList = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"title function_\">VideoPreLoader</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    handlerThread = <span class=\"keyword\">new</span> <span class=\"title class_\">HandlerThread</span>(<span class=\"string\">&quot;VideoPreLoaderThread&quot;</span>);</span><br><span class=\"line\">    handlerThread.start();</span><br><span class=\"line\">    handler = <span class=\"keyword\">new</span> <span class=\"title class_\">Handler</span>(handlerThread.getLooper()) &#123;</span><br><span class=\"line\">      <span class=\"meta\">@Override</span></span><br><span class=\"line\">      <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">handleMessage</span><span class=\"params\">(Message msg)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.handleMessage(msg);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">void</span> <span class=\"title function_\">addPreloadURL</span><span class=\"params\">(<span class=\"keyword\">final</span> VideoPreLoadModel data)</span> &#123;</span><br><span class=\"line\">    handler.post(<span class=\"keyword\">new</span> <span class=\"title class_\">Runnable</span>() &#123;</span><br><span class=\"line\">      <span class=\"meta\">@Override</span></span><br><span class=\"line\">      <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        realPreload(data);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">void</span> <span class=\"title function_\">cancelPreloadURLIfNeeded</span><span class=\"params\">(String url)</span> &#123;</span><br><span class=\"line\">    cancelList.add(url);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">void</span> <span class=\"title function_\">cancelAnyPreLoads</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    handler.removeCallbacksAndMessages(<span class=\"literal\">null</span>);</span><br><span class=\"line\">    cancelList.clear();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">realPreload</span><span class=\"params\">(VideoPreLoadModel data)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (data == <span class=\"literal\">null</span> || isCancel(data.originalUrl)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"type\">HttpURLConnection</span> <span class=\"variable\">conn</span> <span class=\"operator\">=</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"type\">URL</span> <span class=\"variable\">myURL</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">URL</span>(data.proxyUrl);</span><br><span class=\"line\">      conn = (HttpURLConnection) myURL.openConnection();</span><br><span class=\"line\">      conn.connect();</span><br><span class=\"line\">      <span class=\"type\">InputStream</span> <span class=\"variable\">is</span> <span class=\"operator\">=</span> conn.getInputStream();</span><br><span class=\"line\">      <span class=\"type\">byte</span>[] buf = <span class=\"keyword\">new</span> <span class=\"title class_\">byte</span>[<span class=\"number\">1024</span>];</span><br><span class=\"line\">      <span class=\"type\">int</span> <span class=\"variable\">downLoadedSize</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">      <span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">numRead</span> <span class=\"operator\">=</span> is.read(buf);</span><br><span class=\"line\">        downLoadedSize += numRead;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (downLoadedSize &gt;= data.preLoadBytes || numRead == -<span class=\"number\">1</span>) &#123; <span class=\"comment\">//Reached  preload range or end of Input stream.</span></span><br><span class=\"line\">          <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>);</span><br><span class=\"line\">      is.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"type\">boolean</span> <span class=\"title function_\">isCancel</span><span class=\"params\">(String url)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (TextUtils.isEmpty(url)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (String cancelUrl : cancelList) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (cancelUrl.equals(url)) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对于这段代码中其实有“两个”队列，一个是HandlerThread中的队列，熟悉消息机制的同学应该都能明白，内部是一个looper在不断地循环获取消息，当一个消息处理完毕之后才会处理下一个消息。我还定义了一个就是取消队列，因为HandlerThread中的任务我们不太好控制取消具体的任务，所以设置了一个取消队列，当之后的消息再需要执行的时候会首先判断是否是在取消队列里面，这样子就能做到对预加载队列逻辑的控制。</p>\n<h2 id=\"二、关于一些细节问题\"><a href=\"#二、关于一些细节问题\" class=\"headerlink\" title=\"二、关于一些细节问题\"></a>二、关于一些细节问题</h2><p>这样子我们在播放一个视频的时候，只需要传给我们接下来将会播放的视频的URL，我们就能对其预加载并缓存下来，但是会存在其他条件：</p>\n<h5 id=\"预加载的长度？\"><a href=\"#预加载的长度？\" class=\"headerlink\" title=\"预加载的长度？\"></a>预加载的长度？</h5><p>对于视频加载长度，我们很容易想到在视频url请求加入Range在header上面，比如</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">conn.addRequestProperty(<span class=\"string\">&quot;Range&quot;</span>, <span class=\"string\">&quot;0-102400&quot;</span>);</span><br></pre></td></tr></table></figure>\n<p>我们只获取前102400 bytes，不用将整个视频全部进行预加载，我有进行这样的尝试，但是实际发现是有坑的。我做了很多尝试，发现不论怎么请求，拿到的 responseCode 虽然是206，但是 还是把数据给全部下载完了，这就有点不科学了！！</p>\n<p>最终去源码中才发现：源码有对range做正则匹配</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">Pattern</span> <span class=\"variable\">RANGE_HEADER_PATTERN</span> <span class=\"operator\">=</span> Pattern.compile(<span class=\"string\">&quot;[R,r]ange:[ ]?bytes=(\\\\d*)-&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"type\">long</span> <span class=\"title function_\">findRangeOffset</span><span class=\"params\">(String request)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Matcher</span> <span class=\"variable\">matcher</span> <span class=\"operator\">=</span> RANGE_HEADER_PATTERN.matcher(request);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (matcher.find()) &#123;</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">rangeValue</span> <span class=\"operator\">=</span> matcher.group(<span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> Long.parseLong(rangeValue);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>看清楚了 <strong><em>“[R,r]ange:[ ]?bytes=(\\\\d</em>)-“*</strong> 它只去匹配了前面的的，也就是说 我传入了 0-102400 它最终只当作是：Range：0- 来处理，导致addRequestProperty设置的range实现。坑！不过能理解作者为什么这么做，后面总结会讲到。没有办法只有使用最原始的方法进行判断了：在每次获取inputStream的时候进行判断是否达到预加载的大小，虽然有一定的性能开销，但是不去改源码的话也没有 办法了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">      <span class=\"type\">int</span> <span class=\"variable\">numRead</span> <span class=\"operator\">=</span> is.read(buf);</span><br><span class=\"line\">      downLoadedSize += numRead;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (downLoadedSize &gt;= data.preLoadBytes || numRead == -<span class=\"number\">1</span>) &#123; <span class=\"comment\">//Reached  preload range or end of Input stream.</span></span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>);</span><br><span class=\"line\">    is.close();</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h2><p>本文主要讲了基于AndroidVideoCache的预加载具体实现原理，以及其中遇到的坑</p>\n<p>1、预加载主要通过HandlerThread去实现后台网络的访问以及缓存的处理逻辑</p>\n<p>2、加入取消队列去控制对应需要取消的任务</p>\n<p>3、对于预加载的size只能通过读取的时候进行判断，没有办法使用range去判断。其实很容易理解作者为什么正则要这样写，因为它只是一个视频缓存框架，主要是用来做“边播边存”，所以每次去进行请求的时候应该都是在原有的缓存之上去进行缓存数据处理，而缓存最终需要处理完的就是 content-size，不需要再去管Range中的结束范围了。</p>\n","cover":null,"images":[],"content":"<p>最近有做需求关于视频缓存，了解到相关的开源库<a href=\"https://www.jianshu.com/p/dfc18278b053\">AndroidVideoCache</a>，\b一款市面上相对比较流行的视频缓存框架，而我想利用该框架进行视频缓存的处理，并且希望能够支持预加载。然而该框架作者在18年就已经停止了维护，所以留下了无限的编程空间给其他程序员，对于视频预加载，只搜到一篇<a href=\"https://www.jianshu.com/p/dfc18278b053\">《AndroidVideoCache源码详解以及改造系列-源码篇》</a>，然而点进该作者的博客列表，说好的预加载呢？？？后面也没有了下文，搜遍全网好像没有做AndroidVideoCache的预加载相关的事情，那么这样子的话……自己干吧。</p>\n<p>首先需要明白AndroidVideoCache的实现原理，推荐查看<a href=\"https://www.jianshu.com/p/4745de02dcdc\">《AndroidVideoCache-视频边播放边缓存的代理策略》</a>这里不再赘述。</p>\n<p>其实预加载的思路很简单，在进行一个播放视频后，再返回接下来需要预加载的视频url，启用后台线程去请求下载数据，不过中间涉及的细节逻辑比较多。</p>\n<h2 id=\"一、实现方案\"><a href=\"#一、实现方案\" class=\"headerlink\" title=\"一、实现方案\"></a>一、实现方案</h2><p>主要逻辑为：</p>\n<p>1、后台开启一个线程去请求并预加载一部分的数据</p>\n<p>2、可能需要预加载的数据大于&gt;1，利用队列先进入的先进行加载，加上前面的条件 使用HandlerThread再适合不过了。</p>\n<p>我们首先定义好需要去处理的任务情况：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">preload</span><span class=\"params\">( String method，Call call)</span> &#123;</span><br><span class=\"line\">       <span class=\"keyword\">switch</span> (method) &#123;</span><br><span class=\"line\">           <span class=\"keyword\">case</span> <span class=\"string\">&quot;addPreloadURL&quot;</span>:</span><br><span class=\"line\">               addPreloadURL(call); <span class=\"comment\">//添加url到预加载队列</span></span><br><span class=\"line\">               <span class=\"keyword\">break</span>;</span><br><span class=\"line\">           <span class=\"keyword\">case</span> <span class=\"string\">&quot;cancelPreloadURLIfNeeded&quot;</span>:</span><br><span class=\"line\">               cancelPreloadURLIfNeeded(call); <span class=\"comment\">//取消对应的url预加载（因为可能是立马需要播放这个视频，那么就不需要预加载了）</span></span><br><span class=\"line\">               <span class=\"keyword\">break</span>;</span><br><span class=\"line\">           <span class=\"keyword\">case</span> <span class=\"string\">&quot;cancelAnyPreloads&quot;</span>: </span><br><span class=\"line\">               cancelAnyPreLoads();<span class=\"comment\">//取消所有的预加载，主要是方便管理任务</span></span><br><span class=\"line\">               <span class=\"keyword\">break</span>;</span><br><span class=\"line\">           <span class=\"keyword\">default</span>:</span><br><span class=\"line\">           </span><br><span class=\"line\">       &#125;</span><br><span class=\"line\">   &#125;</span><br></pre></td></tr></table></figure>\n<p>那么对于每次的预加载逻辑基本上是这样的方法执行顺序：</p>\n<p> cancelPreloadURLIfNeeded()-&gt;addPreloadURL();   //取消对应url加载的任务，因为有可能该url不需要再进行预加载了（参考抖音，当用户瞬间下滑几个视频，那么很多视频就需要跳过了不需要再进行预加载）</p>\n<p> cancelAnyPreLoads()-&gt;addPreloadURL();   //取消对应url加载的任务（这时候需要立马播放最新的视频，那么就应该让出网速给该视频），之后再添加新一轮的预加载url。</p>\n<p>接下来具体的处理逻辑VideoPreLoader类，我直接放上所有的代码逻辑吧,为方便观察删除了一部分不太重要的逻辑，其实总体流程也比较简单。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">VideoPreLoader</span> &#123;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> Handler handler;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> HandlerThread handlerThread;</span><br><span class=\"line\">  <span class=\"keyword\">private</span> List&lt;String&gt; cancelList = <span class=\"keyword\">new</span> <span class=\"title class_\">ArrayList</span>&lt;&gt;();</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"title function_\">VideoPreLoader</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    handlerThread = <span class=\"keyword\">new</span> <span class=\"title class_\">HandlerThread</span>(<span class=\"string\">&quot;VideoPreLoaderThread&quot;</span>);</span><br><span class=\"line\">    handlerThread.start();</span><br><span class=\"line\">    handler = <span class=\"keyword\">new</span> <span class=\"title class_\">Handler</span>(handlerThread.getLooper()) &#123;</span><br><span class=\"line\">      <span class=\"meta\">@Override</span></span><br><span class=\"line\">      <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">handleMessage</span><span class=\"params\">(Message msg)</span> &#123;</span><br><span class=\"line\">        <span class=\"built_in\">super</span>.handleMessage(msg);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">void</span> <span class=\"title function_\">addPreloadURL</span><span class=\"params\">(<span class=\"keyword\">final</span> VideoPreLoadModel data)</span> &#123;</span><br><span class=\"line\">    handler.post(<span class=\"keyword\">new</span> <span class=\"title class_\">Runnable</span>() &#123;</span><br><span class=\"line\">      <span class=\"meta\">@Override</span></span><br><span class=\"line\">      <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">run</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        realPreload(data);</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">void</span> <span class=\"title function_\">cancelPreloadURLIfNeeded</span><span class=\"params\">(String url)</span> &#123;</span><br><span class=\"line\">    cancelList.add(url);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">void</span> <span class=\"title function_\">cancelAnyPreLoads</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    handler.removeCallbacksAndMessages(<span class=\"literal\">null</span>);</span><br><span class=\"line\">    cancelList.clear();</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"keyword\">void</span> <span class=\"title function_\">realPreload</span><span class=\"params\">(VideoPreLoadModel data)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (data == <span class=\"literal\">null</span> || isCancel(data.originalUrl)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"type\">HttpURLConnection</span> <span class=\"variable\">conn</span> <span class=\"operator\">=</span> <span class=\"literal\">null</span>;</span><br><span class=\"line\">    <span class=\"keyword\">try</span> &#123;</span><br><span class=\"line\">      <span class=\"type\">URL</span> <span class=\"variable\">myURL</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">URL</span>(data.proxyUrl);</span><br><span class=\"line\">      conn = (HttpURLConnection) myURL.openConnection();</span><br><span class=\"line\">      conn.connect();</span><br><span class=\"line\">      <span class=\"type\">InputStream</span> <span class=\"variable\">is</span> <span class=\"operator\">=</span> conn.getInputStream();</span><br><span class=\"line\">      <span class=\"type\">byte</span>[] buf = <span class=\"keyword\">new</span> <span class=\"title class_\">byte</span>[<span class=\"number\">1024</span>];</span><br><span class=\"line\">      <span class=\"type\">int</span> <span class=\"variable\">downLoadedSize</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">      <span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">numRead</span> <span class=\"operator\">=</span> is.read(buf);</span><br><span class=\"line\">        downLoadedSize += numRead;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (downLoadedSize &gt;= data.preLoadBytes || numRead == -<span class=\"number\">1</span>) &#123; <span class=\"comment\">//Reached  preload range or end of Input stream.</span></span><br><span class=\"line\">          <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125; <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>);</span><br><span class=\"line\">      is.close();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    ....</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  <span class=\"keyword\">private</span> <span class=\"type\">boolean</span> <span class=\"title function_\">isCancel</span><span class=\"params\">(String url)</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (TextUtils.isEmpty(url)) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (String cancelUrl : cancelList) &#123;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (cancelUrl.equals(url)) &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>对于这段代码中其实有“两个”队列，一个是HandlerThread中的队列，熟悉消息机制的同学应该都能明白，内部是一个looper在不断地循环获取消息，当一个消息处理完毕之后才会处理下一个消息。我还定义了一个就是取消队列，因为HandlerThread中的任务我们不太好控制取消具体的任务，所以设置了一个取消队列，当之后的消息再需要执行的时候会首先判断是否是在取消队列里面，这样子就能做到对预加载队列逻辑的控制。</p>\n<h2 id=\"二、关于一些细节问题\"><a href=\"#二、关于一些细节问题\" class=\"headerlink\" title=\"二、关于一些细节问题\"></a>二、关于一些细节问题</h2><p>这样子我们在播放一个视频的时候，只需要传给我们接下来将会播放的视频的URL，我们就能对其预加载并缓存下来，但是会存在其他条件：</p>\n<h5 id=\"预加载的长度？\"><a href=\"#预加载的长度？\" class=\"headerlink\" title=\"预加载的长度？\"></a>预加载的长度？</h5><p>对于视频加载长度，我们很容易想到在视频url请求加入Range在header上面，比如</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">conn.addRequestProperty(<span class=\"string\">&quot;Range&quot;</span>, <span class=\"string\">&quot;0-102400&quot;</span>);</span><br></pre></td></tr></table></figure>\n<p>我们只获取前102400 bytes，不用将整个视频全部进行预加载，我有进行这样的尝试，但是实际发现是有坑的。我做了很多尝试，发现不论怎么请求，拿到的 responseCode 虽然是206，但是 还是把数据给全部下载完了，这就有点不科学了！！</p>\n<p>最终去源码中才发现：源码有对range做正则匹配</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"keyword\">static</span> <span class=\"keyword\">final</span> <span class=\"type\">Pattern</span> <span class=\"variable\">RANGE_HEADER_PATTERN</span> <span class=\"operator\">=</span> Pattern.compile(<span class=\"string\">&quot;[R,r]ange:[ ]?bytes=(\\\\d*)-&quot;</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">private</span> <span class=\"type\">long</span> <span class=\"title function_\">findRangeOffset</span><span class=\"params\">(String request)</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">Matcher</span> <span class=\"variable\">matcher</span> <span class=\"operator\">=</span> RANGE_HEADER_PATTERN.matcher(request);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (matcher.find()) &#123;</span><br><span class=\"line\">            <span class=\"type\">String</span> <span class=\"variable\">rangeValue</span> <span class=\"operator\">=</span> matcher.group(<span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">return</span> Long.parseLong(rangeValue);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> -<span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<p>看清楚了 <strong><em>“[R,r]ange:[ ]?bytes=(\\\\d</em>)-“*</strong> 它只去匹配了前面的的，也就是说 我传入了 0-102400 它最终只当作是：Range：0- 来处理，导致addRequestProperty设置的range实现。坑！不过能理解作者为什么这么做，后面总结会讲到。没有办法只有使用最原始的方法进行判断了：在每次获取inputStream的时候进行判断是否达到预加载的大小，虽然有一定的性能开销，但是不去改源码的话也没有 办法了。</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">do</span> &#123;</span><br><span class=\"line\">      <span class=\"type\">int</span> <span class=\"variable\">numRead</span> <span class=\"operator\">=</span> is.read(buf);</span><br><span class=\"line\">      downLoadedSize += numRead;</span><br><span class=\"line\">      <span class=\"keyword\">if</span> (downLoadedSize &gt;= data.preLoadBytes || numRead == -<span class=\"number\">1</span>) &#123; <span class=\"comment\">//Reached  preload range or end of Input stream.</span></span><br><span class=\"line\">        <span class=\"keyword\">break</span>;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; <span class=\"keyword\">while</span> (<span class=\"literal\">true</span>);</span><br><span class=\"line\">    is.close();</span><br></pre></td></tr></table></figure>\n<h2 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h2><p>本文主要讲了基于AndroidVideoCache的预加载具体实现原理，以及其中遇到的坑</p>\n<p>1、预加载主要通过HandlerThread去实现后台网络的访问以及缓存的处理逻辑</p>\n<p>2、加入取消队列去控制对应需要取消的任务</p>\n<p>3、对于预加载的size只能通过读取的时候进行判断，没有办法使用range去判断。其实很容易理解作者为什么正则要这样写，因为它只是一个视频缓存框架，主要是用来做“边播边存”，所以每次去进行请求的时候应该都是在原有的缓存之上去进行缓存数据处理，而缓存最终需要处理完的就是 content-size，不需要再去管Range中的结束范围了。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2020/07/06/基于AndroidVideoCache的预加载.json"}],"info":{"type":"tag","name":"音视频","slug":"音视频"}},"api":"api/tags/音视频/page.1.json"}