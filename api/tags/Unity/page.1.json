{"data":{"index":1,"total":1,"posts":[{"title":"Unity 实现利用 Andorid 能力进行视频渲染播放","slug":"Unity-实现利用-Andorid-能力进行视频渲染播放","date":"2022-10-25T02:00:00.000Z","updated":"2025-09-15T13:10:48.829Z","comments":true,"url":"2022/10/25/Unity-实现利用-Andorid-能力进行视频渲染播放/","excerpt":"<p>在 Unity 中使用 Android 侧提供的视频渲染相关的能力，有两种方案可选：</p>\n<p>第一种是将渲染播放页单独做一个页面，在 Unity事件交互的时候打开对应 Activity 页面，或者获取到 Unity 创建的 Acitivity 动态添加 View。</p>\n<p>第二种是只借助 Android 的渲染能力，将数据渲染到 Unity 的控件上。</p>\n<p>两种方案各有优劣，第一种大大地减少了播放器相关的开发工作量，整个页面逻辑可以实现复用，但是交互页面的话 iOS&#x2F;Android 需要写两套。第二种实现成本相对较高，但是交互可以由 Unity 侧进行，只是播放器使用封装好的 plugin 进行，能达到交互相对较统一，本文也主要是讲述该方案的实现。</p>\n<h2 id=\"Android-平台基本播放逻辑\"><a href=\"#Android-平台基本播放逻辑\" class=\"headerlink\" title=\"Android 平台基本播放逻辑\"></a>Android 平台基本播放逻辑</h2><p>在正式开发改造之前，对 Android 侧的一个播放器渲染流程进行简单的介绍，以 MediaPlayer 为例，利用 MediaPlayer 进行视频解码渲染，并将视频最后输出到 SurfaceView 上,一次播放器视频渲染到View上的的主要代码流程为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initPlayer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MediaPlayer</span> <span class=\"variable\">mediaPlayer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MediaPlayer</span>();</span><br><span class=\"line\">    <span class=\"type\">SurfaceView</span> <span class=\"variable\">surfaceView</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceView</span>(activity);</span><br><span class=\"line\">    surfaceHolder = surfaceView.getHolder();</span><br><span class=\"line\">    surfaceHolder.addCallback(^ &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">surfaceCreated</span><span class=\"params\">(SurfaceHolder holder)</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">Surface</span> <span class=\"variable\">surface</span> <span class=\"operator\">=</span> holder.getSurface();</span><br><span class=\"line\">            mediaPlayer.setSurface(surface);</span><br><span class=\"line\">            mediaPlayer.prepareAsync();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         ……</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    mediaPlayer.setDataSource(URI...);</span><br><span class=\"line\">    mediaPlayer.setOnPreparedListener(mp -&gt; mp.start());   </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>对于渲染 <code>mediaPlayer.setSurface(surface)</code> 设为播放器解码数据的接受器，Surface 来自于 SurfaceView。</p>\n<p>播放器是将数据图形绘制在 Surface 对象上，Surface中会关联一个 BufferQueue 用于提供图像数据缓存，SurfaceFlinger 会把 Surface 对应的图像层混合在一起，将其输出到 FrameBuffer 中（Framebuffer就是一块内存区域，它通常是显示驱动的内部缓冲区在内存中的映射），最后在屏幕上看到合成的图像。</p>\n<p>整个流程引入外部大佬的一张图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg\">\n<h2 id=\"Unity-中的一些改造\"><a href=\"#Unity-中的一些改造\" class=\"headerlink\" title=\"Unity 中的一些改造\"></a>Unity 中的一些改造</h2><p>上面的流程最终是通过播放器解码渲染到 SurfaceView 上，当然，你可以通过获取到 UnityPlayer 对应的 Acitivity 将这个 SurfaceView 动态添加到当前界面，实现“在 Unity 中利用 Android 能力进行视频渲染”。</p>\n<p>所以需要对其进行改造，我们的目的是实现 Android 播放器数据渲染到 Untiy 的组件中。实现这一过程需要借助 FBO(Frame Buffer Object) 的能力。</p>\n<h3 id=\"（一）FBO\"><a href=\"#（一）FBO\" class=\"headerlink\" title=\"（一）FBO\"></a>（一）FBO</h3><p>在 OpenGL 渲染管线中几何数据和纹理经过变换和一些测试处理，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存(framebuffer)，OpenGL渲染管线的最终位置是在帧缓冲区中，默认情况下 OpenGL 使用的是窗口系统提供的帧缓冲区。</p>\n<p>但有些场景是不想要直接渲染到窗口上的(例如加视频特效)，于是 OpenGL 提供了一种方式来创建额外的帧缓冲区对象(FBO)。使用帧缓冲区对象，OpenGL 可以将原先绘制到窗口提供的帧缓冲区重定向到 FBO 之中。FBO本身不是一块内存，没有空间，真正存储东西，可实际读写的是依附于FBO的东西：纹理(texture)和渲染缓存(renderbuffer)，依附的方式，是一个二维数组来管理，结构如图所示：</p>\n<p><img src=\"https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&width=380\"></p>\n<h3 id=\"（二）具体实现\"><a href=\"#（二）具体实现\" class=\"headerlink\" title=\"（二）具体实现\"></a>（二）具体实现</h3><p>使用 FBO 我们可以将渲染目标渲染到其他的空间，我们目的是将播放器解码后的数据渲染到 Unity 控件的纹理空间中。<br>渲染播放器将输出到 FBO 中，FBO 指向 Unity 控件数据的输入，从而实现：Android 的播放器输出数据显示到 Unity 的控件中。</p>\n<h3 id=\"（三）从渲染输出数据到外部纹理\"><a href=\"#（三）从渲染输出数据到外部纹理\" class=\"headerlink\" title=\"（三）从渲染输出数据到外部纹理\"></a>（三）从渲染输出数据到外部纹理</h3><p>由于 <code>mediaPlayer.setSurface(surface)</code> 对应的 Surface 来源于 SurafaceView，会直接渲染到屏幕上，这里我们需要使用 构造一个新的 SurfaceTexture 以将图像流式传输到给定的 OpenGL 纹理;</p>\n<p>要获取到播放器渲染得数据，需要借助 SurfaceTexture ，SurfaceTexture 是Surface 和 OpenGL ES 纹理的结合，其对图像流的处理并不直接显示，而是从图像流中捕获帧作为 OpenGL 的外部纹理，图像流来自相机预览和视频解码。</p>\n<p>SurfaceTexture 创建的 Surface 是数据的生产者，而 SurfaceTexture 是对应的消费者，Surface 接收媒体数据并将数据发送到 SurfaceTexture，当调用 updateTexImage 的时候，创建SurfaceTexture 的纹理对象相应的内容将更新为最新图像帧，也就是会将图像帧转换为 GL 纹理，并将该纹理绑定到 GL_TEXTURE_EXTERNAL_OES 纹理对象上。具体实现逻辑参考：<a href=\"https://juejin.cn/post/7012517274768179236\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">SurfaceTexture</span> <span class=\"variable\">surfaceTexture</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceTexture</span>(videoTextureId);</span><br><span class=\"line\">player.setUpSurface(<span class=\"keyword\">new</span> <span class=\"title class_\">Surface</span>(surfaceTexture), width, height);</span><br><span class=\"line\">surfaceTexture.setDefaultBufferSize(width, height);</span><br><span class=\"line\">surfaceTexture.setOnFrameAvailableListener(surfaceTexture -&gt; &#123;……&#125;);;</span><br></pre></td></tr></table></figure>\n\n\n<p>其中 videoTextureId 来源于创建的 OES 纹理：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createOESTextureID</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span>[] texture = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"comment\">// 创建纹理对象，一个容器对象，保存渲染所需要的纹理数据，例如：图像数据</span></span><br><span class=\"line\">        <span class=\"comment\">//在OpenGL 中纹理对象是一个无符号整数，是一个纹理对象的句柄</span></span><br><span class=\"line\">        GLES30.glGenTextures(texture.length, texture, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 绑定纹理ID到纹理单元的纹理目标上</span></span><br><span class=\"line\">        GLES30.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, texture[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 设置纹理参数</span></span><br><span class=\"line\">        ……</span><br><span class=\"line\"></span><br><span class=\"line\">        GLES30.glGenerateMipmap(GLES11Ext.GL_TEXTURE_EXTERNAL_OES);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> texture[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"（四）FBO纹理数据到-Unity-的纹理数据\"><a href=\"#（四）FBO纹理数据到-Unity-的纹理数据\" class=\"headerlink\" title=\"（四）FBO纹理数据到 Unity 的纹理数据\"></a>（四）FBO纹理数据到 Unity 的纹理数据</h3><p>学习了解到Unity中可以使用 RawImage 或者 quad 等相关控件可以显示纹理，这里以 RawImage 为例。在 Unity 脚本编写初始化的逻辑，构造一个 Texture2D 对象，将句柄传递到 Android，并赋值给 RawImage，并将texture id 传递到 Android 平台，完成一次渲染的重定向。</p>\n<figure class=\"highlight c#\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">InitPlayer</span>()</span></span><br><span class=\"line\"> &#123;    </span><br><span class=\"line\">    Texture2D texture2D = <span class=\"keyword\">new</span> Texture2D(width, height, TextureFormat.RGB24, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>);</span><br><span class=\"line\">    androidObj.Call(<span class=\"string\">&quot;init&quot;</span>, (<span class=\"built_in\">int</span>)texture2D.GetNativeTexturePtr(), width, height);</span><br><span class=\"line\">    RawImage.texture = texture2D;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>创建FBO</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createFBO</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>[] fbo = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">    GLES30.glGenFramebuffers(fbo.length, fbo, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fbo[<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>为<code>SurfaceTexture</code> 设置了 <code>OnFrameAvailableListener</code> 后，当有新的图形流数据生成之后，就可以通过  <code>mSurfaceTexture.updateTexImage()</code> 将当前图片流更新到纹理所关联的OpenGLES中纹理，并绘制 FBO.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">publc <span class=\"keyword\">void</span> <span class=\"title function_\">draw</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 绑定 FrameBuffer 到当前的绘制环境上， 后续 GL 绘制都会到这个 framebuffer</span></span><br><span class=\"line\">    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fbo[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//2.把一个2D纹理作为帧缓冲区附着</span></span><br><span class=\"line\">    <span class=\"comment\">//即所有渲染操作的结果将会被储存在 unityTextureId 对应的纹理图像中</span></span><br><span class=\"line\">    GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, unityTextureId, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//绑定指定纹理到当前激活的纹理单元</span></span><br><span class=\"line\">    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, videoTextureId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//…… 省略 Opengl 绘制的常规流程</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这一步是最关键的，实现了将 FBO 的输出指向 Unity 里面创建的纹理，也就实现了 Android 渲染与 Unity 之间的数据打通。</p>\n<p>这里的 unityTextureId 来源于在 Unity 中初始化的 <code>(int)texture2D.GetNativeTexturePtr()</code>值。</p>\n<p>整体的流程为：</p>\n<img src=\"https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png\">\n<p>效果图：</p>\n<img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif\">\n\n<p>图中播放视频区域为 Unity 的 RawImage 控件，渲染的视频通过 Pag 等相关素材由渲染SDK合成。</p>\n<p>如图所示，视频画面正常地进行渲染，图中有两个区域展示了视频画面，上面的使用的 Quad 组件，下面是用的 RawImage，流程都一直，只是在 Unity 使用 Texture2D 的时候通过 <code>Quad.mainTexture = texture2D</code> 赋值。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲了 Unity 利用 Android 提供的能力进行视频相关的特效渲染的方案，总体正常运行。还需要一些优化，例如对 <code>Multithreaded Rendering</code>配置还未支持，以及一些逻辑可能受限于游戏侧的配置，例如图形渲染的配置使用的 OpenGL3.0，如果使用 OpenGL2.0 或者 Vulkan，还需要单独调整相关逻辑。</p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg","https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&width=380","https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png","https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif"],"content":"<p>在 Unity 中使用 Android 侧提供的视频渲染相关的能力，有两种方案可选：</p>\n<p>第一种是将渲染播放页单独做一个页面，在 Unity事件交互的时候打开对应 Activity 页面，或者获取到 Unity 创建的 Acitivity 动态添加 View。</p>\n<p>第二种是只借助 Android 的渲染能力，将数据渲染到 Unity 的控件上。</p>\n<p>两种方案各有优劣，第一种大大地减少了播放器相关的开发工作量，整个页面逻辑可以实现复用，但是交互页面的话 iOS&#x2F;Android 需要写两套。第二种实现成本相对较高，但是交互可以由 Unity 侧进行，只是播放器使用封装好的 plugin 进行，能达到交互相对较统一，本文也主要是讲述该方案的实现。</p>\n<h2 id=\"Android-平台基本播放逻辑\"><a href=\"#Android-平台基本播放逻辑\" class=\"headerlink\" title=\"Android 平台基本播放逻辑\"></a>Android 平台基本播放逻辑</h2><p>在正式开发改造之前，对 Android 侧的一个播放器渲染流程进行简单的介绍，以 MediaPlayer 为例，利用 MediaPlayer 进行视频解码渲染，并将视频最后输出到 SurfaceView 上,一次播放器视频渲染到View上的的主要代码流程为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initPlayer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MediaPlayer</span> <span class=\"variable\">mediaPlayer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MediaPlayer</span>();</span><br><span class=\"line\">    <span class=\"type\">SurfaceView</span> <span class=\"variable\">surfaceView</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceView</span>(activity);</span><br><span class=\"line\">    surfaceHolder = surfaceView.getHolder();</span><br><span class=\"line\">    surfaceHolder.addCallback(^ &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">surfaceCreated</span><span class=\"params\">(SurfaceHolder holder)</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">Surface</span> <span class=\"variable\">surface</span> <span class=\"operator\">=</span> holder.getSurface();</span><br><span class=\"line\">            mediaPlayer.setSurface(surface);</span><br><span class=\"line\">            mediaPlayer.prepareAsync();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         ……</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    mediaPlayer.setDataSource(URI...);</span><br><span class=\"line\">    mediaPlayer.setOnPreparedListener(mp -&gt; mp.start());   </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>对于渲染 <code>mediaPlayer.setSurface(surface)</code> 设为播放器解码数据的接受器，Surface 来自于 SurfaceView。</p>\n<p>播放器是将数据图形绘制在 Surface 对象上，Surface中会关联一个 BufferQueue 用于提供图像数据缓存，SurfaceFlinger 会把 Surface 对应的图像层混合在一起，将其输出到 FrameBuffer 中（Framebuffer就是一块内存区域，它通常是显示驱动的内部缓冲区在内存中的映射），最后在屏幕上看到合成的图像。</p>\n<p>整个流程引入外部大佬的一张图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg\">\n<h2 id=\"Unity-中的一些改造\"><a href=\"#Unity-中的一些改造\" class=\"headerlink\" title=\"Unity 中的一些改造\"></a>Unity 中的一些改造</h2><p>上面的流程最终是通过播放器解码渲染到 SurfaceView 上，当然，你可以通过获取到 UnityPlayer 对应的 Acitivity 将这个 SurfaceView 动态添加到当前界面，实现“在 Unity 中利用 Android 能力进行视频渲染”。</p>\n<p>所以需要对其进行改造，我们的目的是实现 Android 播放器数据渲染到 Untiy 的组件中。实现这一过程需要借助 FBO(Frame Buffer Object) 的能力。</p>\n<h3 id=\"（一）FBO\"><a href=\"#（一）FBO\" class=\"headerlink\" title=\"（一）FBO\"></a>（一）FBO</h3><p>在 OpenGL 渲染管线中几何数据和纹理经过变换和一些测试处理，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存(framebuffer)，OpenGL渲染管线的最终位置是在帧缓冲区中，默认情况下 OpenGL 使用的是窗口系统提供的帧缓冲区。</p>\n<p>但有些场景是不想要直接渲染到窗口上的(例如加视频特效)，于是 OpenGL 提供了一种方式来创建额外的帧缓冲区对象(FBO)。使用帧缓冲区对象，OpenGL 可以将原先绘制到窗口提供的帧缓冲区重定向到 FBO 之中。FBO本身不是一块内存，没有空间，真正存储东西，可实际读写的是依附于FBO的东西：纹理(texture)和渲染缓存(renderbuffer)，依附的方式，是一个二维数组来管理，结构如图所示：</p>\n<p><img src=\"https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&width=380\"></p>\n<h3 id=\"（二）具体实现\"><a href=\"#（二）具体实现\" class=\"headerlink\" title=\"（二）具体实现\"></a>（二）具体实现</h3><p>使用 FBO 我们可以将渲染目标渲染到其他的空间，我们目的是将播放器解码后的数据渲染到 Unity 控件的纹理空间中。<br>渲染播放器将输出到 FBO 中，FBO 指向 Unity 控件数据的输入，从而实现：Android 的播放器输出数据显示到 Unity 的控件中。</p>\n<h3 id=\"（三）从渲染输出数据到外部纹理\"><a href=\"#（三）从渲染输出数据到外部纹理\" class=\"headerlink\" title=\"（三）从渲染输出数据到外部纹理\"></a>（三）从渲染输出数据到外部纹理</h3><p>由于 <code>mediaPlayer.setSurface(surface)</code> 对应的 Surface 来源于 SurafaceView，会直接渲染到屏幕上，这里我们需要使用 构造一个新的 SurfaceTexture 以将图像流式传输到给定的 OpenGL 纹理;</p>\n<p>要获取到播放器渲染得数据，需要借助 SurfaceTexture ，SurfaceTexture 是Surface 和 OpenGL ES 纹理的结合，其对图像流的处理并不直接显示，而是从图像流中捕获帧作为 OpenGL 的外部纹理，图像流来自相机预览和视频解码。</p>\n<p>SurfaceTexture 创建的 Surface 是数据的生产者，而 SurfaceTexture 是对应的消费者，Surface 接收媒体数据并将数据发送到 SurfaceTexture，当调用 updateTexImage 的时候，创建SurfaceTexture 的纹理对象相应的内容将更新为最新图像帧，也就是会将图像帧转换为 GL 纹理，并将该纹理绑定到 GL_TEXTURE_EXTERNAL_OES 纹理对象上。具体实现逻辑参考：<a href=\"https://juejin.cn/post/7012517274768179236\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">SurfaceTexture</span> <span class=\"variable\">surfaceTexture</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceTexture</span>(videoTextureId);</span><br><span class=\"line\">player.setUpSurface(<span class=\"keyword\">new</span> <span class=\"title class_\">Surface</span>(surfaceTexture), width, height);</span><br><span class=\"line\">surfaceTexture.setDefaultBufferSize(width, height);</span><br><span class=\"line\">surfaceTexture.setOnFrameAvailableListener(surfaceTexture -&gt; &#123;……&#125;);;</span><br></pre></td></tr></table></figure>\n\n\n<p>其中 videoTextureId 来源于创建的 OES 纹理：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createOESTextureID</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span>[] texture = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"comment\">// 创建纹理对象，一个容器对象，保存渲染所需要的纹理数据，例如：图像数据</span></span><br><span class=\"line\">        <span class=\"comment\">//在OpenGL 中纹理对象是一个无符号整数，是一个纹理对象的句柄</span></span><br><span class=\"line\">        GLES30.glGenTextures(texture.length, texture, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 绑定纹理ID到纹理单元的纹理目标上</span></span><br><span class=\"line\">        GLES30.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, texture[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 设置纹理参数</span></span><br><span class=\"line\">        ……</span><br><span class=\"line\"></span><br><span class=\"line\">        GLES30.glGenerateMipmap(GLES11Ext.GL_TEXTURE_EXTERNAL_OES);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> texture[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"（四）FBO纹理数据到-Unity-的纹理数据\"><a href=\"#（四）FBO纹理数据到-Unity-的纹理数据\" class=\"headerlink\" title=\"（四）FBO纹理数据到 Unity 的纹理数据\"></a>（四）FBO纹理数据到 Unity 的纹理数据</h3><p>学习了解到Unity中可以使用 RawImage 或者 quad 等相关控件可以显示纹理，这里以 RawImage 为例。在 Unity 脚本编写初始化的逻辑，构造一个 Texture2D 对象，将句柄传递到 Android，并赋值给 RawImage，并将texture id 传递到 Android 平台，完成一次渲染的重定向。</p>\n<figure class=\"highlight c#\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">InitPlayer</span>()</span></span><br><span class=\"line\"> &#123;    </span><br><span class=\"line\">    Texture2D texture2D = <span class=\"keyword\">new</span> Texture2D(width, height, TextureFormat.RGB24, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>);</span><br><span class=\"line\">    androidObj.Call(<span class=\"string\">&quot;init&quot;</span>, (<span class=\"built_in\">int</span>)texture2D.GetNativeTexturePtr(), width, height);</span><br><span class=\"line\">    RawImage.texture = texture2D;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>创建FBO</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createFBO</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>[] fbo = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">    GLES30.glGenFramebuffers(fbo.length, fbo, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fbo[<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>为<code>SurfaceTexture</code> 设置了 <code>OnFrameAvailableListener</code> 后，当有新的图形流数据生成之后，就可以通过  <code>mSurfaceTexture.updateTexImage()</code> 将当前图片流更新到纹理所关联的OpenGLES中纹理，并绘制 FBO.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">publc <span class=\"keyword\">void</span> <span class=\"title function_\">draw</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 绑定 FrameBuffer 到当前的绘制环境上， 后续 GL 绘制都会到这个 framebuffer</span></span><br><span class=\"line\">    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fbo[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//2.把一个2D纹理作为帧缓冲区附着</span></span><br><span class=\"line\">    <span class=\"comment\">//即所有渲染操作的结果将会被储存在 unityTextureId 对应的纹理图像中</span></span><br><span class=\"line\">    GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, unityTextureId, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//绑定指定纹理到当前激活的纹理单元</span></span><br><span class=\"line\">    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, videoTextureId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//…… 省略 Opengl 绘制的常规流程</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这一步是最关键的，实现了将 FBO 的输出指向 Unity 里面创建的纹理，也就实现了 Android 渲染与 Unity 之间的数据打通。</p>\n<p>这里的 unityTextureId 来源于在 Unity 中初始化的 <code>(int)texture2D.GetNativeTexturePtr()</code>值。</p>\n<p>整体的流程为：</p>\n<img src=\"https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png\">\n<p>效果图：</p>\n<img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif\">\n\n<p>图中播放视频区域为 Unity 的 RawImage 控件，渲染的视频通过 Pag 等相关素材由渲染SDK合成。</p>\n<p>如图所示，视频画面正常地进行渲染，图中有两个区域展示了视频画面，上面的使用的 Quad 组件，下面是用的 RawImage，流程都一直，只是在 Unity 使用 Texture2D 的时候通过 <code>Quad.mainTexture = texture2D</code> 赋值。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲了 Unity 利用 Android 提供的能力进行视频相关的特效渲染的方案，总体正常运行。还需要一些优化，例如对 <code>Multithreaded Rendering</code>配置还未支持，以及一些逻辑可能受限于游戏侧的配置，例如图形渲染的配置使用的 OpenGL3.0，如果使用 OpenGL2.0 或者 Vulkan，还需要单独调整相关逻辑。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"Unity","slug":"Unity","api":"api/tags/Unity.json"},{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2022/10/25/Unity-实现利用-Andorid-能力进行视频渲染播放.json"}],"info":{"type":"tag","name":"Unity","slug":"Unity"}},"api":"api/tags/Unity/page.1.json"}