{"data":{"index":1,"total":1,"posts":[{"title":"3D LUT 滤镜 shader 源码分析","slug":"LUT-Shader-源码分析","date":"2022-11-24T03:36:00.000Z","updated":"2025-09-15T13:11:04.747Z","comments":true,"url":"2022/11/24/LUT-Shader-源码分析/","excerpt":"<p>最近在做滤镜相关的渲染学习，目前大部分 LUT 滤镜代码实现都是参考由 GPUImage 提供的 LookupFilter 的逻辑，整个代码实现不多。参考网上的博文也有各种解释，参考了大量博文之后终于理解了，所以自己重新整理了一份，方便以后阅读理解，对整体代码的实现过程结合LUT的原理进行一个简单整理。</p>\n<h2 id=\"GPUImageLookupFilter-shader-源码\"><a href=\"#GPUImageLookupFilter-shader-源码\" class=\"headerlink\" title=\"GPUImageLookupFilter shader 源码\"></a>GPUImageLookupFilter shader 源码</h2><figure class=\"highlight glsl\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate;      </span><br><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture;  <span class=\"comment\">// 目标纹理，对应原始资源</span></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture2; <span class=\"comment\">// 查找表纹理，对应LUT图片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"keyword\">lowp</span> <span class=\"type\">float</span> intensity;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> main()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//获取原始图层颜色</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec4</span> textureColor = <span class=\"built_in\">texture2D</span>(inputImageTexture, textureCoordinate);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取蓝色通道颜色，textureColor.b 的范围为(0,1)，blueColor 范围为(0,63) </span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">float</span> blueColor = textureColor.b * <span class=\"number\">63.0</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad1为查找颜色所在左边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad1;</span><br><span class=\"line\">    quad1.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">floor</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad1.x = <span class=\"built_in\">floor</span>(blueColor) - (quad1.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad2为查找颜色所在右边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad2;</span><br><span class=\"line\">    quad2.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">ceil</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad2.x = <span class=\"built_in\">ceil</span>(blueColor) - (quad2.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取到左边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos1;</span><br><span class=\"line\">    texPos1.x = (quad1.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos1.y = (quad1.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">   <span class=\"comment\">//获取到右边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos2;</span><br><span class=\"line\">    texPos2.x = (quad2.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos2.y = (quad2.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取对应位置纹理的颜色 RGBA 值</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor1 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos1);</span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor2 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos2);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//真正的颜色是 newColor1 和 newColor2 的混合</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor = <span class=\"built_in\">mix</span>(newColor1, newColor2, <span class=\"built_in\">fract</span>(blueColor));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">gl_FragColor</span> = <span class=\"built_in\">mix</span>(textureColor, <span class=\"type\">vec4</span>(newColor.rgb, textureColor.w), intensity);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>整个源码的主要逻辑为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>。上面注释已将具体的实现过程描述清楚，但与我们的 LUT 图片割裂，接下来结合 LUT 的实现原理以及具体的数据来形象地描述整个实现流程。</p>\n<p>假设我们输入的参数为：<br>textureColor &#x3D; ver4(.0, .0, 0.5, 1.0)</p>\n<h2 id=\"查找颜色所在位置的小正方形\"><a href=\"#查找颜色所在位置的小正方形\" class=\"headerlink\" title=\"查找颜色所在位置的小正方形\"></a>查找颜色所在位置的小正方形</h2><p>我们知道LUT有64个小正方形，目标是为了找到对应小正方形里面的对应的颜色，我们需要先确认是第几个小正方形，正是通过 textureColor.b * 63 查找</p>\n<p>带入<code>blueColor</code> -&gt; textureColor.b &#x3D; 0.5<br>对 <code>textureColor.b * 63.0</code> &#x3D; 31.5</p>\n<p>也就是说我们需要第 [31.5] 位置小正方形，但是索引(从0-63共64个)都是正数，对于 31.5 索引 我们该怎么确定是 31 还是第 32 个呢？GPUImage给出的一种插值方式就是两个都要，然后进行一次混合，从而使得值能够俊均匀的在两个小正方形色块中。</p>\n<p>具体逻辑为：</p>\n<p>quad1.y &#x3D; floor(floor(blueColor) &#x2F; 8.0) &#x3D; 3，确定为小方块在纵坐标索引3，也就是第4行。</p>\n<img src=\"https://cdn.julis.wang/blog/img/f2e1e14bc30c4e438664b90fa5ad8103.png?imageView2/2/w/500\">\n\n<p><code>quad1.x = floor(blueColor) - (quad1.y * 8.0) = 31 - 24 = 7</code></p>\n<p>也就确定了小方块为(3,7) 也就是第4排第8个。</p>\n<img src=\"https://cdn.julis.wang/blog/img/bc8a47389a654dbeb5e4fc5e5584d1f9.png?imageView2/2/w/500\">\n<p>同理，对于第2个小方块确定的位置为(4,0) 也就是第5排第1个。</p>\n<p> <code>quad2.y = floor(ceil(blueColor) / 8.0) = 4</code></p>\n<p> <code>quad2.x = ceil(blueColor) - (quad2.y * 8.0)= 0</code></p>\n<h2 id=\"查找小正方形内的具体颜色\"><a href=\"#查找小正方形内的具体颜色\" class=\"headerlink\" title=\"查找小正方形内的具体颜色\"></a>查找小正方形内的具体颜色</h2><p>已经获取到对应的方块了，接下来需要确定方块内的像素的位置了。一般一个LUT的大小为 512x512，由8x8小方块构成，也就是每个方块的的像素为64x64，如下图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/a4d6f390dd7b41fab75b568b37fb1e08.png?imageView2/2/w/500\">\n<p>计算x坐标的逻辑为：</p>\n<p><code>texPos1.x = (quad1.x * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.r)</code></p>\n<p>这一段是相对比较难理解的，我们可以分几部分进行理解：</p>\n<p>第一部分：<strong>(quad1.x * 0.125)</strong></p>\n<p>  我们得到 quad1.x &#x3D; 7，也就是第8列，*0.125将坐标转化在(0,1)之间，也就是得到在01坐标系内如图红线的位置。</p>\n<img src=\"https://cdn.julis.wang/blog/img/56098332877d4724beae4000c4fdf5fa.png?imageView2/2/w/500\">\n<p>第二部分：<strong>((0.125 - 1.0&#x2F;512.0) * textureColor.r)</strong></p>\n<p>我们可以把它当成 <code>(63.0/512.0)* textureColor.r</code> , <code>63.0/512.0</code>代表着一个512x512中每个小方块的64份数据（为什么是63？别忘了0的存在），textureColor.r 数据在 0-1之间，这样就能确认在第一部分结果基础之上的偏移值。</p>\n<img src=\"https://cdn.julis.wang/blog/img/7b3cba46937a4be4b0e45bc3077d3aad.png?imageView2/2/w/500\">\n<p>第三部分：<strong>0.5&#x2F;512.0</strong></p>\n<p>这一部分主要是 +0.5 做四舍五入运算，为保证第512行取到的是511.5&#x2F;512，第1行取到的是 0.5&#x2F;512.0。</p>\n<p>同理，计算y的坐标，以及计算另一个小正方形内的位置是一样的。</p>\n<p>最后在通过对从两个小正方形获取到的颜色进行 mix，并返回给着色器，GPU再对原始图像进行每一个像素点绘制，从而实现滤镜的效果。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>LUT 对应的 Shader 执行过程主要为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>，整个流程都比较好理解，但代码相对而言比较难理解，网上看了很多其他的大佬写的一些文章，最开始自己看的时候也是很难理解的，后面终于悟了，所以想通过自己的理解，尽力更形象地解释（虽然可能也没有很形象），如果还有什么疑问，欢迎一起交流学习。</p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/f2e1e14bc30c4e438664b90fa5ad8103.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/bc8a47389a654dbeb5e4fc5e5584d1f9.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/a4d6f390dd7b41fab75b568b37fb1e08.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/56098332877d4724beae4000c4fdf5fa.png?imageView2/2/w/500","https://cdn.julis.wang/blog/img/7b3cba46937a4be4b0e45bc3077d3aad.png?imageView2/2/w/500"],"content":"<p>最近在做滤镜相关的渲染学习，目前大部分 LUT 滤镜代码实现都是参考由 GPUImage 提供的 LookupFilter 的逻辑，整个代码实现不多。参考网上的博文也有各种解释，参考了大量博文之后终于理解了，所以自己重新整理了一份，方便以后阅读理解，对整体代码的实现过程结合LUT的原理进行一个简单整理。</p>\n<h2 id=\"GPUImageLookupFilter-shader-源码\"><a href=\"#GPUImageLookupFilter-shader-源码\" class=\"headerlink\" title=\"GPUImageLookupFilter shader 源码\"></a>GPUImageLookupFilter shader 源码</h2><figure class=\"highlight glsl\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate;      </span><br><span class=\"line\"><span class=\"keyword\">varying</span> <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> textureCoordinate2;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture;  <span class=\"comment\">// 目标纹理，对应原始资源</span></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"type\">sampler2D</span> inputImageTexture2; <span class=\"comment\">// 查找表纹理，对应LUT图片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">uniform</span> <span class=\"keyword\">lowp</span> <span class=\"type\">float</span> intensity;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">void</span> main()</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"comment\">//获取原始图层颜色</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec4</span> textureColor = <span class=\"built_in\">texture2D</span>(inputImageTexture, textureCoordinate);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取蓝色通道颜色，textureColor.b 的范围为(0,1)，blueColor 范围为(0,63) </span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">float</span> blueColor = textureColor.b * <span class=\"number\">63.0</span>;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad1为查找颜色所在左边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad1;</span><br><span class=\"line\">    quad1.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">floor</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad1.x = <span class=\"built_in\">floor</span>(blueColor) - (quad1.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//quad2为查找颜色所在右边位置的小正方形</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> quad2;</span><br><span class=\"line\">    quad2.y = <span class=\"built_in\">floor</span>(<span class=\"built_in\">ceil</span>(blueColor) / <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    quad2.x = <span class=\"built_in\">ceil</span>(blueColor) - (quad2.y * <span class=\"number\">8.0</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取到左边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos1;</span><br><span class=\"line\">    texPos1.x = (quad1.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos1.y = (quad1.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">   <span class=\"comment\">//获取到右边小方形里面的颜色值</span></span><br><span class=\"line\">    <span class=\"keyword\">highp</span> <span class=\"type\">vec2</span> texPos2;</span><br><span class=\"line\">    texPos2.x = (quad2.x * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.r);</span><br><span class=\"line\">    texPos2.y = (quad2.y * <span class=\"number\">0.125</span>) + <span class=\"number\">0.5</span>/<span class=\"number\">512.0</span> + ((<span class=\"number\">0.125</span> - <span class=\"number\">1.0</span>/<span class=\"number\">512.0</span>) * textureColor.g);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//获取对应位置纹理的颜色 RGBA 值</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor1 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos1);</span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor2 = <span class=\"built_in\">texture2D</span>(inputImageTexture2, texPos2);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//真正的颜色是 newColor1 和 newColor2 的混合</span></span><br><span class=\"line\">    <span class=\"keyword\">lowp</span> <span class=\"type\">vec4</span> newColor = <span class=\"built_in\">mix</span>(newColor1, newColor2, <span class=\"built_in\">fract</span>(blueColor));</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">gl_FragColor</span> = <span class=\"built_in\">mix</span>(textureColor, <span class=\"type\">vec4</span>(newColor.rgb, textureColor.w), intensity);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n\n<p>整个源码的主要逻辑为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>。上面注释已将具体的实现过程描述清楚，但与我们的 LUT 图片割裂，接下来结合 LUT 的实现原理以及具体的数据来形象地描述整个实现流程。</p>\n<p>假设我们输入的参数为：<br>textureColor &#x3D; ver4(.0, .0, 0.5, 1.0)</p>\n<h2 id=\"查找颜色所在位置的小正方形\"><a href=\"#查找颜色所在位置的小正方形\" class=\"headerlink\" title=\"查找颜色所在位置的小正方形\"></a>查找颜色所在位置的小正方形</h2><p>我们知道LUT有64个小正方形，目标是为了找到对应小正方形里面的对应的颜色，我们需要先确认是第几个小正方形，正是通过 textureColor.b * 63 查找</p>\n<p>带入<code>blueColor</code> -&gt; textureColor.b &#x3D; 0.5<br>对 <code>textureColor.b * 63.0</code> &#x3D; 31.5</p>\n<p>也就是说我们需要第 [31.5] 位置小正方形，但是索引(从0-63共64个)都是正数，对于 31.5 索引 我们该怎么确定是 31 还是第 32 个呢？GPUImage给出的一种插值方式就是两个都要，然后进行一次混合，从而使得值能够俊均匀的在两个小正方形色块中。</p>\n<p>具体逻辑为：</p>\n<p>quad1.y &#x3D; floor(floor(blueColor) &#x2F; 8.0) &#x3D; 3，确定为小方块在纵坐标索引3，也就是第4行。</p>\n<img src=\"https://cdn.julis.wang/blog/img/f2e1e14bc30c4e438664b90fa5ad8103.png?imageView2/2/w/500\">\n\n<p><code>quad1.x = floor(blueColor) - (quad1.y * 8.0) = 31 - 24 = 7</code></p>\n<p>也就确定了小方块为(3,7) 也就是第4排第8个。</p>\n<img src=\"https://cdn.julis.wang/blog/img/bc8a47389a654dbeb5e4fc5e5584d1f9.png?imageView2/2/w/500\">\n<p>同理，对于第2个小方块确定的位置为(4,0) 也就是第5排第1个。</p>\n<p> <code>quad2.y = floor(ceil(blueColor) / 8.0) = 4</code></p>\n<p> <code>quad2.x = ceil(blueColor) - (quad2.y * 8.0)= 0</code></p>\n<h2 id=\"查找小正方形内的具体颜色\"><a href=\"#查找小正方形内的具体颜色\" class=\"headerlink\" title=\"查找小正方形内的具体颜色\"></a>查找小正方形内的具体颜色</h2><p>已经获取到对应的方块了，接下来需要确定方块内的像素的位置了。一般一个LUT的大小为 512x512，由8x8小方块构成，也就是每个方块的的像素为64x64，如下图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/a4d6f390dd7b41fab75b568b37fb1e08.png?imageView2/2/w/500\">\n<p>计算x坐标的逻辑为：</p>\n<p><code>texPos1.x = (quad1.x * 0.125) + 0.5/512.0 + ((0.125 - 1.0/512.0) * textureColor.r)</code></p>\n<p>这一段是相对比较难理解的，我们可以分几部分进行理解：</p>\n<p>第一部分：<strong>(quad1.x * 0.125)</strong></p>\n<p>  我们得到 quad1.x &#x3D; 7，也就是第8列，*0.125将坐标转化在(0,1)之间，也就是得到在01坐标系内如图红线的位置。</p>\n<img src=\"https://cdn.julis.wang/blog/img/56098332877d4724beae4000c4fdf5fa.png?imageView2/2/w/500\">\n<p>第二部分：<strong>((0.125 - 1.0&#x2F;512.0) * textureColor.r)</strong></p>\n<p>我们可以把它当成 <code>(63.0/512.0)* textureColor.r</code> , <code>63.0/512.0</code>代表着一个512x512中每个小方块的64份数据（为什么是63？别忘了0的存在），textureColor.r 数据在 0-1之间，这样就能确认在第一部分结果基础之上的偏移值。</p>\n<img src=\"https://cdn.julis.wang/blog/img/7b3cba46937a4be4b0e45bc3077d3aad.png?imageView2/2/w/500\">\n<p>第三部分：<strong>0.5&#x2F;512.0</strong></p>\n<p>这一部分主要是 +0.5 做四舍五入运算，为保证第512行取到的是511.5&#x2F;512，第1行取到的是 0.5&#x2F;512.0。</p>\n<p>同理，计算y的坐标，以及计算另一个小正方形内的位置是一样的。</p>\n<p>最后在通过对从两个小正方形获取到的颜色进行 mix，并返回给着色器，GPU再对原始图像进行每一个像素点绘制，从而实现滤镜的效果。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>LUT 对应的 Shader 执行过程主要为：<strong>查找颜色所在位置的小正方形、查找小正方形内的具体颜色、颜色混合</strong>，整个流程都比较好理解，但代码相对而言比较难理解，网上看了很多其他的大佬写的一些文章，最开始自己看的时候也是很难理解的，后面终于悟了，所以想通过自己的理解，尽力更形象地解释（虽然可能也没有很形象），如果还有什么疑问，欢迎一起交流学习。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2022/11/24/LUT-Shader-源码分析.json"},{"title":"Unity 实现利用 Andorid 能力进行视频渲染播放","slug":"Unity-实现利用-Andorid-能力进行视频渲染播放","date":"2022-10-25T02:00:00.000Z","updated":"2025-09-15T13:10:48.829Z","comments":true,"url":"2022/10/25/Unity-实现利用-Andorid-能力进行视频渲染播放/","excerpt":"<p>在 Unity 中使用 Android 侧提供的视频渲染相关的能力，有两种方案可选：</p>\n<p>第一种是将渲染播放页单独做一个页面，在 Unity事件交互的时候打开对应 Activity 页面，或者获取到 Unity 创建的 Acitivity 动态添加 View。</p>\n<p>第二种是只借助 Android 的渲染能力，将数据渲染到 Unity 的控件上。</p>\n<p>两种方案各有优劣，第一种大大地减少了播放器相关的开发工作量，整个页面逻辑可以实现复用，但是交互页面的话 iOS&#x2F;Android 需要写两套。第二种实现成本相对较高，但是交互可以由 Unity 侧进行，只是播放器使用封装好的 plugin 进行，能达到交互相对较统一，本文也主要是讲述该方案的实现。</p>\n<h2 id=\"Android-平台基本播放逻辑\"><a href=\"#Android-平台基本播放逻辑\" class=\"headerlink\" title=\"Android 平台基本播放逻辑\"></a>Android 平台基本播放逻辑</h2><p>在正式开发改造之前，对 Android 侧的一个播放器渲染流程进行简单的介绍，以 MediaPlayer 为例，利用 MediaPlayer 进行视频解码渲染，并将视频最后输出到 SurfaceView 上,一次播放器视频渲染到View上的的主要代码流程为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initPlayer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MediaPlayer</span> <span class=\"variable\">mediaPlayer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MediaPlayer</span>();</span><br><span class=\"line\">    <span class=\"type\">SurfaceView</span> <span class=\"variable\">surfaceView</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceView</span>(activity);</span><br><span class=\"line\">    surfaceHolder = surfaceView.getHolder();</span><br><span class=\"line\">    surfaceHolder.addCallback(^ &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">surfaceCreated</span><span class=\"params\">(SurfaceHolder holder)</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">Surface</span> <span class=\"variable\">surface</span> <span class=\"operator\">=</span> holder.getSurface();</span><br><span class=\"line\">            mediaPlayer.setSurface(surface);</span><br><span class=\"line\">            mediaPlayer.prepareAsync();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         ……</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    mediaPlayer.setDataSource(URI...);</span><br><span class=\"line\">    mediaPlayer.setOnPreparedListener(mp -&gt; mp.start());   </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>对于渲染 <code>mediaPlayer.setSurface(surface)</code> 设为播放器解码数据的接受器，Surface 来自于 SurfaceView。</p>\n<p>播放器是将数据图形绘制在 Surface 对象上，Surface中会关联一个 BufferQueue 用于提供图像数据缓存，SurfaceFlinger 会把 Surface 对应的图像层混合在一起，将其输出到 FrameBuffer 中（Framebuffer就是一块内存区域，它通常是显示驱动的内部缓冲区在内存中的映射），最后在屏幕上看到合成的图像。</p>\n<p>整个流程引入外部大佬的一张图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg\">\n<h2 id=\"Unity-中的一些改造\"><a href=\"#Unity-中的一些改造\" class=\"headerlink\" title=\"Unity 中的一些改造\"></a>Unity 中的一些改造</h2><p>上面的流程最终是通过播放器解码渲染到 SurfaceView 上，当然，你可以通过获取到 UnityPlayer 对应的 Acitivity 将这个 SurfaceView 动态添加到当前界面，实现“在 Unity 中利用 Android 能力进行视频渲染”。</p>\n<p>所以需要对其进行改造，我们的目的是实现 Android 播放器数据渲染到 Untiy 的组件中。实现这一过程需要借助 FBO(Frame Buffer Object) 的能力。</p>\n<h3 id=\"（一）FBO\"><a href=\"#（一）FBO\" class=\"headerlink\" title=\"（一）FBO\"></a>（一）FBO</h3><p>在 OpenGL 渲染管线中几何数据和纹理经过变换和一些测试处理，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存(framebuffer)，OpenGL渲染管线的最终位置是在帧缓冲区中，默认情况下 OpenGL 使用的是窗口系统提供的帧缓冲区。</p>\n<p>但有些场景是不想要直接渲染到窗口上的(例如加视频特效)，于是 OpenGL 提供了一种方式来创建额外的帧缓冲区对象(FBO)。使用帧缓冲区对象，OpenGL 可以将原先绘制到窗口提供的帧缓冲区重定向到 FBO 之中。FBO本身不是一块内存，没有空间，真正存储东西，可实际读写的是依附于FBO的东西：纹理(texture)和渲染缓存(renderbuffer)，依附的方式，是一个二维数组来管理，结构如图所示：</p>\n<p><img src=\"https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&width=380\"></p>\n<h3 id=\"（二）具体实现\"><a href=\"#（二）具体实现\" class=\"headerlink\" title=\"（二）具体实现\"></a>（二）具体实现</h3><p>使用 FBO 我们可以将渲染目标渲染到其他的空间，我们目的是将播放器解码后的数据渲染到 Unity 控件的纹理空间中。<br>渲染播放器将输出到 FBO 中，FBO 指向 Unity 控件数据的输入，从而实现：Android 的播放器输出数据显示到 Unity 的控件中。</p>\n<h3 id=\"（三）从渲染输出数据到外部纹理\"><a href=\"#（三）从渲染输出数据到外部纹理\" class=\"headerlink\" title=\"（三）从渲染输出数据到外部纹理\"></a>（三）从渲染输出数据到外部纹理</h3><p>由于 <code>mediaPlayer.setSurface(surface)</code> 对应的 Surface 来源于 SurafaceView，会直接渲染到屏幕上，这里我们需要使用 构造一个新的 SurfaceTexture 以将图像流式传输到给定的 OpenGL 纹理;</p>\n<p>要获取到播放器渲染得数据，需要借助 SurfaceTexture ，SurfaceTexture 是Surface 和 OpenGL ES 纹理的结合，其对图像流的处理并不直接显示，而是从图像流中捕获帧作为 OpenGL 的外部纹理，图像流来自相机预览和视频解码。</p>\n<p>SurfaceTexture 创建的 Surface 是数据的生产者，而 SurfaceTexture 是对应的消费者，Surface 接收媒体数据并将数据发送到 SurfaceTexture，当调用 updateTexImage 的时候，创建SurfaceTexture 的纹理对象相应的内容将更新为最新图像帧，也就是会将图像帧转换为 GL 纹理，并将该纹理绑定到 GL_TEXTURE_EXTERNAL_OES 纹理对象上。具体实现逻辑参考：<a href=\"https://juejin.cn/post/7012517274768179236\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">SurfaceTexture</span> <span class=\"variable\">surfaceTexture</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceTexture</span>(videoTextureId);</span><br><span class=\"line\">player.setUpSurface(<span class=\"keyword\">new</span> <span class=\"title class_\">Surface</span>(surfaceTexture), width, height);</span><br><span class=\"line\">surfaceTexture.setDefaultBufferSize(width, height);</span><br><span class=\"line\">surfaceTexture.setOnFrameAvailableListener(surfaceTexture -&gt; &#123;……&#125;);;</span><br></pre></td></tr></table></figure>\n\n\n<p>其中 videoTextureId 来源于创建的 OES 纹理：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createOESTextureID</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span>[] texture = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"comment\">// 创建纹理对象，一个容器对象，保存渲染所需要的纹理数据，例如：图像数据</span></span><br><span class=\"line\">        <span class=\"comment\">//在OpenGL 中纹理对象是一个无符号整数，是一个纹理对象的句柄</span></span><br><span class=\"line\">        GLES30.glGenTextures(texture.length, texture, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 绑定纹理ID到纹理单元的纹理目标上</span></span><br><span class=\"line\">        GLES30.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, texture[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 设置纹理参数</span></span><br><span class=\"line\">        ……</span><br><span class=\"line\"></span><br><span class=\"line\">        GLES30.glGenerateMipmap(GLES11Ext.GL_TEXTURE_EXTERNAL_OES);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> texture[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"（四）FBO纹理数据到-Unity-的纹理数据\"><a href=\"#（四）FBO纹理数据到-Unity-的纹理数据\" class=\"headerlink\" title=\"（四）FBO纹理数据到 Unity 的纹理数据\"></a>（四）FBO纹理数据到 Unity 的纹理数据</h3><p>学习了解到Unity中可以使用 RawImage 或者 quad 等相关控件可以显示纹理，这里以 RawImage 为例。在 Unity 脚本编写初始化的逻辑，构造一个 Texture2D 对象，将句柄传递到 Android，并赋值给 RawImage，并将texture id 传递到 Android 平台，完成一次渲染的重定向。</p>\n<figure class=\"highlight c#\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">InitPlayer</span>()</span></span><br><span class=\"line\"> &#123;    </span><br><span class=\"line\">    Texture2D texture2D = <span class=\"keyword\">new</span> Texture2D(width, height, TextureFormat.RGB24, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>);</span><br><span class=\"line\">    androidObj.Call(<span class=\"string\">&quot;init&quot;</span>, (<span class=\"built_in\">int</span>)texture2D.GetNativeTexturePtr(), width, height);</span><br><span class=\"line\">    RawImage.texture = texture2D;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>创建FBO</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createFBO</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>[] fbo = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">    GLES30.glGenFramebuffers(fbo.length, fbo, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fbo[<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>为<code>SurfaceTexture</code> 设置了 <code>OnFrameAvailableListener</code> 后，当有新的图形流数据生成之后，就可以通过  <code>mSurfaceTexture.updateTexImage()</code> 将当前图片流更新到纹理所关联的OpenGLES中纹理，并绘制 FBO.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">publc <span class=\"keyword\">void</span> <span class=\"title function_\">draw</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 绑定 FrameBuffer 到当前的绘制环境上， 后续 GL 绘制都会到这个 framebuffer</span></span><br><span class=\"line\">    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fbo[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//2.把一个2D纹理作为帧缓冲区附着</span></span><br><span class=\"line\">    <span class=\"comment\">//即所有渲染操作的结果将会被储存在 unityTextureId 对应的纹理图像中</span></span><br><span class=\"line\">    GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, unityTextureId, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//绑定指定纹理到当前激活的纹理单元</span></span><br><span class=\"line\">    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, videoTextureId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//…… 省略 Opengl 绘制的常规流程</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这一步是最关键的，实现了将 FBO 的输出指向 Unity 里面创建的纹理，也就实现了 Android 渲染与 Unity 之间的数据打通。</p>\n<p>这里的 unityTextureId 来源于在 Unity 中初始化的 <code>(int)texture2D.GetNativeTexturePtr()</code>值。</p>\n<p>整体的流程为：</p>\n<img src=\"https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png\">\n<p>效果图：</p>\n<img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif\">\n\n<p>图中播放视频区域为 Unity 的 RawImage 控件，渲染的视频通过 Pag 等相关素材由渲染SDK合成。</p>\n<p>如图所示，视频画面正常地进行渲染，图中有两个区域展示了视频画面，上面的使用的 Quad 组件，下面是用的 RawImage，流程都一直，只是在 Unity 使用 Texture2D 的时候通过 <code>Quad.mainTexture = texture2D</code> 赋值。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲了 Unity 利用 Android 提供的能力进行视频相关的特效渲染的方案，总体正常运行。还需要一些优化，例如对 <code>Multithreaded Rendering</code>配置还未支持，以及一些逻辑可能受限于游戏侧的配置，例如图形渲染的配置使用的 OpenGL3.0，如果使用 OpenGL2.0 或者 Vulkan，还需要单独调整相关逻辑。</p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg","https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&width=380","https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png","https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif"],"content":"<p>在 Unity 中使用 Android 侧提供的视频渲染相关的能力，有两种方案可选：</p>\n<p>第一种是将渲染播放页单独做一个页面，在 Unity事件交互的时候打开对应 Activity 页面，或者获取到 Unity 创建的 Acitivity 动态添加 View。</p>\n<p>第二种是只借助 Android 的渲染能力，将数据渲染到 Unity 的控件上。</p>\n<p>两种方案各有优劣，第一种大大地减少了播放器相关的开发工作量，整个页面逻辑可以实现复用，但是交互页面的话 iOS&#x2F;Android 需要写两套。第二种实现成本相对较高，但是交互可以由 Unity 侧进行，只是播放器使用封装好的 plugin 进行，能达到交互相对较统一，本文也主要是讲述该方案的实现。</p>\n<h2 id=\"Android-平台基本播放逻辑\"><a href=\"#Android-平台基本播放逻辑\" class=\"headerlink\" title=\"Android 平台基本播放逻辑\"></a>Android 平台基本播放逻辑</h2><p>在正式开发改造之前，对 Android 侧的一个播放器渲染流程进行简单的介绍，以 MediaPlayer 为例，利用 MediaPlayer 进行视频解码渲染，并将视频最后输出到 SurfaceView 上,一次播放器视频渲染到View上的的主要代码流程为：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">initPlayer</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">MediaPlayer</span> <span class=\"variable\">mediaPlayer</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">MediaPlayer</span>();</span><br><span class=\"line\">    <span class=\"type\">SurfaceView</span> <span class=\"variable\">surfaceView</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceView</span>(activity);</span><br><span class=\"line\">    surfaceHolder = surfaceView.getHolder();</span><br><span class=\"line\">    surfaceHolder.addCallback(^ &#123;</span><br><span class=\"line\">        <span class=\"meta\">@Override</span></span><br><span class=\"line\">        <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">surfaceCreated</span><span class=\"params\">(SurfaceHolder holder)</span> &#123;</span><br><span class=\"line\">            <span class=\"type\">Surface</span> <span class=\"variable\">surface</span> <span class=\"operator\">=</span> holder.getSurface();</span><br><span class=\"line\">            mediaPlayer.setSurface(surface);</span><br><span class=\"line\">            mediaPlayer.prepareAsync();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">         ……</span><br><span class=\"line\">    &#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    mediaPlayer.setDataSource(URI...);</span><br><span class=\"line\">    mediaPlayer.setOnPreparedListener(mp -&gt; mp.start());   </span><br><span class=\"line\">&#125;  </span><br></pre></td></tr></table></figure>\n\n<p>对于渲染 <code>mediaPlayer.setSurface(surface)</code> 设为播放器解码数据的接受器，Surface 来自于 SurfaceView。</p>\n<p>播放器是将数据图形绘制在 Surface 对象上，Surface中会关联一个 BufferQueue 用于提供图像数据缓存，SurfaceFlinger 会把 Surface 对应的图像层混合在一起，将其输出到 FrameBuffer 中（Framebuffer就是一块内存区域，它通常是显示驱动的内部缓冲区在内存中的映射），最后在屏幕上看到合成的图像。</p>\n<p>整个流程引入外部大佬的一张图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/k3u1fbpfcp.jpg\">\n<h2 id=\"Unity-中的一些改造\"><a href=\"#Unity-中的一些改造\" class=\"headerlink\" title=\"Unity 中的一些改造\"></a>Unity 中的一些改造</h2><p>上面的流程最终是通过播放器解码渲染到 SurfaceView 上，当然，你可以通过获取到 UnityPlayer 对应的 Acitivity 将这个 SurfaceView 动态添加到当前界面，实现“在 Unity 中利用 Android 能力进行视频渲染”。</p>\n<p>所以需要对其进行改造，我们的目的是实现 Android 播放器数据渲染到 Untiy 的组件中。实现这一过程需要借助 FBO(Frame Buffer Object) 的能力。</p>\n<h3 id=\"（一）FBO\"><a href=\"#（一）FBO\" class=\"headerlink\" title=\"（一）FBO\"></a>（一）FBO</h3><p>在 OpenGL 渲染管线中几何数据和纹理经过变换和一些测试处理，最后以二维像素的形式显示在屏幕上。OpenGL管线的最终渲染目的地被称作帧缓存(framebuffer)，OpenGL渲染管线的最终位置是在帧缓冲区中，默认情况下 OpenGL 使用的是窗口系统提供的帧缓冲区。</p>\n<p>但有些场景是不想要直接渲染到窗口上的(例如加视频特效)，于是 OpenGL 提供了一种方式来创建额外的帧缓冲区对象(FBO)。使用帧缓冲区对象，OpenGL 可以将原先绘制到窗口提供的帧缓冲区重定向到 FBO 之中。FBO本身不是一块内存，没有空间，真正存储东西，可实际读写的是依附于FBO的东西：纹理(texture)和渲染缓存(renderbuffer)，依附的方式，是一个二维数组来管理，结构如图所示：</p>\n<p><img src=\"https://www.songho.ca/opengl/files/gl_fbo01.png?height=278&width=380\"></p>\n<h3 id=\"（二）具体实现\"><a href=\"#（二）具体实现\" class=\"headerlink\" title=\"（二）具体实现\"></a>（二）具体实现</h3><p>使用 FBO 我们可以将渲染目标渲染到其他的空间，我们目的是将播放器解码后的数据渲染到 Unity 控件的纹理空间中。<br>渲染播放器将输出到 FBO 中，FBO 指向 Unity 控件数据的输入，从而实现：Android 的播放器输出数据显示到 Unity 的控件中。</p>\n<h3 id=\"（三）从渲染输出数据到外部纹理\"><a href=\"#（三）从渲染输出数据到外部纹理\" class=\"headerlink\" title=\"（三）从渲染输出数据到外部纹理\"></a>（三）从渲染输出数据到外部纹理</h3><p>由于 <code>mediaPlayer.setSurface(surface)</code> 对应的 Surface 来源于 SurafaceView，会直接渲染到屏幕上，这里我们需要使用 构造一个新的 SurfaceTexture 以将图像流式传输到给定的 OpenGL 纹理;</p>\n<p>要获取到播放器渲染得数据，需要借助 SurfaceTexture ，SurfaceTexture 是Surface 和 OpenGL ES 纹理的结合，其对图像流的处理并不直接显示，而是从图像流中捕获帧作为 OpenGL 的外部纹理，图像流来自相机预览和视频解码。</p>\n<p>SurfaceTexture 创建的 Surface 是数据的生产者，而 SurfaceTexture 是对应的消费者，Surface 接收媒体数据并将数据发送到 SurfaceTexture，当调用 updateTexImage 的时候，创建SurfaceTexture 的纹理对象相应的内容将更新为最新图像帧，也就是会将图像帧转换为 GL 纹理，并将该纹理绑定到 GL_TEXTURE_EXTERNAL_OES 纹理对象上。具体实现逻辑参考：<a href=\"https://juejin.cn/post/7012517274768179236\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">SurfaceTexture</span> <span class=\"variable\">surfaceTexture</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">SurfaceTexture</span>(videoTextureId);</span><br><span class=\"line\">player.setUpSurface(<span class=\"keyword\">new</span> <span class=\"title class_\">Surface</span>(surfaceTexture), width, height);</span><br><span class=\"line\">surfaceTexture.setDefaultBufferSize(width, height);</span><br><span class=\"line\">surfaceTexture.setOnFrameAvailableListener(surfaceTexture -&gt; &#123;……&#125;);;</span><br></pre></td></tr></table></figure>\n\n\n<p>其中 videoTextureId 来源于创建的 OES 纹理：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createOESTextureID</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span>[] texture = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"comment\">// 创建纹理对象，一个容器对象，保存渲染所需要的纹理数据，例如：图像数据</span></span><br><span class=\"line\">        <span class=\"comment\">//在OpenGL 中纹理对象是一个无符号整数，是一个纹理对象的句柄</span></span><br><span class=\"line\">        GLES30.glGenTextures(texture.length, texture, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 绑定纹理ID到纹理单元的纹理目标上</span></span><br><span class=\"line\">        GLES30.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, texture[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 设置纹理参数</span></span><br><span class=\"line\">        ……</span><br><span class=\"line\"></span><br><span class=\"line\">        GLES30.glGenerateMipmap(GLES11Ext.GL_TEXTURE_EXTERNAL_OES);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> texture[<span class=\"number\">0</span>];</span><br><span class=\"line\">    &#125;</span><br></pre></td></tr></table></figure>\n\n\n<h3 id=\"（四）FBO纹理数据到-Unity-的纹理数据\"><a href=\"#（四）FBO纹理数据到-Unity-的纹理数据\" class=\"headerlink\" title=\"（四）FBO纹理数据到 Unity 的纹理数据\"></a>（四）FBO纹理数据到 Unity 的纹理数据</h3><p>学习了解到Unity中可以使用 RawImage 或者 quad 等相关控件可以显示纹理，这里以 RawImage 为例。在 Unity 脚本编写初始化的逻辑，构造一个 Texture2D 对象，将句柄传递到 Android，并赋值给 RawImage，并将texture id 传递到 Android 平台，完成一次渲染的重定向。</p>\n<figure class=\"highlight c#\"><table><tr><td class=\"code\"><pre><span class=\"line\"> <span class=\"function\"><span class=\"keyword\">void</span> <span class=\"title\">InitPlayer</span>()</span></span><br><span class=\"line\"> &#123;    </span><br><span class=\"line\">    Texture2D texture2D = <span class=\"keyword\">new</span> Texture2D(width, height, TextureFormat.RGB24, <span class=\"literal\">false</span>, <span class=\"literal\">false</span>);</span><br><span class=\"line\">    androidObj.Call(<span class=\"string\">&quot;init&quot;</span>, (<span class=\"built_in\">int</span>)texture2D.GetNativeTexturePtr(), width, height);</span><br><span class=\"line\">    RawImage.texture = texture2D;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>创建FBO</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">createFBO</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"type\">int</span>[] fbo = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[<span class=\"number\">1</span>];</span><br><span class=\"line\">    GLES30.glGenFramebuffers(fbo.length, fbo, <span class=\"number\">0</span>);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> fbo[<span class=\"number\">0</span>];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>为<code>SurfaceTexture</code> 设置了 <code>OnFrameAvailableListener</code> 后，当有新的图形流数据生成之后，就可以通过  <code>mSurfaceTexture.updateTexImage()</code> 将当前图片流更新到纹理所关联的OpenGLES中纹理，并绘制 FBO.</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">publc <span class=\"keyword\">void</span> <span class=\"title function_\">draw</span><span class=\"params\">()</span> &#123;</span><br><span class=\"line\">    <span class=\"comment\">//1. 绑定 FrameBuffer 到当前的绘制环境上， 后续 GL 绘制都会到这个 framebuffer</span></span><br><span class=\"line\">    GLES20.glBindFramebuffer(GLES20.GL_FRAMEBUFFER, fbo[<span class=\"number\">0</span>]);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//2.把一个2D纹理作为帧缓冲区附着</span></span><br><span class=\"line\">    <span class=\"comment\">//即所有渲染操作的结果将会被储存在 unityTextureId 对应的纹理图像中</span></span><br><span class=\"line\">    GLES20.glFramebufferTexture2D(GLES20.GL_FRAMEBUFFER, GLES20.GL_COLOR_ATTACHMENT0, GLES20.GL_TEXTURE_2D, unityTextureId, <span class=\"number\">0</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">//绑定指定纹理到当前激活的纹理单元</span></span><br><span class=\"line\">    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, videoTextureId);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">//…… 省略 Opengl 绘制的常规流程</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>这一步是最关键的，实现了将 FBO 的输出指向 Unity 里面创建的纹理，也就实现了 Android 渲染与 Unity 之间的数据打通。</p>\n<p>这里的 unityTextureId 来源于在 Unity 中初始化的 <code>(int)texture2D.GetNativeTexturePtr()</code>值。</p>\n<p>整体的流程为：</p>\n<img src=\"https://cdn.julis.wang/blog/img/bb890ed53d3e449391813b46e6dbec4e.png\">\n<p>效果图：</p>\n<img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/e9b8deec9acf448b8498471b287a2536.gif\">\n\n<p>图中播放视频区域为 Unity 的 RawImage 控件，渲染的视频通过 Pag 等相关素材由渲染SDK合成。</p>\n<p>如图所示，视频画面正常地进行渲染，图中有两个区域展示了视频画面，上面的使用的 Quad 组件，下面是用的 RawImage，流程都一直，只是在 Unity 使用 Texture2D 的时候通过 <code>Quad.mainTexture = texture2D</code> 赋值。</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>本文主要讲了 Unity 利用 Android 提供的能力进行视频相关的特效渲染的方案，总体正常运行。还需要一些优化，例如对 <code>Multithreaded Rendering</code>配置还未支持，以及一些逻辑可能受限于游戏侧的配置，例如图形渲染的配置使用的 OpenGL3.0，如果使用 OpenGL2.0 或者 Vulkan，还需要单独调整相关逻辑。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"Unity","slug":"Unity","api":"api/tags/Unity.json"},{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2022/10/25/Unity-实现利用-Andorid-能力进行视频渲染播放.json"},{"title":"我写了一个场所码小工具（上海实用）","slug":"写了一个场所码小工具","date":"2022-07-03T15:14:00.000Z","updated":"2025-05-20T11:46:57.000Z","comments":true,"url":"2022/07/03/写了一个场所码小工具/","excerpt":"<p>随着新冠疫情的常态化，以上海为例，出示健康码、扫码场所码已经成了每天进出地铁、公司必备操作。对于上班工作，每天的场所码都是同一个地点，对应的场所码图片也不会发生变化，每次扫场所码的时候要不就是前面有很多人一起挤着，或者遇到下雨天不方便等情况。对于我自己而言，我会把场所码保存下来，方便下一次“扫场所码”，但由于存在图库，每次依然需要打开支付宝或者微信选择对应的图片进行扫描，所以就做了个工具，实现：<strong>一键打开健康码、自动保存场所码、一键打开存储的场所码。</strong></p>\n<h2 id=\"小工具使用\"><a href=\"#小工具使用\" class=\"headerlink\" title=\"小工具使用\"></a>小工具使用</h2> <img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/af7fdb3bd9884abf9e8bacfa04936511.png\">\n\n<p>如图所示提供两个按钮：</p>\n<ul>\n<li>扫一扫</li>\n</ul>\n<p>扫一扫主要是为了扫场所码，它会扫码对应的二维码图片进行扫描（或者从相册进行读取），识别到对应的场所码信息会自动跳转到支付宝（当前只支持打开支付宝的场所码），并将这一次的结果保存到数据库中。如果下次需要同一个场所码，可以从列表中选择对应的场所码数据并点击直接跳转到场所码，不需要再进行手动扫描。</p>\n<ul>\n<li>健康码</li>\n</ul>\n<p>主要是实现一键打开健康码功能</p>\n<p>对于场所码的信息，在第一次添加的过程中会弹出提示框提示修改场所码的信息进行备注，当然也可以在对应的类目左滑进行编辑操作。</p>\n  <img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/94555e91ef1f4a51942f1a80c349e6d6.png\">\n\n  <img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/0d09051321934a7e8936209a94b6d110.png\">\n<h2 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h2><p>整个实现是使用Flutter写的，能实现跨平台，但由于iOS需要开发者证书相关上架AppStore较为麻烦，暂时没法进行分发。</p>\n<p>Android 同学可以扫描下面的二维码进行体验使用，或者点击 <a href=\"https://www.pgyer.com/W4La\">场所码小助手</a>去该页面下载</p>\n<img src=\"https://www.pgyer.com/app/qrcode/W4La\" width = 40% alt=\"图片名称\" align=center />\n \n","cover":null,"images":["https://cdn.julis.wang/blog/img/af7fdb3bd9884abf9e8bacfa04936511.png","https://cdn.julis.wang/blog/img/94555e91ef1f4a51942f1a80c349e6d6.png","https://cdn.julis.wang/blog/img/0d09051321934a7e8936209a94b6d110.png","https://www.pgyer.com/app/qrcode/W4La"],"content":"<p>随着新冠疫情的常态化，以上海为例，出示健康码、扫码场所码已经成了每天进出地铁、公司必备操作。对于上班工作，每天的场所码都是同一个地点，对应的场所码图片也不会发生变化，每次扫场所码的时候要不就是前面有很多人一起挤着，或者遇到下雨天不方便等情况。对于我自己而言，我会把场所码保存下来，方便下一次“扫场所码”，但由于存在图库，每次依然需要打开支付宝或者微信选择对应的图片进行扫描，所以就做了个工具，实现：<strong>一键打开健康码、自动保存场所码、一键打开存储的场所码。</strong></p>\n<h2 id=\"小工具使用\"><a href=\"#小工具使用\" class=\"headerlink\" title=\"小工具使用\"></a>小工具使用</h2> <img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/af7fdb3bd9884abf9e8bacfa04936511.png\">\n\n<p>如图所示提供两个按钮：</p>\n<ul>\n<li>扫一扫</li>\n</ul>\n<p>扫一扫主要是为了扫场所码，它会扫码对应的二维码图片进行扫描（或者从相册进行读取），识别到对应的场所码信息会自动跳转到支付宝（当前只支持打开支付宝的场所码），并将这一次的结果保存到数据库中。如果下次需要同一个场所码，可以从列表中选择对应的场所码数据并点击直接跳转到场所码，不需要再进行手动扫描。</p>\n<ul>\n<li>健康码</li>\n</ul>\n<p>主要是实现一键打开健康码功能</p>\n<p>对于场所码的信息，在第一次添加的过程中会弹出提示框提示修改场所码的信息进行备注，当然也可以在对应的类目左滑进行编辑操作。</p>\n  <img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/94555e91ef1f4a51942f1a80c349e6d6.png\">\n\n  <img width=\"40%\" src=\"https://cdn.julis.wang/blog/img/0d09051321934a7e8936209a94b6d110.png\">\n<h2 id=\"下载\"><a href=\"#下载\" class=\"headerlink\" title=\"下载\"></a>下载</h2><p>整个实现是使用Flutter写的，能实现跨平台，但由于iOS需要开发者证书相关上架AppStore较为麻烦，暂时没法进行分发。</p>\n<p>Android 同学可以扫描下面的二维码进行体验使用，或者点击 <a href=\"https://www.pgyer.com/W4La\">场所码小助手</a>去该页面下载</p>\n<img src=\"https://www.pgyer.com/app/qrcode/W4La\" width = 40% alt=\"图片名称\" align=center />\n \n","categories":[],"tags":[],"api":"api/posts/2022/07/03/写了一个场所码小工具.json"},{"title":"FFmpeg之AVFrame转Android Bitmap","slug":"FFmpeg之AVFrame转Android-Bitmap","date":"2022-05-22T13:03:00.000Z","updated":"2025-09-15T13:06:47.170Z","comments":true,"url":"2022/05/22/FFmpeg之AVFrame转Android-Bitmap/","excerpt":"<p>此前很多工作都设计到使用 FFmpeg 对视频帧进行获取，在 FFmpeg 解码视频文件获取到帧数据结构是 <code>AVFrame</code>, 对于应用层我们没有办法直接拿到进行数据处理，需要转换为 Android 平台特有的处理结构。而我是需要对应的帧图片数据，那么在 Android 侧需要将其转化为 <code>Bitmap</code> ,之前整理的过程中发现了这篇<a href=\"https://segmentfault.com/a/1190000016674715?utm_source=sf-similar-article\">《Android音视频开发】从AVFrame到MediaFrame数组(二)》</a>博客文章 ，觉得写得很不错，非常精简，适合我的需求，于是对齐进行整理，并标注一下自己在过程中遇到的一些坑点。</p>\n<h2 id=\"Native层创建Bitmap\"><a href=\"#Native层创建Bitmap\" class=\"headerlink\" title=\"Native层创建Bitmap\"></a>Native层创建Bitmap</h2><p><code>Bitmap</code> 是对 <a href=\"https://docs.microsoft.com/en-us/dotnet/api/skiasharp.skbitmap?view=skiasharp-2.80.2\">SkBitmap</a> 的包装。具体说来， Bitmap 的实现包括 Java 层和 JNI 层，JNI 层依赖 Skia，<code>SkBitmap</code> 本质上可简单理解为内存中的一个字节数组</p>\n<p>想要生成 <code>Bitmap</code>,  我们首先需要构造一个 <code>Bitmap</code> 对象，Java层有很多种方式可以生成Bitmap对象，最简单的方式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Bitmap.createBitmap(width,height,<span class=\"keyword\">new</span> <span class=\"title class_\">Bitmap</span>.Config.ARGB_8888)</span><br></pre></td></tr></table></figure>\n\n<p>由于整个 <code>FFmpeg</code>的操作在 JNI 侧进行，对应的操作需要使用 <code>JNIEnv</code>  进行相关的调用，主要逻辑如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">jobject <span class=\"title\">create_bitmap</span><span class=\"params\">(JNIEnv *env, <span class=\"type\">int</span> width, <span class=\"type\">int</span> height)</span> </span>&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 找到 Bitmap.class 和 该类中的 createBitmap 方法</span></span><br><span class=\"line\">    jclass clz_bitmap = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_bitmap = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_bitmap, <span class=\"string\">&quot;createBitmap&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;(IILandroid/graphics/Bitmap$Config;)Landroid/graphics/Bitmap;&quot;</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 配置 Bitmap</span></span><br><span class=\"line\">    jstring str_config = env-&gt;<span class=\"built_in\">NewStringUTF</span>(<span class=\"string\">&quot;ARGB_8888&quot;</span>);</span><br><span class=\"line\">    jclass clz_config = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap$Config&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_config = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_config, <span class=\"string\">&quot;valueOf&quot;</span>, <span class=\"string\">&quot;(Ljava/lang/String;)Landroid/graphics/Bitmap$Config;&quot;</span>);</span><br><span class=\"line\">    jobject obj_config = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(clz_config, mtd_config, str_config);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 创建 Bitmap 对象</span></span><br><span class=\"line\">    jobject bitmap = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(</span><br><span class=\"line\">            clz_bitmap, mtd_bitmap, width, height, obj_config);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> bitmap;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"获取Bitmap像素数据地址，并锁定\"><a href=\"#获取Bitmap像素数据地址，并锁定\" class=\"headerlink\" title=\"获取Bitmap像素数据地址，并锁定\"></a>获取Bitmap像素数据地址，并锁定</h2><figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *addr_pixels;</span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br></pre></td></tr></table></figure>\n\n<p>解释一下这两句话：</p>\n<blockquote>\n<p>第一句的作用声明并定义一个指向任意类型的指针变量，名称是addr_pixels。我们定义它的目的，是让它指向bitmap像素数据(即:<br>addr_pixels的值为bitmap像素数据的地址)。注意哦，这时候，addr_pixels的值是一个随机的值(假定此时为：0x01)，由系统分配，它还不指向bitmap像素数据。<br>第二句话的作用就是将bitmap的像素数据地址赋值给addr_pixels，此时它的值被修改(假定为：0x002)。并且锁定该地址，保证不会被移动。【注：地址不会被移动这里我也不太懂什么意思，有兴趣的可以去查看该方法的API文档】<br>【注：】此时的bitmap由像素数据的地址，但是该地址内还没有任何像素数据哦，或者说它的像素数据为\\0</p>\n</blockquote>\n<p>到这里，我们已经有了源像素数据在AVFrame中，有了目的像素数据地址addr_pixels，那么接下来的任务就是将AVFrame中的像素数据写入到addr_pixels指向的那片内存中去。</p>\n<h2 id=\"向Bitmap中写入像素数据\"><a href=\"#向Bitmap中写入像素数据\" class=\"headerlink\" title=\"向Bitmap中写入像素数据\"></a>向Bitmap中写入像素数据</h2><p>这里要说一下，我们获取到的AVFrame的像素格式通常是YUV格式的，而Bitmap的像素格式通常是RGB格式的。因此我们需要将YUV格式的像素数据转换成RGB格式进行存储。而RGB的存储空间Bitmap不是已经给我门提供好了吗？嘿嘿，直接用就OK了，那现在问题就是YUV如何转换成RGB呢？<br>关于YUV和RGB之间的转换，我知道的有三种方式：</p>\n<ul>\n<li>通过公式换算</li>\n<li>FFmpeg提供的libswscale</li>\n<li>Google提供的libyuv<br>这里我们选择libyuv因为它的性能好、使用简单。</li>\n</ul>\n<p>说它使用简单，到底有多简单，嘿，一个函数就够了！！</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">libyuv::<span class=\"built_in\">I420ToABGR</span>(frame-&gt;data[<span class=\"number\">0</span>], frame-&gt;linesize[<span class=\"number\">0</span>], <span class=\"comment\">// Y</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">1</span>], frame-&gt;linesize[<span class=\"number\">1</span>], <span class=\"comment\">// U</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">2</span>], frame-&gt;linesize[<span class=\"number\">2</span>], <span class=\"comment\">// V</span></span><br><span class=\"line\">                   (<span class=\"type\">uint8_t</span> *) addr_pixels, linesize,  <span class=\"comment\">// RGBA</span></span><br><span class=\"line\">                   frame-&gt;width, frame-&gt;height);</span><br></pre></td></tr></table></figure>\n\n<p>解释一下这个函数：</p>\n<ul>\n<li>I420ToABGR: I420表示的是YUV420P格式，ABGR表示的RGBA格式(execuse me?? 是的，你没看错，Google说RGBA格式的数据在底层的存储方式是ABGR，顺序反过来，看下libyuv源码的函数注释就知道了)</li>\n<li>frame-&gt;data&amp;linesize: 这些个参数表示的是源YUV数据，上面有标注</li>\n<li>(uint8_t *) addr_pixels: 嘿，这个就是说往这块空间里写入像素数据啦</li>\n<li>linesize: 这个表示的是该图片一行数据的字节大小，Bitmap按照RBGA格式存储，也就是说一个像素是4个字节，那么一行共有：frame-&gt;width 个像素，所以：<br>linesize &#x3D; frame-&gt; width * 4</li>\n</ul>\n<p>【注：】关于这一小块功能的实现，可能其他地方你会看到这样的写法，他们用了如下接口：</p>\n<p>&#x2F;&#x2F; 思路</p>\n<blockquote>\n<p>是：新建一个AVFrame(RGB格式)，通过av_image_fill_arrays来实现AVFrame(RGB)中像素数据和Bitmap像素数据的关联，也就是让AVFrame(RGB)像素数据指针等于addr_pixels<br>pRGBFrame &#x3D; av_frame_alloc() av_image_get_buffer_size()<br>av_image_fill_arrays() &#x2F;*<br>我也是写到这里的时候，才想到这个问题，为什么要这样用呢，直接使用addr_pixels不是也一样可以么？<br>不过大家都这么用，应该是有它不可替代的使用场景的。因此这里也说一下av_image_fill_arrays这个函数。<br>*&#x2F;</p>\n<p>&#x2F;&#x2F; TODO: 解释下这个函数的作用 av_image_fill_arrays(dst_data, dst_linesize,<br>src_data, pix_fmt, width, height, align); 它的作用就是</p>\n<ol>\n<li>根据src_data，设置dst_data，事实上根据现象或者自己去调试，可以发现dst_data的值就是src_data的值(我印象中好像值是相同的，这会我忘了，后面我再验证下)</li>\n<li>根据pix_fmt, width, height设置linesize的值，其实linesize的计算就和我上面给出的那个公式是一样子的值</li>\n</ol>\n</blockquote>\n<p>OK, 函数执行完毕，我们Bitmap就有了像素数据，下面就是把Bitmap上传给Java层</p>\n<h2 id=\"Native回调Java接口\"><a href=\"#Native回调Java接口\" class=\"headerlink\" title=\"Native回调Java接口\"></a>Native回调Java接口</h2><p>说下Java层</p>\n<p>有一个MainActivity.java用于界面的显示<br>有一个JNIHelper.java用于Java层和Native层的沟通</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">JNIHelper</span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Java层接收到Bitmap后，可以开始搞事情了</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Native层的回调代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">jclass clz = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;me/oogh/xplayer/JNIHelper&quot;</span>);</span><br><span class=\"line\">jmethodID method = env-&gt;<span class=\"built_in\">GetMethodID</span>(clz, <span class=\"string\">&quot;onReceived&quot;</span>, <span class=\"string\">&quot;(Landroid/graphics/Bitmap;)V&quot;</span>);</span><br><span class=\"line\">env-&gt;<span class=\"built_in\">CallVoidMethod</span>(obj, method, bitmap);</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"AndroidBitmap-lockPixels-方法\"><a href=\"#AndroidBitmap-lockPixels-方法\" class=\"headerlink\" title=\"AndroidBitmap_lockPixels 方法\"></a>AndroidBitmap_lockPixels 方法</h2><p>以上就是整个文章的内容，使用起来也是 no problem!  但使用过程中遇到的问题就是内存回收的问题，最开始使用的时候并没有过多关注JNI层 <code>AndroidBitmap_lockPixels</code>这个方法，以至于后来我在处理Bitmap内存回收上遇到了一些问题。 <code>AndroidBitmap_lockPixels</code> 与之对应还有一个   <code>AndroidBitmap_unlockPixels</code></p>\n<p><code>AndroidBitmap_lockPixels</code> “函数作用锁定了像素缓存以确保像素的内存不会被移动”，这句话看起来好像挺难理解，但是我们在 Java层面有与之类似的操作，那就是 <code>SurfaceHolder.lockCanvas()</code>，还记得我们在绘制的过程中需要先使用 <code>lockCanvas</code> 锁定画布，返回的画布对象<code>Canvas</code>然后使用 <code>unlockCanvasAndPost(Canvas canvas)</code> 结束锁定画布，并提交改变。<code>AndroidBitmap_lockPixels</code> 与  <code>AndroidBitmap_unlockPixels</code>做的是类似的事情，都是锁住一块内存区域，保证其安全。</p>\n<p>回到上面说的内存回收的问题，由于自己使用失误，流程大概是这样：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br><span class=\"line\"><span class=\"comment\">//在两者之间，将生成好的 Bitmap Obj 回调到Java层</span></span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_unlockPixels</span>(env, bitmap);</span><br></pre></td></tr></table></figure>\n\n<p>然后在Java层有这样的逻辑：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//……一些业务逻辑</span></span><br><span class=\"line\">    <span class=\"comment\">//我们习惯性对bitmap使用recycle对其数据进行回收……</span></span><br><span class=\"line\">    bitmap.recycle()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>但是我发现使用了 <code>bitmap.recycle()</code>与不使用，内存中 Native区域仍然占了一大部分，后来在<code>AndroidBitmap_lockPixels</code>的注释才发现不对的地方：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Given a java bitmap object, attempt to lock the pixel address.</span></span><br><span class=\"line\"><span class=\"comment\"> * Locking will ensure that the memory for the pixels will not move</span></span><br><span class=\"line\"><span class=\"comment\"> * until the unlockPixels call, and ensure that, if the pixels had been</span></span><br><span class=\"line\"><span class=\"comment\"> * previously purged, they will have been restored.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this call succeeds, it must be balanced by a call to</span></span><br><span class=\"line\"><span class=\"comment\"> * AndroidBitmap_unlockPixels, after which time the address of the pixels should</span></span><br><span class=\"line\"><span class=\"comment\"> * no longer be used.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this succeeds, *addrPtr will be set to the pixel address. If the call</span></span><br><span class=\"line\"><span class=\"comment\"> * fails, addrPtr will be ignored.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">AndroidBitmap_lockPixels</span><span class=\"params\">(JNIEnv* env, jobject jbitmap, <span class=\"type\">void</span>** addrPtr)</span></span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>其中：</p>\n<blockquote>\n<p><strong>if the pixels had been previously purged, they will have been restored.</strong></p>\n</blockquote>\n<p>也就是说在<code>AndroidBitmap_unlockPixels</code> 调用之前，如果像素数据被销毁了，他们会被恢复！至于为什么会被恢复，这个就需要之后再进行研究了。</p>\n<p>后来对逻辑进行更改，将 Bitmap.recycle()的逻辑移动到 AndroidBitmap_unlockPixels之后。</p>\n","cover":null,"images":[],"content":"<p>此前很多工作都设计到使用 FFmpeg 对视频帧进行获取，在 FFmpeg 解码视频文件获取到帧数据结构是 <code>AVFrame</code>, 对于应用层我们没有办法直接拿到进行数据处理，需要转换为 Android 平台特有的处理结构。而我是需要对应的帧图片数据，那么在 Android 侧需要将其转化为 <code>Bitmap</code> ,之前整理的过程中发现了这篇<a href=\"https://segmentfault.com/a/1190000016674715?utm_source=sf-similar-article\">《Android音视频开发】从AVFrame到MediaFrame数组(二)》</a>博客文章 ，觉得写得很不错，非常精简，适合我的需求，于是对齐进行整理，并标注一下自己在过程中遇到的一些坑点。</p>\n<h2 id=\"Native层创建Bitmap\"><a href=\"#Native层创建Bitmap\" class=\"headerlink\" title=\"Native层创建Bitmap\"></a>Native层创建Bitmap</h2><p><code>Bitmap</code> 是对 <a href=\"https://docs.microsoft.com/en-us/dotnet/api/skiasharp.skbitmap?view=skiasharp-2.80.2\">SkBitmap</a> 的包装。具体说来， Bitmap 的实现包括 Java 层和 JNI 层，JNI 层依赖 Skia，<code>SkBitmap</code> 本质上可简单理解为内存中的一个字节数组</p>\n<p>想要生成 <code>Bitmap</code>,  我们首先需要构造一个 <code>Bitmap</code> 对象，Java层有很多种方式可以生成Bitmap对象，最简单的方式如下：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\">Bitmap.createBitmap(width,height,<span class=\"keyword\">new</span> <span class=\"title class_\">Bitmap</span>.Config.ARGB_8888)</span><br></pre></td></tr></table></figure>\n\n<p>由于整个 <code>FFmpeg</code>的操作在 JNI 侧进行，对应的操作需要使用 <code>JNIEnv</code>  进行相关的调用，主要逻辑如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">jobject <span class=\"title\">create_bitmap</span><span class=\"params\">(JNIEnv *env, <span class=\"type\">int</span> width, <span class=\"type\">int</span> height)</span> </span>&#123;</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 找到 Bitmap.class 和 该类中的 createBitmap 方法</span></span><br><span class=\"line\">    jclass clz_bitmap = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_bitmap = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_bitmap, <span class=\"string\">&quot;createBitmap&quot;</span>,</span><br><span class=\"line\">            <span class=\"string\">&quot;(IILandroid/graphics/Bitmap$Config;)Landroid/graphics/Bitmap;&quot;</span>);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 配置 Bitmap</span></span><br><span class=\"line\">    jstring str_config = env-&gt;<span class=\"built_in\">NewStringUTF</span>(<span class=\"string\">&quot;ARGB_8888&quot;</span>);</span><br><span class=\"line\">    jclass clz_config = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;android/graphics/Bitmap$Config&quot;</span>);</span><br><span class=\"line\">    jmethodID mtd_config = env-&gt;<span class=\"built_in\">GetStaticMethodID</span>(</span><br><span class=\"line\">            clz_config, <span class=\"string\">&quot;valueOf&quot;</span>, <span class=\"string\">&quot;(Ljava/lang/String;)Landroid/graphics/Bitmap$Config;&quot;</span>);</span><br><span class=\"line\">    jobject obj_config = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(clz_config, mtd_config, str_config);</span><br><span class=\"line\">    </span><br><span class=\"line\">    <span class=\"comment\">// 创建 Bitmap 对象</span></span><br><span class=\"line\">    jobject bitmap = env-&gt;<span class=\"built_in\">CallStaticObjectMethod</span>(</span><br><span class=\"line\">            clz_bitmap, mtd_bitmap, width, height, obj_config);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> bitmap;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"获取Bitmap像素数据地址，并锁定\"><a href=\"#获取Bitmap像素数据地址，并锁定\" class=\"headerlink\" title=\"获取Bitmap像素数据地址，并锁定\"></a>获取Bitmap像素数据地址，并锁定</h2><figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">void</span> *addr_pixels;</span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br></pre></td></tr></table></figure>\n\n<p>解释一下这两句话：</p>\n<blockquote>\n<p>第一句的作用声明并定义一个指向任意类型的指针变量，名称是addr_pixels。我们定义它的目的，是让它指向bitmap像素数据(即:<br>addr_pixels的值为bitmap像素数据的地址)。注意哦，这时候，addr_pixels的值是一个随机的值(假定此时为：0x01)，由系统分配，它还不指向bitmap像素数据。<br>第二句话的作用就是将bitmap的像素数据地址赋值给addr_pixels，此时它的值被修改(假定为：0x002)。并且锁定该地址，保证不会被移动。【注：地址不会被移动这里我也不太懂什么意思，有兴趣的可以去查看该方法的API文档】<br>【注：】此时的bitmap由像素数据的地址，但是该地址内还没有任何像素数据哦，或者说它的像素数据为\\0</p>\n</blockquote>\n<p>到这里，我们已经有了源像素数据在AVFrame中，有了目的像素数据地址addr_pixels，那么接下来的任务就是将AVFrame中的像素数据写入到addr_pixels指向的那片内存中去。</p>\n<h2 id=\"向Bitmap中写入像素数据\"><a href=\"#向Bitmap中写入像素数据\" class=\"headerlink\" title=\"向Bitmap中写入像素数据\"></a>向Bitmap中写入像素数据</h2><p>这里要说一下，我们获取到的AVFrame的像素格式通常是YUV格式的，而Bitmap的像素格式通常是RGB格式的。因此我们需要将YUV格式的像素数据转换成RGB格式进行存储。而RGB的存储空间Bitmap不是已经给我门提供好了吗？嘿嘿，直接用就OK了，那现在问题就是YUV如何转换成RGB呢？<br>关于YUV和RGB之间的转换，我知道的有三种方式：</p>\n<ul>\n<li>通过公式换算</li>\n<li>FFmpeg提供的libswscale</li>\n<li>Google提供的libyuv<br>这里我们选择libyuv因为它的性能好、使用简单。</li>\n</ul>\n<p>说它使用简单，到底有多简单，嘿，一个函数就够了！！</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">libyuv::<span class=\"built_in\">I420ToABGR</span>(frame-&gt;data[<span class=\"number\">0</span>], frame-&gt;linesize[<span class=\"number\">0</span>], <span class=\"comment\">// Y</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">1</span>], frame-&gt;linesize[<span class=\"number\">1</span>], <span class=\"comment\">// U</span></span><br><span class=\"line\">                   frame-&gt;data[<span class=\"number\">2</span>], frame-&gt;linesize[<span class=\"number\">2</span>], <span class=\"comment\">// V</span></span><br><span class=\"line\">                   (<span class=\"type\">uint8_t</span> *) addr_pixels, linesize,  <span class=\"comment\">// RGBA</span></span><br><span class=\"line\">                   frame-&gt;width, frame-&gt;height);</span><br></pre></td></tr></table></figure>\n\n<p>解释一下这个函数：</p>\n<ul>\n<li>I420ToABGR: I420表示的是YUV420P格式，ABGR表示的RGBA格式(execuse me?? 是的，你没看错，Google说RGBA格式的数据在底层的存储方式是ABGR，顺序反过来，看下libyuv源码的函数注释就知道了)</li>\n<li>frame-&gt;data&amp;linesize: 这些个参数表示的是源YUV数据，上面有标注</li>\n<li>(uint8_t *) addr_pixels: 嘿，这个就是说往这块空间里写入像素数据啦</li>\n<li>linesize: 这个表示的是该图片一行数据的字节大小，Bitmap按照RBGA格式存储，也就是说一个像素是4个字节，那么一行共有：frame-&gt;width 个像素，所以：<br>linesize &#x3D; frame-&gt; width * 4</li>\n</ul>\n<p>【注：】关于这一小块功能的实现，可能其他地方你会看到这样的写法，他们用了如下接口：</p>\n<p>&#x2F;&#x2F; 思路</p>\n<blockquote>\n<p>是：新建一个AVFrame(RGB格式)，通过av_image_fill_arrays来实现AVFrame(RGB)中像素数据和Bitmap像素数据的关联，也就是让AVFrame(RGB)像素数据指针等于addr_pixels<br>pRGBFrame &#x3D; av_frame_alloc() av_image_get_buffer_size()<br>av_image_fill_arrays() &#x2F;*<br>我也是写到这里的时候，才想到这个问题，为什么要这样用呢，直接使用addr_pixels不是也一样可以么？<br>不过大家都这么用，应该是有它不可替代的使用场景的。因此这里也说一下av_image_fill_arrays这个函数。<br>*&#x2F;</p>\n<p>&#x2F;&#x2F; TODO: 解释下这个函数的作用 av_image_fill_arrays(dst_data, dst_linesize,<br>src_data, pix_fmt, width, height, align); 它的作用就是</p>\n<ol>\n<li>根据src_data，设置dst_data，事实上根据现象或者自己去调试，可以发现dst_data的值就是src_data的值(我印象中好像值是相同的，这会我忘了，后面我再验证下)</li>\n<li>根据pix_fmt, width, height设置linesize的值，其实linesize的计算就和我上面给出的那个公式是一样子的值</li>\n</ol>\n</blockquote>\n<p>OK, 函数执行完毕，我们Bitmap就有了像素数据，下面就是把Bitmap上传给Java层</p>\n<h2 id=\"Native回调Java接口\"><a href=\"#Native回调Java接口\" class=\"headerlink\" title=\"Native回调Java接口\"></a>Native回调Java接口</h2><p>说下Java层</p>\n<p>有一个MainActivity.java用于界面的显示<br>有一个JNIHelper.java用于Java层和Native层的沟通</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">JNIHelper</span> &#123;</span><br><span class=\"line\">   <span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">       <span class=\"comment\">// <span class=\"doctag\">TODO:</span> Java层接收到Bitmap后，可以开始搞事情了</span></span><br><span class=\"line\">   &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>Native层的回调代码如下：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\">jclass clz = env-&gt;<span class=\"built_in\">FindClass</span>(<span class=\"string\">&quot;me/oogh/xplayer/JNIHelper&quot;</span>);</span><br><span class=\"line\">jmethodID method = env-&gt;<span class=\"built_in\">GetMethodID</span>(clz, <span class=\"string\">&quot;onReceived&quot;</span>, <span class=\"string\">&quot;(Landroid/graphics/Bitmap;)V&quot;</span>);</span><br><span class=\"line\">env-&gt;<span class=\"built_in\">CallVoidMethod</span>(obj, method, bitmap);</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"AndroidBitmap-lockPixels-方法\"><a href=\"#AndroidBitmap-lockPixels-方法\" class=\"headerlink\" title=\"AndroidBitmap_lockPixels 方法\"></a>AndroidBitmap_lockPixels 方法</h2><p>以上就是整个文章的内容，使用起来也是 no problem!  但使用过程中遇到的问题就是内存回收的问题，最开始使用的时候并没有过多关注JNI层 <code>AndroidBitmap_lockPixels</code>这个方法，以至于后来我在处理Bitmap内存回收上遇到了一些问题。 <code>AndroidBitmap_lockPixels</code> 与之对应还有一个   <code>AndroidBitmap_unlockPixels</code></p>\n<p><code>AndroidBitmap_lockPixels</code> “函数作用锁定了像素缓存以确保像素的内存不会被移动”，这句话看起来好像挺难理解，但是我们在 Java层面有与之类似的操作，那就是 <code>SurfaceHolder.lockCanvas()</code>，还记得我们在绘制的过程中需要先使用 <code>lockCanvas</code> 锁定画布，返回的画布对象<code>Canvas</code>然后使用 <code>unlockCanvasAndPost(Canvas canvas)</code> 结束锁定画布，并提交改变。<code>AndroidBitmap_lockPixels</code> 与  <code>AndroidBitmap_unlockPixels</code>做的是类似的事情，都是锁住一块内存区域，保证其安全。</p>\n<p>回到上面说的内存回收的问题，由于自己使用失误，流程大概是这样：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">AndroidBitmap_lockPixels</span>(env, bitmap, &amp;addr_pixels);</span><br><span class=\"line\"><span class=\"comment\">//在两者之间，将生成好的 Bitmap Obj 回调到Java层</span></span><br><span class=\"line\"><span class=\"built_in\">AndroidBitmap_unlockPixels</span>(env, bitmap);</span><br></pre></td></tr></table></figure>\n\n<p>然后在Java层有这样的逻辑：</p>\n<figure class=\"highlight java\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">void</span> <span class=\"title function_\">onReceived</span><span class=\"params\">(Bitmap bitmap)</span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//……一些业务逻辑</span></span><br><span class=\"line\">    <span class=\"comment\">//我们习惯性对bitmap使用recycle对其数据进行回收……</span></span><br><span class=\"line\">    bitmap.recycle()</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>但是我发现使用了 <code>bitmap.recycle()</code>与不使用，内存中 Native区域仍然占了一大部分，后来在<code>AndroidBitmap_lockPixels</code>的注释才发现不对的地方：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">/**</span></span><br><span class=\"line\"><span class=\"comment\"> * Given a java bitmap object, attempt to lock the pixel address.</span></span><br><span class=\"line\"><span class=\"comment\"> * Locking will ensure that the memory for the pixels will not move</span></span><br><span class=\"line\"><span class=\"comment\"> * until the unlockPixels call, and ensure that, if the pixels had been</span></span><br><span class=\"line\"><span class=\"comment\"> * previously purged, they will have been restored.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this call succeeds, it must be balanced by a call to</span></span><br><span class=\"line\"><span class=\"comment\"> * AndroidBitmap_unlockPixels, after which time the address of the pixels should</span></span><br><span class=\"line\"><span class=\"comment\"> * no longer be used.</span></span><br><span class=\"line\"><span class=\"comment\"> *</span></span><br><span class=\"line\"><span class=\"comment\"> * If this succeeds, *addrPtr will be set to the pixel address. If the call</span></span><br><span class=\"line\"><span class=\"comment\"> * fails, addrPtr will be ignored.</span></span><br><span class=\"line\"><span class=\"comment\"> */</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">AndroidBitmap_lockPixels</span><span class=\"params\">(JNIEnv* env, jobject jbitmap, <span class=\"type\">void</span>** addrPtr)</span></span>;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>其中：</p>\n<blockquote>\n<p><strong>if the pixels had been previously purged, they will have been restored.</strong></p>\n</blockquote>\n<p>也就是说在<code>AndroidBitmap_unlockPixels</code> 调用之前，如果像素数据被销毁了，他们会被恢复！至于为什么会被恢复，这个就需要之后再进行研究了。</p>\n<p>后来对逻辑进行更改，将 Bitmap.recycle()的逻辑移动到 AndroidBitmap_unlockPixels之后。</p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"FFmpeg","slug":"FFmpeg","api":"api/tags/FFmpeg.json"},{"name":"音视频","slug":"音视频","api":"api/tags/音视频.json"}],"api":"api/posts/2022/05/22/FFmpeg之AVFrame转Android-Bitmap.json"},{"title":"记一次Android依赖库版本不兼容的问题处理过程","slug":"记一次Android依赖-wire-低版本与高版本不兼容的处理过程","date":"2022-03-31T02:47:00.000Z","updated":"2025-05-20T11:46:57.000Z","comments":true,"url":"2022/03/31/记一次Android依赖-wire-低版本与高版本不兼容的处理过程/","excerpt":"<p>此前我们项目组开发了相关 SDK 并集成到 App 工程中进行测试，发现业务App中的 <a href=\"https://github.com/square/wire\">wire</a> (一个与 protobuf 相关的库)，版本为1.5.1，而 SDK 中所依赖的版本为3.7.0，两者之间相互不兼容。如果要让业务升级到高版本的库的话，初步排查低版本中使用的某个类而高版本中已废除，单纯的就这一个类涉及100多个文件，工程量太大了，SDK中亦然。所以不能通过简单地更改版本号来解决版本冲突问题，最后经过一系列的尝试，终于解决了该问题。</p>\n<h2 id=\"分析和处理\"><a href=\"#分析和处理\" class=\"headerlink\" title=\"分析和处理\"></a>分析和处理</h2><p>最初集成 SDK 到 App 运行时发现报错：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: Failed resolution of: Lcom/squareup/wire/ProtoEnum;</span><br></pre></td></tr></table></figure>\n<p>检查代码发现 <code>ProtoEnum</code>位于wire 1.5.1中，打印依赖树发现App中的库被SDK所依赖的库给覆盖掉了，自动升级到新的版本，但新的版本又不存在该类。</p>\n<blockquote>\n<p><code>|    |    |    \\--- com.squareup.wire:wire-runtime:1.5.1 -&gt; 3.7.0</code></p>\n</blockquote>\n<p>以为只是简单的版本冲突的问题，尝试解冲突，我们知道处理Android版本冲突主要使用 <code>exclude``transitive</code> <code>force</code> gralde 处理依赖的关键字解决依赖冲突，但我无论使用什么操作整个项目中所打出来的Apk只存在一个版本：要么1.5.1要么3.7.0，对比两个库：</p>\n<img src=\"https://cdn.julis.wang/blog/img/acb66f45a28a458fb4d00ae03cecafc1.png\">\n<p>发现高版本相比于低版本多了太多的类，以及一部分类进行了改名，至此我们可以得出一个结论：<strong>wire库高版本(3.7.1)与低版本(1.5.1)完全不兼容。</strong> 现在摆在我面前有两条路可以走：</p>\n<p>一、手动升级App中的低版本</p>\n<p>二、手动降级SDK中的高版本</p>\n<p>对于第一种，发现到App中大量文件使用 Wire 中的 <code>Message</code> 类，虽然两个版本都有<code>Message</code>类，但是两者“今非昔比”，涉及到太多的方法改动，而 <code>Message</code> 类在App有100+文件使用，如果一个个改过去，可能XXXXXXXX了。</p>\n<p>两个库之间有这么大的差异，甚至1.X版本不支持kotlin，而SDK中大量代码都是使用的 Kotlin，那么第二种降低SDK的高版本也自然变得不太现实。</p>\n<p>向大佬们请教，有被指点到：<strong>是否可以通过 ffat-aar+混淆的方式将 wire库跟SDK合并打包到一起？</strong></p>\n<p>！！大佬毕竟大佬，我的脑子瞬间有一种叮咚的感觉，其实这种方式也就是将SDK变向的重命名，将两个不同的版本库进行“共存”，现在要做的就是：<strong>将 SDK 中的高版本的库包名给改掉，以达到两个不同版本库之间的兼容。</strong></p>\n<p>说干就干，打好了库之后运行发现：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.RuntimeException: Duplicate class a.a.a.a found in modules</span><br><span class=\"line\">etified-target-SDK-0.0.1.11-SNAPSHOT-runtime.jar</span><br><span class=\"line\">and ctlogin-0.4.23.04_lol_47-runtime.jar</span><br><span class=\"line\">(clogin-sso.clogin:0.4.23.04_lol_47)</span><br></pre></td></tr></table></figure>\n<img src=\"https://cdn.julis.wang/blog/img/41a8f2543baa48c5bf6b5a363cde6a9b.png\">\n<p>我当时内心就是这个表情,心想：难道<code>wtlogin</code> 大佬们也是想采用这种方式来避免一些库的兼容问题？这问题不大，因为混淆默认从a-z进行命名，只要给混淆再配一些参数就能避免掉这个问题，于是加上以下参数</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">-obfuscationdictionary obfucationdictionary.txt</span><br><span class=\"line\">-classobfuscationdictionary obfucationdictionary.txt</span><br><span class=\"line\">-packageobfuscationdictionary obfucationdictionary.txt</span><br></pre></td></tr></table></figure>\n<p>当再次运行的时候，发现又报错了：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: Failed resolution of: Lcom/squareup/wire/ProtoAdapter;</span><br></pre></td></tr></table></figure>\n<p>？？？<code>ProtoAdapter</code> 是属于3.7.0高版本的类，不是已经被混淆了么？为什么还会报这个错？</p>\n<img src=\"https://cdn.julis.wang/blog/img/32adc8ebefda4dfea050974c533fb4de.png\">\n<p>难道是使用混淆的方式是不行的？于是继续搜寻解决方案，了解到使用 <a href=\"https://github.com/shevek/jarjar\">jarjar.jar</a>可以对包重新命名打包，尝试了一下运行，依然报错</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: Failed resolution of: Lcom/squareup/wire/ProtoAdapter;</span><br></pre></td></tr></table></figure>\n\n<p>到底哪里还有 使用 <code>com/squareup/wire/ProtoAdapter</code>呢？使用<code>jd-gui</code>对重新打好的 jar 包的内容进行搜索.</p>\n<img src=\"https://cdn.julis.wang/blog/img/14e36a6fa7da4feda26779f83b4e9b63.png\">\n<p>果然有搜到相应的内容，这是一个字符串，第一反应就是反射。我瞬间明白了，这是一串字符串并且是写死的，回到App中来，如下图所示：<code>wire</code>库的作用是将 .proto 文件生成咱们通常所说的 Model 类，下图的\t<code>AudioEffect</code>就是通过 .proto 文件中定义好的属性生成的。</p>\n<img src=\"https://cdn.julis.wang/blog/img/1512ded69e1248ac9e19ef5a4b995c63.png\">\n<p>那该怎么办呢？就一个依赖库版本冲突的问题，常规的方法就这些呀，难道真的要肝一波了么？</p>\n<img src=\"https://cdn.julis.wang/blog/img/a5011fe0e772443f8e00fbbdd0b7f09f.png\">\n<p>到这里我们大概知道是怎么回事了，我们的目标很简单，就是改一个包名，但是包又在  <code>wire-gradle-plugin</code> 插件中，所以改wire的运行库当然是不行的……</p>\n<p>那还有一个终极办法：<strong>改源码</strong>。</p>\n<p>说干就干，直接拉 <a href=\"https://github.com/square/wire\">wire</a> 源码</p>\n<img src=\"https://cdn.julis.wang/blog/img/2dab42479a29453391b68a979008ae31.png\">\n<p>，主要关注以下几个目录：<br><code>wire-gradle-plugin</code>: wire-gradle 插件的主要源码<br><code>wire-compiler</code>: wire编译.proto相关的操作<br><code>wire-runtime</code>: wire运行时所需要的类<br>接下来要做的就是 <strong>右键+rename</strong>,重新编译打包，也……就1000多处改动</p>\n<img src=\"https://cdn.julis.wang/blog/img/daac3e4f19db4e5c9b8d1354ab65b7b7.png\">\n\n<p>打完包之后发现整个插件生成出来类的包还是包含<code>&quot;com.squareup.wire.ProtoAdapter&quot;</code>，我的目标是生成：<code>&quot;com.squareup.xxxx_.ProtoAdapter&quot;</code>，跟wire-plguin-gradle的源码，发现有这么一处：</p>\n<img src=\"https://cdn.julis.wang/blog/img/fed8bf2812474044b6344ec7d0acf290.png\">\n\n<p>它回在运行中重新从仓库中拉取<code>com.squareup.wire:wire-runtime:3.7.0</code>，于是需要对整个<code> wire-runtime</code>也重新打包，最终生成了一系列jar包如下所示：然后将其作为 plugin </p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"code\"><pre><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-profiles-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-compiler-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-kotlin-generator-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-gradle-plugin-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-schema-jvm-3.7.0.jar&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>然后再将运行时所需要的类跟随SDK一起打包</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"code\"><pre><span class=\"line\">implementation files(<span class=\"string\">&#x27;src/libs/wire-runtime-jvm-3.7.0.jar&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>重新编译打包，run、install 成功运行！</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>1、整个一系列操作，让我又学到了很多平时没有接触过的东西，比如：jarjar.jar，这个库对一些比较小的库存，或者说轻量级的库重新命名会比较快速的解决。本文中所述的 wire 库实在是太复杂，只能从源码层面进行操作了。还有就是在看 wire工程源码的时候又发现了一个<strong>shadowJar</strong>(利用gradle shadowjar构建包含依赖的JAR包)，之前一直用 ffat-aar打入的依赖，不知道这个插件怎么样，后面学习学习试试。</p>\n<p>2、在改整个源码之前，其实内心是比较抗拒的，因为一般改源码这种操作都是比较危险，或者更耗时，有可能就算改完了，也不一定能正常运行，一度想放弃，但实在是不想去改业务中那100+的文件，既然一条路走到黑，还是走下去吧，让我明白一定要坚持下去，不要放弃。</p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/acb66f45a28a458fb4d00ae03cecafc1.png","https://cdn.julis.wang/blog/img/41a8f2543baa48c5bf6b5a363cde6a9b.png","https://cdn.julis.wang/blog/img/32adc8ebefda4dfea050974c533fb4de.png","https://cdn.julis.wang/blog/img/14e36a6fa7da4feda26779f83b4e9b63.png","https://cdn.julis.wang/blog/img/1512ded69e1248ac9e19ef5a4b995c63.png","https://cdn.julis.wang/blog/img/a5011fe0e772443f8e00fbbdd0b7f09f.png","https://cdn.julis.wang/blog/img/2dab42479a29453391b68a979008ae31.png","https://cdn.julis.wang/blog/img/daac3e4f19db4e5c9b8d1354ab65b7b7.png","https://cdn.julis.wang/blog/img/fed8bf2812474044b6344ec7d0acf290.png"],"content":"<p>此前我们项目组开发了相关 SDK 并集成到 App 工程中进行测试，发现业务App中的 <a href=\"https://github.com/square/wire\">wire</a> (一个与 protobuf 相关的库)，版本为1.5.1，而 SDK 中所依赖的版本为3.7.0，两者之间相互不兼容。如果要让业务升级到高版本的库的话，初步排查低版本中使用的某个类而高版本中已废除，单纯的就这一个类涉及100多个文件，工程量太大了，SDK中亦然。所以不能通过简单地更改版本号来解决版本冲突问题，最后经过一系列的尝试，终于解决了该问题。</p>\n<h2 id=\"分析和处理\"><a href=\"#分析和处理\" class=\"headerlink\" title=\"分析和处理\"></a>分析和处理</h2><p>最初集成 SDK 到 App 运行时发现报错：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: Failed resolution of: Lcom/squareup/wire/ProtoEnum;</span><br></pre></td></tr></table></figure>\n<p>检查代码发现 <code>ProtoEnum</code>位于wire 1.5.1中，打印依赖树发现App中的库被SDK所依赖的库给覆盖掉了，自动升级到新的版本，但新的版本又不存在该类。</p>\n<blockquote>\n<p><code>|    |    |    \\--- com.squareup.wire:wire-runtime:1.5.1 -&gt; 3.7.0</code></p>\n</blockquote>\n<p>以为只是简单的版本冲突的问题，尝试解冲突，我们知道处理Android版本冲突主要使用 <code>exclude``transitive</code> <code>force</code> gralde 处理依赖的关键字解决依赖冲突，但我无论使用什么操作整个项目中所打出来的Apk只存在一个版本：要么1.5.1要么3.7.0，对比两个库：</p>\n<img src=\"https://cdn.julis.wang/blog/img/acb66f45a28a458fb4d00ae03cecafc1.png\">\n<p>发现高版本相比于低版本多了太多的类，以及一部分类进行了改名，至此我们可以得出一个结论：<strong>wire库高版本(3.7.1)与低版本(1.5.1)完全不兼容。</strong> 现在摆在我面前有两条路可以走：</p>\n<p>一、手动升级App中的低版本</p>\n<p>二、手动降级SDK中的高版本</p>\n<p>对于第一种，发现到App中大量文件使用 Wire 中的 <code>Message</code> 类，虽然两个版本都有<code>Message</code>类，但是两者“今非昔比”，涉及到太多的方法改动，而 <code>Message</code> 类在App有100+文件使用，如果一个个改过去，可能XXXXXXXX了。</p>\n<p>两个库之间有这么大的差异，甚至1.X版本不支持kotlin，而SDK中大量代码都是使用的 Kotlin，那么第二种降低SDK的高版本也自然变得不太现实。</p>\n<p>向大佬们请教，有被指点到：<strong>是否可以通过 ffat-aar+混淆的方式将 wire库跟SDK合并打包到一起？</strong></p>\n<p>！！大佬毕竟大佬，我的脑子瞬间有一种叮咚的感觉，其实这种方式也就是将SDK变向的重命名，将两个不同的版本库进行“共存”，现在要做的就是：<strong>将 SDK 中的高版本的库包名给改掉，以达到两个不同版本库之间的兼容。</strong></p>\n<p>说干就干，打好了库之后运行发现：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.RuntimeException: Duplicate class a.a.a.a found in modules</span><br><span class=\"line\">etified-target-SDK-0.0.1.11-SNAPSHOT-runtime.jar</span><br><span class=\"line\">and ctlogin-0.4.23.04_lol_47-runtime.jar</span><br><span class=\"line\">(clogin-sso.clogin:0.4.23.04_lol_47)</span><br></pre></td></tr></table></figure>\n<img src=\"https://cdn.julis.wang/blog/img/41a8f2543baa48c5bf6b5a363cde6a9b.png\">\n<p>我当时内心就是这个表情,心想：难道<code>wtlogin</code> 大佬们也是想采用这种方式来避免一些库的兼容问题？这问题不大，因为混淆默认从a-z进行命名，只要给混淆再配一些参数就能避免掉这个问题，于是加上以下参数</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">-obfuscationdictionary obfucationdictionary.txt</span><br><span class=\"line\">-classobfuscationdictionary obfucationdictionary.txt</span><br><span class=\"line\">-packageobfuscationdictionary obfucationdictionary.txt</span><br></pre></td></tr></table></figure>\n<p>当再次运行的时候，发现又报错了：</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: Failed resolution of: Lcom/squareup/wire/ProtoAdapter;</span><br></pre></td></tr></table></figure>\n<p>？？？<code>ProtoAdapter</code> 是属于3.7.0高版本的类，不是已经被混淆了么？为什么还会报这个错？</p>\n<img src=\"https://cdn.julis.wang/blog/img/32adc8ebefda4dfea050974c533fb4de.png\">\n<p>难道是使用混淆的方式是不行的？于是继续搜寻解决方案，了解到使用 <a href=\"https://github.com/shevek/jarjar\">jarjar.jar</a>可以对包重新命名打包，尝试了一下运行，依然报错</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"code\"><pre><span class=\"line\">java.lang.NoClassDefFoundError: Failed resolution of: Lcom/squareup/wire/ProtoAdapter;</span><br></pre></td></tr></table></figure>\n\n<p>到底哪里还有 使用 <code>com/squareup/wire/ProtoAdapter</code>呢？使用<code>jd-gui</code>对重新打好的 jar 包的内容进行搜索.</p>\n<img src=\"https://cdn.julis.wang/blog/img/14e36a6fa7da4feda26779f83b4e9b63.png\">\n<p>果然有搜到相应的内容，这是一个字符串，第一反应就是反射。我瞬间明白了，这是一串字符串并且是写死的，回到App中来，如下图所示：<code>wire</code>库的作用是将 .proto 文件生成咱们通常所说的 Model 类，下图的\t<code>AudioEffect</code>就是通过 .proto 文件中定义好的属性生成的。</p>\n<img src=\"https://cdn.julis.wang/blog/img/1512ded69e1248ac9e19ef5a4b995c63.png\">\n<p>那该怎么办呢？就一个依赖库版本冲突的问题，常规的方法就这些呀，难道真的要肝一波了么？</p>\n<img src=\"https://cdn.julis.wang/blog/img/a5011fe0e772443f8e00fbbdd0b7f09f.png\">\n<p>到这里我们大概知道是怎么回事了，我们的目标很简单，就是改一个包名，但是包又在  <code>wire-gradle-plugin</code> 插件中，所以改wire的运行库当然是不行的……</p>\n<p>那还有一个终极办法：<strong>改源码</strong>。</p>\n<p>说干就干，直接拉 <a href=\"https://github.com/square/wire\">wire</a> 源码</p>\n<img src=\"https://cdn.julis.wang/blog/img/2dab42479a29453391b68a979008ae31.png\">\n<p>，主要关注以下几个目录：<br><code>wire-gradle-plugin</code>: wire-gradle 插件的主要源码<br><code>wire-compiler</code>: wire编译.proto相关的操作<br><code>wire-runtime</code>: wire运行时所需要的类<br>接下来要做的就是 <strong>右键+rename</strong>,重新编译打包，也……就1000多处改动</p>\n<img src=\"https://cdn.julis.wang/blog/img/daac3e4f19db4e5c9b8d1354ab65b7b7.png\">\n\n<p>打完包之后发现整个插件生成出来类的包还是包含<code>&quot;com.squareup.wire.ProtoAdapter&quot;</code>，我的目标是生成：<code>&quot;com.squareup.xxxx_.ProtoAdapter&quot;</code>，跟wire-plguin-gradle的源码，发现有这么一处：</p>\n<img src=\"https://cdn.julis.wang/blog/img/fed8bf2812474044b6344ec7d0acf290.png\">\n\n<p>它回在运行中重新从仓库中拉取<code>com.squareup.wire:wire-runtime:3.7.0</code>，于是需要对整个<code> wire-runtime</code>也重新打包，最终生成了一系列jar包如下所示：然后将其作为 plugin </p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"code\"><pre><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-profiles-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-compiler-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-kotlin-generator-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-gradle-plugin-3.7.0.jar&#x27;</span>)</span><br><span class=\"line\">classpath files(<span class=\"string\">&#x27;wire-schema-jvm-3.7.0.jar&#x27;</span>)</span><br></pre></td></tr></table></figure>\n<p>然后再将运行时所需要的类跟随SDK一起打包</p>\n<figure class=\"highlight groovy\"><table><tr><td class=\"code\"><pre><span class=\"line\">implementation files(<span class=\"string\">&#x27;src/libs/wire-runtime-jvm-3.7.0.jar&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>重新编译打包，run、install 成功运行！</p>\n<h2 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h2><p>1、整个一系列操作，让我又学到了很多平时没有接触过的东西，比如：jarjar.jar，这个库对一些比较小的库存，或者说轻量级的库重新命名会比较快速的解决。本文中所述的 wire 库实在是太复杂，只能从源码层面进行操作了。还有就是在看 wire工程源码的时候又发现了一个<strong>shadowJar</strong>(利用gradle shadowjar构建包含依赖的JAR包)，之前一直用 ffat-aar打入的依赖，不知道这个插件怎么样，后面学习学习试试。</p>\n<p>2、在改整个源码之前，其实内心是比较抗拒的，因为一般改源码这种操作都是比较危险，或者更耗时，有可能就算改完了，也不一定能正常运行，一度想放弃，但实在是不想去改业务中那100+的文件，既然一条路走到黑，还是走下去吧，让我明白一定要坚持下去，不要放弃。</p>\n","categories":[],"tags":[{"name":"技术文章","slug":"technology","api":"api/tags/technology.json"}],"api":"api/posts/2022/03/31/记一次Android依赖-wire-低版本与高版本不兼容的处理过程.json"},{"title":"Android 视频抽帧","slug":"Android-视频抽帧相关","date":"2022-02-22T08:11:00.000Z","updated":"2025-05-20T11:46:57.000Z","comments":true,"url":"2022/02/22/Android-视频抽帧相关/","excerpt":"<p>要对移动端的抽帧，对于 iOS 来说，有 <a href=\"https://developer.apple.com/documentation/avfoundation\">AVFoundation</a> 这样一个神奇的库，开箱即用，已经支持了抽帧并且效率非常的高。而 Android 就不那么乐观了，Android 自带的 <code>MediaMetadataRetriever</code> 也能实现抽帧并将帧数据转化为 Bitmap，但效率非常低，平均抽取一帧需要 200ms-300ms，这当然满足不了我们的需求。无独有偶，Android 还提供了另一个类 <code>MediaCodec</code>-用于对音视频进行编解码的类，它通过访问底层的 codec 来实现编解码的功能，我们能对解码的数据进行定制化处理，本文也主要讲解利用 MediaCodec 进行抽帧。</p>\n<h2 id=\"一、MediaCodec\"><a href=\"#一、MediaCodec\" class=\"headerlink\" title=\"一、MediaCodec\"></a>一、MediaCodec</h2><p>为什么选择 MediaCodec? 项目的前期做了比较多的调研，在 Android 平台上除了 <code>MediaCodec</code> 还可以实现抽帧的方案有：<code>MediaMetadataRetriever</code>、<code>OpenCV</code>、<code>FFmpeg</code>，对于前两者实现效率非常的低，获取成本也比较大，对于 <code>FFmpeg</code>方案有进行了一定的尝试，ffmpeg是软解码抽帧(当然ffmpeg也可以 ffmpeg+mediaCodec 进行硬解码)，在设置 <code>AVCodecContext-&gt;thread_count=8</code> 速度提升了很多个档次，但对于 CPU 的使用率非常的高，消耗资源比较严重，不利于手机的流畅度，这里不再赘述。</p>\n<img src=\"https://cdn.julis.wang/blog/img/59036b70b7b44ccbad6b0b5c75445820.png\">\n\n<p>如上图所示 ffmpeg 软解 CPU 的使用率，维持在80%左右。</p>\n<p>MediaCodec 实现抽帧主要是参考 bigflake 网站提供的抽帧 Demo:<br><a href=\"https://bigflake.com/mediacodec/ExtractMpegFramesTest_egl14.java.txt\">ExtractMpegFramesTest</a></p>\n<p>主要方案流程如下图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/b256cc7c59104dfa992e7672be571009.png\">\n<p>方案使用 <code>MediaExtractor</code> 获取 Codec-specific Data(对于H.264来说，”csd-0”和”csd-1”分别对应sps和pps；对于AAC来说，”csd-0”对应ADTS)发送给 <code>MediaCodec</code> 进行解码，将解码后的数据存放在 <code>Surface</code>，由于不需要将解码后的帧进行播放展示，我们进行离屏渲染(Pbuffer)，通过 <code>glReadPixels()</code> 将 GPU 渲染完存在显存数据，回传内存。获取到对应帧 Buffer 数据之后，再利用<code>Bitmap.copyPixelsFromBuffer</code> 创建 Android 平台 Bitmap 对象。</p>\n<p>但整个方案尝试下来之后发现：使用 <code>glReadPixels</code> 将显存数据回传，以及保存 Bitmap 是比较耗时以及消耗内存的操作。</p>\n<p>那么我们可以将数据不进行回传也不保存为 Bitmap，而直接使用 GPU 上的数据进行识别么？</p>\n<h2 id=\"二、GPU-Buffer-生成流程\"><a href=\"#二、GPU-Buffer-生成流程\" class=\"headerlink\" title=\"二、GPU Buffer 生成流程\"></a>二、GPU Buffer 生成流程</h2><p>在创建GPU Buffer之前我们需要简单介绍一下 <code>SurfaceTexture</code>，SurfaceTexture 是离屏渲染,内部包含了一个BufferQueue，可以把 Surface 生成的图像流，转换为纹理，供进一步加工使用。那么 <code>SurfaceTexture</code> 与前面的 MediaCodec 结合起来</p>\n<p>我们的目的是为了将 GPU 上的图片 buffer 传递给算法侧进行识别，来自 SurfaceTexture 只支持外部 GLES 纹理<code>GL_TEXTURE_EXTERNAL_OES</code>，而算法一般都是基于 <code>OpenGL</code>使用 <code>GL_TEXTURE_2D</code> , 所以需要客户端这边做一个转换工作。</p>\n<p>外部纹理 <code>GL_TEXTURE_EXTERNAL_OES</code> 的主要优势是它们能够直接从 BufferQueue 数据进行渲染。</p>\n<p>整体流程如下图所示：<br><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f9b96bb960304a46867fe7d65a27f540~tplv-k3u1fbpfcp-zoom-crop-mark:3024:3024:3024:1702.awebp??height=952&width=1610\"></p>\n<p>在拿到了 GPU Buffer 之后，就可以与算法愉快的进行相关的识别了，并且使用硬解抽帧之后对于 CPU 的使用率降到了15%左右。</p>\n<img src=\"https://cdn.julis.wang/blog/img/e05acc59ef49439e98293d38695991c2.png\">\n\n<h2 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h2><p>使用 MediaCodec 抽帧最大的优点就是能够使用硬件进行解码，降低 CPU 的使用率，并且整个帧数据可以存在于 GPU 上，算法侧也能直接拿取数据进行进行识别，能比较好的提升 “抽帧-识别” 的效率。但由于硬解码在不同的硬件上表现的性能有一定的差异，以及在不同的视频与FFmpeg上也有存在不同的性能差异，各有优劣，所以在后续的方案上，针对于不同的视频可能会采取不同的方案进行抽帧。</p>\n<p>参考：</p>\n<p>1.<a href=\"https://bigflake.com/mediacodec/ExtractMpegFramesTest_egl14.java.txt\">ExtractMpegFramesTest.java</a></p>\n<p>2.<a href=\"https://source.android.google.cn/devices/graphics/arch-st?hl=zh-c\">SurfaceTexture</a></p>\n<p>3.<a href=\"https://juejin.cn/post/701251727476817923\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/59036b70b7b44ccbad6b0b5c75445820.png","https://cdn.julis.wang/blog/img/b256cc7c59104dfa992e7672be571009.png","https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f9b96bb960304a46867fe7d65a27f540~tplv-k3u1fbpfcp-zoom-crop-mark:3024:3024:3024:1702.awebp??height=952&width=1610","https://cdn.julis.wang/blog/img/e05acc59ef49439e98293d38695991c2.png"],"content":"<p>要对移动端的抽帧，对于 iOS 来说，有 <a href=\"https://developer.apple.com/documentation/avfoundation\">AVFoundation</a> 这样一个神奇的库，开箱即用，已经支持了抽帧并且效率非常的高。而 Android 就不那么乐观了，Android 自带的 <code>MediaMetadataRetriever</code> 也能实现抽帧并将帧数据转化为 Bitmap，但效率非常低，平均抽取一帧需要 200ms-300ms，这当然满足不了我们的需求。无独有偶，Android 还提供了另一个类 <code>MediaCodec</code>-用于对音视频进行编解码的类，它通过访问底层的 codec 来实现编解码的功能，我们能对解码的数据进行定制化处理，本文也主要讲解利用 MediaCodec 进行抽帧。</p>\n<h2 id=\"一、MediaCodec\"><a href=\"#一、MediaCodec\" class=\"headerlink\" title=\"一、MediaCodec\"></a>一、MediaCodec</h2><p>为什么选择 MediaCodec? 项目的前期做了比较多的调研，在 Android 平台上除了 <code>MediaCodec</code> 还可以实现抽帧的方案有：<code>MediaMetadataRetriever</code>、<code>OpenCV</code>、<code>FFmpeg</code>，对于前两者实现效率非常的低，获取成本也比较大，对于 <code>FFmpeg</code>方案有进行了一定的尝试，ffmpeg是软解码抽帧(当然ffmpeg也可以 ffmpeg+mediaCodec 进行硬解码)，在设置 <code>AVCodecContext-&gt;thread_count=8</code> 速度提升了很多个档次，但对于 CPU 的使用率非常的高，消耗资源比较严重，不利于手机的流畅度，这里不再赘述。</p>\n<img src=\"https://cdn.julis.wang/blog/img/59036b70b7b44ccbad6b0b5c75445820.png\">\n\n<p>如上图所示 ffmpeg 软解 CPU 的使用率，维持在80%左右。</p>\n<p>MediaCodec 实现抽帧主要是参考 bigflake 网站提供的抽帧 Demo:<br><a href=\"https://bigflake.com/mediacodec/ExtractMpegFramesTest_egl14.java.txt\">ExtractMpegFramesTest</a></p>\n<p>主要方案流程如下图所示：</p>\n<img src=\"https://cdn.julis.wang/blog/img/b256cc7c59104dfa992e7672be571009.png\">\n<p>方案使用 <code>MediaExtractor</code> 获取 Codec-specific Data(对于H.264来说，”csd-0”和”csd-1”分别对应sps和pps；对于AAC来说，”csd-0”对应ADTS)发送给 <code>MediaCodec</code> 进行解码，将解码后的数据存放在 <code>Surface</code>，由于不需要将解码后的帧进行播放展示，我们进行离屏渲染(Pbuffer)，通过 <code>glReadPixels()</code> 将 GPU 渲染完存在显存数据，回传内存。获取到对应帧 Buffer 数据之后，再利用<code>Bitmap.copyPixelsFromBuffer</code> 创建 Android 平台 Bitmap 对象。</p>\n<p>但整个方案尝试下来之后发现：使用 <code>glReadPixels</code> 将显存数据回传，以及保存 Bitmap 是比较耗时以及消耗内存的操作。</p>\n<p>那么我们可以将数据不进行回传也不保存为 Bitmap，而直接使用 GPU 上的数据进行识别么？</p>\n<h2 id=\"二、GPU-Buffer-生成流程\"><a href=\"#二、GPU-Buffer-生成流程\" class=\"headerlink\" title=\"二、GPU Buffer 生成流程\"></a>二、GPU Buffer 生成流程</h2><p>在创建GPU Buffer之前我们需要简单介绍一下 <code>SurfaceTexture</code>，SurfaceTexture 是离屏渲染,内部包含了一个BufferQueue，可以把 Surface 生成的图像流，转换为纹理，供进一步加工使用。那么 <code>SurfaceTexture</code> 与前面的 MediaCodec 结合起来</p>\n<p>我们的目的是为了将 GPU 上的图片 buffer 传递给算法侧进行识别，来自 SurfaceTexture 只支持外部 GLES 纹理<code>GL_TEXTURE_EXTERNAL_OES</code>，而算法一般都是基于 <code>OpenGL</code>使用 <code>GL_TEXTURE_2D</code> , 所以需要客户端这边做一个转换工作。</p>\n<p>外部纹理 <code>GL_TEXTURE_EXTERNAL_OES</code> 的主要优势是它们能够直接从 BufferQueue 数据进行渲染。</p>\n<p>整体流程如下图所示：<br><img src=\"https://p6-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f9b96bb960304a46867fe7d65a27f540~tplv-k3u1fbpfcp-zoom-crop-mark:3024:3024:3024:1702.awebp??height=952&width=1610\"></p>\n<p>在拿到了 GPU Buffer 之后，就可以与算法愉快的进行相关的识别了，并且使用硬解抽帧之后对于 CPU 的使用率降到了15%左右。</p>\n<img src=\"https://cdn.julis.wang/blog/img/e05acc59ef49439e98293d38695991c2.png\">\n\n<h2 id=\"三、总结\"><a href=\"#三、总结\" class=\"headerlink\" title=\"三、总结\"></a>三、总结</h2><p>使用 MediaCodec 抽帧最大的优点就是能够使用硬件进行解码，降低 CPU 的使用率，并且整个帧数据可以存在于 GPU 上，算法侧也能直接拿取数据进行进行识别，能比较好的提升 “抽帧-识别” 的效率。但由于硬解码在不同的硬件上表现的性能有一定的差异，以及在不同的视频与FFmpeg上也有存在不同的性能差异，各有优劣，所以在后续的方案上，针对于不同的视频可能会采取不同的方案进行抽帧。</p>\n<p>参考：</p>\n<p>1.<a href=\"https://bigflake.com/mediacodec/ExtractMpegFramesTest_egl14.java.txt\">ExtractMpegFramesTest.java</a></p>\n<p>2.<a href=\"https://source.android.google.cn/devices/graphics/arch-st?hl=zh-c\">SurfaceTexture</a></p>\n<p>3.<a href=\"https://juejin.cn/post/701251727476817923\">Android Opengl OES 纹理渲染到 GL_TEXTURE_2D</a></p>\n","categories":[{"name":"技术文章","slug":"technology","api":"api/categories/technology.json"}],"tags":[{"name":"FFmpeg","slug":"FFmpeg","api":"api/tags/FFmpeg.json"},{"name":"MediaCodec","slug":"MediaCodec","api":"api/tags/MediaCodec.json"}],"api":"api/posts/2022/02/22/Android-视频抽帧相关.json"},{"title":"快捷-Mac桌面adb push小工具","slug":"快捷-Mac桌面adb-push小工具","date":"2022-01-15T15:43:00.000Z","updated":"2025-09-15T13:12:07.867Z","comments":true,"url":"2022/01/15/快捷-Mac桌面adb-push小工具/","excerpt":"<p>最近在Android开发过程中会遇到很多传文件的操作，市面上也有比较多的“文件管理器”，例如锤子的HandShaker，或者谷歌官方的<a href=\"https://www.android.com/filetransfer/\">filetransfer</a>,但他们都需要打开“传输文件”这个行为，个人感觉比较繁琐。作为Android开发，使用adb命令行去传输文件是不错的，但每次输入<code>adb push xxx</code>还是挺麻烦，而且必须打开 Terminal 才能运行。为了让所有操作更简化，我做了一个简单的桌面工具，只需要连上手机，正常打开adb，拖到文件到对应区域，则可以实现文件传输。</p>\n<h2 id=\"Macos-Adb-Helper\"><a href=\"#Macos-Adb-Helper\" class=\"headerlink\" title=\"Macos Adb Helper\"></a>Macos Adb Helper</h2><p>由于自己主要是从事Android相关的开发，对于Mac桌面端的开发不太熟悉，但是 Flutter 这个神奇的跨平台框架可以让你稍微消除这个平衡。</p>\n<p>实现一键拖拽文件发送到Android手机，主要需要实现的功能有两个：</p>\n<p>1、文件拖动功能</p>\n<p>2、发送功能</p>\n<h3 id=\"文件拖动功能\"><a href=\"#文件拖动功能\" class=\"headerlink\" title=\"文件拖动功能\"></a>文件拖动功能</h3><p>拖动功能也不需要自己去写，已经有现成的库 <a href=\"https://pub.dev/packages/desktop_drop\">desktop_drop</a> 实现好了该功能。看了下源码与大多数 plugin 一样，通过methed channel进行mac os进行通信，对桌面开发相关Api不太熟，实现原理没有太深究。</p>\n<h3 id=\"发送功能\"><a href=\"#发送功能\" class=\"headerlink\" title=\"发送功能\"></a>发送功能</h3><p>发送功能主要使用 adb 功能提供的 push命令实现。不过要在 Flutter 上面实现执行命令的功能还是比较简单，只要使用 <code>Process.start(&#39;adb&#39;, [&#39;push&#39;, ...])</code>即可，但是中间会遇到权限不足的问题，报错：<code>ProcessException (ProcessException: Operation not permitted)</code><br>需要关闭沙盒权限才行，将<code>com.apple.security.app-sandbox</code>之设为<code>false</code>。 那咱们这个程序就不太安全咯?不知道还有没有更好的办法?个人感觉不大行……</p>\n<p>在这里还遇到个比较蛋疼的问题，对于实现大文件的发送，通常时间比较久，使用<code>Process.start</code>没有找到相关的api能够直接拿到进度，不过也就简单实用，问题不大 ^_^#。</p>\n<p>效果展示</p>\n<img src=\"https://cdn.julis.wang/blog/img/mac_adb_helper.gif\">\n\n<p>项目地址：<a href=\"https://github.com/VomPom/macos_adb_helper\">macos_adb_helper</a></p>\n","cover":null,"images":["https://cdn.julis.wang/blog/img/mac_adb_helper.gif"],"content":"<p>最近在Android开发过程中会遇到很多传文件的操作，市面上也有比较多的“文件管理器”，例如锤子的HandShaker，或者谷歌官方的<a href=\"https://www.android.com/filetransfer/\">filetransfer</a>,但他们都需要打开“传输文件”这个行为，个人感觉比较繁琐。作为Android开发，使用adb命令行去传输文件是不错的，但每次输入<code>adb push xxx</code>还是挺麻烦，而且必须打开 Terminal 才能运行。为了让所有操作更简化，我做了一个简单的桌面工具，只需要连上手机，正常打开adb，拖到文件到对应区域，则可以实现文件传输。</p>\n<h2 id=\"Macos-Adb-Helper\"><a href=\"#Macos-Adb-Helper\" class=\"headerlink\" title=\"Macos Adb Helper\"></a>Macos Adb Helper</h2><p>由于自己主要是从事Android相关的开发，对于Mac桌面端的开发不太熟悉，但是 Flutter 这个神奇的跨平台框架可以让你稍微消除这个平衡。</p>\n<p>实现一键拖拽文件发送到Android手机，主要需要实现的功能有两个：</p>\n<p>1、文件拖动功能</p>\n<p>2、发送功能</p>\n<h3 id=\"文件拖动功能\"><a href=\"#文件拖动功能\" class=\"headerlink\" title=\"文件拖动功能\"></a>文件拖动功能</h3><p>拖动功能也不需要自己去写，已经有现成的库 <a href=\"https://pub.dev/packages/desktop_drop\">desktop_drop</a> 实现好了该功能。看了下源码与大多数 plugin 一样，通过methed channel进行mac os进行通信，对桌面开发相关Api不太熟，实现原理没有太深究。</p>\n<h3 id=\"发送功能\"><a href=\"#发送功能\" class=\"headerlink\" title=\"发送功能\"></a>发送功能</h3><p>发送功能主要使用 adb 功能提供的 push命令实现。不过要在 Flutter 上面实现执行命令的功能还是比较简单，只要使用 <code>Process.start(&#39;adb&#39;, [&#39;push&#39;, ...])</code>即可，但是中间会遇到权限不足的问题，报错：<code>ProcessException (ProcessException: Operation not permitted)</code><br>需要关闭沙盒权限才行，将<code>com.apple.security.app-sandbox</code>之设为<code>false</code>。 那咱们这个程序就不太安全咯?不知道还有没有更好的办法?个人感觉不大行……</p>\n<p>在这里还遇到个比较蛋疼的问题，对于实现大文件的发送，通常时间比较久，使用<code>Process.start</code>没有找到相关的api能够直接拿到进度，不过也就简单实用，问题不大 ^_^#。</p>\n<p>效果展示</p>\n<img src=\"https://cdn.julis.wang/blog/img/mac_adb_helper.gif\">\n\n<p>项目地址：<a href=\"https://github.com/VomPom/macos_adb_helper\">macos_adb_helper</a></p>\n","categories":[{"name":"思考总结","slug":"thinking","api":"api/categories/thinking.json"}],"tags":[{"name":"Flutter","slug":"Flutter","api":"api/tags/Flutter.json"},{"name":"工具","slug":"工具","api":"api/tags/工具.json"}],"api":"api/posts/2022/01/15/快捷-Mac桌面adb-push小工具.json"}],"info":{"type":"archive","year":2022}},"api":"api/archives/2022/page.1.json"}